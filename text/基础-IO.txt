------------------------------------------------------------
网络编程 模型       BIO   ->   NIO   ===>   Netty   ->  AIO
------------------------------------------------------------

IO读取：

    Application -> Java IO  --->  OS IO  --->  IO设备（磁盘、网卡、内存...）


    所谓 BIO、NIO、AIO，真正的 IO实现者 都是 操作系统OS

        Java IO 的读写   都是调用的   操作系统 的 IO读写API


    Java的 各种IO模型    依赖于    操作系统 对 BIO、NIO、AIO 式的 read/write实现


        OS实现：

            阻塞式   （等待就绪）            同步                                read/write         -->     BIO
            非阻塞式 （不等待就绪）           同步                                read/write         -->     NIO
            非阻塞式 （不等待就绪）           异步（子线程 - 操作系统层面 代劳）     read/write         -->     AIO



IO源：                                                 - https://zhuanlan.zhihu.com/p/100014103

    所有 IO设备 都是 IO源，包括：

        磁盘、网卡、内存

        由于 内存 的读写很快，所以一般不会成为瓶颈所在

    我们经常说的瓶颈在于：

        磁盘IO        // 对 磁盘 的读写

        网络IO        // 对 网卡 的读写



BIO 和 NIO 的区别：      // 流 -> buffer  、   阻塞 -> 非阻塞

    1、BIO 是面向   流(字节流/字符流)       NIO  是面向  buffer

    2、BIO 是       阻塞IO                NIO  是      非阻塞IO     // 本质上NIO 还是同步的


    正是因为NIO拥有BIO没有的特性，所以它能够提升 性能


    上述两点 如何提升了性能

        1、流 VS buffer               // 面向流 比 面向buffer 的 效率更低

            编程语言 对外部设备的 读写操作 实际调用的是：

                操作系统内核的 read/Write接口

            操作系统又有：

                用户空间 和 内核空间

                用户空间 无法直接访问 内核空间，内核空间 也无法直接访问 用户空间

            当我们想把 数据 写入到 文件 中时，实际至少经过两次拷贝：

                1、用户空间内存 到 内核空间内存

                2、内核空间内存 到 文件磁盘（实际不止这2次拷贝）


            面向 流 的操作：                // BIO

                每次 只能向文件 写入 一个字节 或 n个字节

                假设每次写 一个字节，那么 100个字节大小的数据 要经历：

                    100次 用户空间到内核空间 的复制，再从 内核空间到文件磁盘 的复制操作

            面向 buffer 的操作：            // NIO

                每次可以 向文件 写入 一个buffer大小 的数据（而不只是 1个或n个 字节）

                这样子，100个字节大小的数据：

                    可以 一次性

                    从用户空间复制到内核空间，再从内核空间复制到文件磁盘

                    显然 节省了 空间复制的 次数，从而 节省时间（空间换时间）


                // 有人说，传统的BIO 也有 buffer 的，比如bufferxxx类，但是 传统的 BIO buffer 操作 还是没法和 NIO的buffer 相比



        2、阻塞 VS 非阻塞

            传统的BIO是阻塞的，经典的体现在socket上

            因为传统socket的阻塞，为了提高server的并发能力，我们会将每个socket放到一个thread中，这样子避免阻塞。

            这样子并发量大的话，会需要同时存在大量的线程。

            然而，服务器的硬件资源显然是有限的，

            如果并发有1w，那么我们需要1w的线程数量，线程占用的内存空间资源，cpu资源，线程切换带来的时间成本，显然是不行的

            基于此，linux对线程数存在着最大数量的限制


            这时候，NIO出现了：

                它解决了高并发情况下，需要大量创建线程的情况

                由于 操作系统内核 的 多路复用机制（select、poll、epoll 等）

            Java 有了 selector、channel

                这样子，一个线程中的 selector，可以 注册多个 channel

                selector 通过轮询 找到 可用的channel，而 不必创建多个线程 来处理 channel

    注意：

        与 Selector 一起使用时，Channel 必须处于 非阻塞模式下

        这意味着不能将 FileChannel 与 Selector 一起使用，因为 FileChannel 不能切换到 非阻塞模式

        而 套接字通道 都可以



NIO 本质上还是 同步的IO

    // nio本质上还是同步的
    Selector selector = Selector.open();
    channel.configureBlocking(false);
    SelectionKey key = channel.register(selector, SelectionKey.OP_READ);

    // 不断获取就绪的事件，所以是死循环
    while(true) {

        // 判断是否有就绪的事件
        int readyChannels = selector.select();
        if(readyChannels == 0) continue;

        // 获取就绪的事件
        Set selectedKeys = selector.selectedKeys();
        Iterator keyIterator = selectedKeys.iterator();

        while(keyIterator.hasNext()) {
        SelectionKey key = keyIterator.next();

            // 处理对应的事件
            if(key.isAcceptable()) {
                // a connection was accepted by a ServerSocketChannel
            } else if (key.isConnectable()) {
                // a connection was established with a remote server
            } else if (key.isReadable()) {
                // a channel is ready for reading
            } else if (key.isWritable()) {
                // a channel is ready for writing
            }
            keyIterator.remove();
        }
    }



    分析上面的代码，可以看到

        现在 一个线程 可以处理 多个IO

        而不是 传统IO 的那种 一个socket IO  只能放在  一个新线程中

        但是某个任一时刻，该线程 只能处理 某个就绪的IO事件，所以 并没有利用 cpu多核的能力（多线程）


    NIO 本质上还是 同步IO（只不过解耦了）


        只不过是用 轮询 替代了 1对1的阻塞等待


        所谓非阻塞：

            未就绪时 -> 不用等待（非阻塞）


        真-同步：

            IO就绪后  ->  read/write  仍旧由  当前Thread  自己执行      // read/write -> 同步方法  ===>  当前线程 同步执行 read/write

            ------------------------------------------------------------------
            因为只有等到 IO事件 就绪 后，才能执行 相应的 read/write 操作

                read和write操作 还是要等操作系统内核 就绪后 由 当前线程 去执行

            非阻塞 体现在 不阻塞当前线程

                传统io 在等待IO就绪时 只能阻塞 当前线程，无法干其它事情

            当前线程在 等待 io就绪 的过程中 可以先做其它事情

                通过 selector轮询 来判断是否就绪

                什么时候轮询，轮询的间隔多大 就看业务需要了



        真-异步：       // AIO - 子线程 异步 read/write  -->  Future.get  偷取 劳动果实

            就绪后  ->  read/write  由  子Thread  执行             // read/write -> 同步方法  ===>  子线程 异步执行 read/write  ->  Future.get



AIO：

    AIO
        Java 中有 Future 类

        有时候我们会把 耗时操作 放到 其它线程 处理

        其它线程 处理完 之后

        主线程（生成 子线程 的线程） 直接调用  Future.get   --->   获取劳动成果     // 活是别人干的，直接窃取别人成果，节省时间

            生活中，你想使用厨房煮一晚面

            你老妈子正在厨房干活，你没法使用厨房

            你就跟老妈子说，我要煮面，厨房可以用了通知我，你就去干自己的事情了

            谁知道，你老妈子直接帮你煮了面，你不用自己花时间煮面了



    NIO 读写操作 本质上还是要由 当前线程 去执行：

        读写操作 所需要耗费的时间 无法避免，该消耗的还是要消耗

        只是避免了 等待的时间（等待 IO就绪），所以 NIO 是 非阻塞（无需等待），但是还是 同步 的操作


    AIO，连 读写所需耗费的时间 都给你节省了：

        读写操作 由 操作系统层面 去帮你执行

        你只需 在需要的时候 去获取成果就好



零拷贝                                                         // - https://developer.ibm.com/zh/articles/j-zerocopy

    假设 从文件1读，然后 写入到文件2：

        1、传统的拷贝 需要经过：

            io设备1 -> 内核空间（read） -> 用户空间 -> 内核空间（socket） -> io设备2                  // 4次 拷贝


            劣势：

                从上面的流程可以看到

                内核 -> 用户 、 用户 -> 内核

                这两次复制 多余了


        2、所谓的 零拷贝，就是 不经过用户空间，直接变成：

            io设备1 -> 内核空间（read）  --CPU-->  内核空间（socket） -> io设备2                     // 3次 拷贝


            优势：

                节省了一次拷贝的时间

                还避免了 用户模式 和 内核模式 切换的时间

                也让 cpu 有了更多空闲


    实现：                                                       // - https://my.oschina.net/u/3990817/blog/3045359

        1、OS 底层实现：

            1、mmap + write                                          // 3次 拷贝

                mmap 是一种 内存映射文件 的方法

                    即 将 一个文件或其它对象 映射到 进程的地址空间

                    实现  文件磁盘地址（物理地址）  和  进程虚拟地址空间 中 一段虚拟地址（虚拟地址）  的  一一对映关系

                这样可以省掉

                    原来 内核read缓冲区   copy数据到    用户缓冲区

                但是还是需要

                    内核read缓冲区       将数据copy到   内核socket缓冲区



            2、sendfile                                              // 3次 拷贝  / 2次拷贝

                目的：

                    简化 通过网络 在两个通道之间 进行的数据传输过程


                sendfile系统调用的引入：                                        // 3次 拷贝

                    不仅减少了数据复制，还减少了上下文切换的次数

                    数据传送只发生在内核空间，所以减少了一次上下文切换

                    但是还是存在一次copy，能不能把这一次copy也省略掉


                优化：                                                         // 2次 拷贝

                    Linux 2.4 内核中做了改进

                    将 Kernel buffer 中 对应的数据描述信息（内存地址，偏移量）记录到 相应的socket缓冲区当中

                    这样连 内核空间 中的一次 cpu copy 也省掉了



        2、Java 实现

            java.nio.channels.FileChannel

                map                 // mmap + write

                transferTo          // sendfile




    场景：

        两个文件 之间的复制，或者  文件 -> socket  的复制

        应该想到使用 transferTo 方法


    应用：

        Java NIO、Netty、Kafka、RocketMQ

        --------------------------------
        RocketMQ的消息采用顺序写到commitlog文件，然后利用consume queue文件作为索引；
        RocketMQ采用零拷贝mmap+write的方式来回应Consumer的请求；

        同样kafka中存在大量的网络数据持久化到磁盘和磁盘文件通过网络发送的过程，
        kafka使用了sendfile零拷贝方式；





CPU 与 DMA引擎（direct memory access）：              // DMA - direct memory access - 直接内存存取

    一般情况下

        io设备  ->  内核空间   的复制操作    ===>    是由  DMA引擎  执行的

        内核空间 ->  用户空间  的复制操作    ===>    是由  CPU      执行的


核心类：

    JDK NIO：

        SocketChannel

        ServerSocketChannel


        MappedByteBuffer

        DirectByteBuffer


        FileChannel

            map()

            transferTo    // 零拷贝

                与传统方法相比， transferTo() API 大约减少了 65% 的时间
                这就极有可能提高了 需要在 I/O 通道间 大量拷贝数据 的应用程序 的性能，如 Web 服务器



    Netty：

        NioSocketChannel

            doBind、doConnect、doFinishConnect、doDisconnect、doClose...

            read、write


        CompositeChannelBuffer



模型：

    通信通道 组成：

        I/O（socket）     +       线程


    模型 演进：

        BIO + 线程（1-1）    --->    BIO + 线程池（1-1 + 线程复用）       --->    NIO + 线程池（N-1 --> IO复用 + 线程复用）


    BIO 瓶颈：

        阻塞      ->      等待 I/O就绪

        工作线程 一旦调用了 阻塞API

            在 I/O就绪 前   -->  线程 会 一直阻塞  -->  也就 无法处理 其他socket连接


    优化 方向：

        1、I/O（socket）

            阻塞      -->     非阻塞         ====>   单通道  ->  I/O（socket）复用

        2、线程

            单线程    -->     线程池         ====>   单线程  ->  线程 复用



    优化目的：

        一个线程 处理 多个socket连接

        BIO 相关的 API 是无法实现的：

            因为 BIO 相关的 socket 读写操作 都是 阻塞式

            一旦调用了 阻塞式API

            在 I/O就绪 前，调用线程 会 一直阻塞，也就 无法处理 其他的	socket 连接了


        优化方向：

            非阻塞 API   ====>  NIO（Java 原生）   --->  封装框架 Netty（基于 原生NIO）


1、BIO 线程模型：

    每一个socket 都对应 一个独立的线程

        优化 ————> 线程复用：

            独立线程  ->  引入 线程池：

                为了避免 频繁创建、销毁线程，可以采用 线程池

                但是  socket 和 线程  之间的 对应关系 并不会变化


    BIO：                    // 1-1          --->    I/O阻塞   +   Thread复用

        阻塞 API

            由于阻塞

            一个 I/O 未就绪前，会 永远阻塞        =====>   只能   1（socket）   对   1（Thread）


        模型优化：

            I/O上阻塞，无优化空间

            线程上，引入 线程池 优化

            I/O模型 最多优化到 线程池（线程复用），就没得优化了

            I/O上无法优化




    NIO：                    // N-1          --->    I/O复用   +   Thread复用

        非阻塞 API

            I/O 未就绪前，线程 不阻塞  ->  可处理 其他socket请求         =====>   为    N（socket）   对    1（Thread）  提供了可能



2、NIO 线程模型：

    NIO 模型：

        Reactor 模式


    Java 实现：

        Netty

            EventLoopGroup

                最佳实践 ———— 2个 EventLoopGroup：

                    1、bossGroup     ->  用来处理  连接请求

                        connect、disconnect

                    2、workerGroup   ->  用来处理  读写请求

                        socket -1    复杂均衡   ->          EventLoop -1
                        socket -2    复杂均衡   ->          EventLoop -2
                        socket -3    复杂均衡   ->          EventLoop -3
                                        ...



    Netty 中有⼀个核⼼概念是 EventLoopGroup：

        顾名思义，⼀个 EventLoopGroup 由 一组EventLoop 组成

        实际使⽤中，⼀般都会创建两个 EventLoopGroup：

            ⼀个称为 bossGroup          ->  处理 连接请求

            ⼀个称为 workerGroup        ->  处理 连接请求


        为什么会有 两个EventLoopGroup：

            这个和 socket 处理网络请求的机制 有关

                处理 TCP连接请求 和 读写请求 是 通过两个不同的socket 完成的：

                    1、处理 TCP网络连接请求 socket

                    2、处理 读写请求 socket


            socket 1（先处理 TCP connect）  -> TCP连接成功    =====>  创建 socket 2（处理 读写请求） -> 并将 后续读写请求 转交给 socket 2 处理

                每当 有一个TCP连接 成功建立

                都会 创建一个新的 socket

                之后对 TCP连接 的 读写请求 都是由 新创建的socket 处理完成的




        连接请求：

            上⾯我们在讨论⽹络请求的时候，为了简化模型，只是讨论了读写请求，⽽没有讨论连接请求

            在Netty中：

                bossGroup           处理  connect    请求

                workerGroup         处理  read/write 请求


            bossGroup 处理完连接请求后，会将这个连接提交给 workerGroup 来处理，workerGroup ⾥⾯有多个 EventLoop


            新的连接会交给哪个 EventLoop 来处理：

                这就需要⼀个 负载均衡算法，Netty中 ⽬前使⽤的是 轮询算法


        总结：

            Netty 是⼀个款优秀的 ⽹络编程框架，性能⾮常好


            为了实现⾼性能的⽬标，Netty 做了很多优化

                优化了 ByteBuffer、支持 零拷贝

                和 并发编程相关的 就是它的 线程模型


            Netty 的线程模型 设计得很精巧：

                每个网络连接都关联到了一个线程上

                这样做的好处是：

                    对于⼀个⽹络连接，读写操作都是单线程执⾏的，从⽽避免了并发程序的各种问题


2、AIO 线程模型：

