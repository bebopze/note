========================================================================================================================
基础
========================================================================================================================



========================================================================================================================
数据库
========================================================================================================================



------------------------------------------------------------------------------------------------------------------------
NoSQL
------------------------------------------------------------------------------------------------------------------------
之前单表热点数据接近亿级，查询时间达到了8秒左右

    后来进行了分表，按照ID取模分了一百张表，历史数据取模插入到分表中
    新建了一张表用来保存全局唯一ID，每次新建热点都会更新全局唯一ID，保证分表之后ID唯一性

    查询使用es
    GitHub上找了一个开源的MySQL数据同步到es的工具，模拟的从库，保证了数据同步的实时性

    热点数据查询性能降低到了1秒内
    性能得到很大提升

------------------------------------------------------------------------------------------------------------------------
业务中有个模块写极多读少，这种情况下是不是直接把这个数据拆分出来用单独nosql存储比较好？还是先写到nosql，再慢写到mysql好？
如果慢写到mysql
一是可能会出现数据不一致问题，
二是写请求会积累很多，内存型nosql支撑不住，可能要用leveldb之类的，磁盘多了一份数据，等要迁移的时候又增加了运维管理成本

    如果是长期都是写多读少，那么可以考虑nosql
    如果是瞬时峰值的话，还是用消息队列削峰填谷


------------------------------------------------------------------------------------------------------------------------




------------------------------------------------------------------------------------------------------------------------








========================================================================================================================
缓存
========================================================================================================================

------------------------------------------------------------------------------------------------------------------------
数据查询瓶颈
------------------------------------------------------------------------------------------------------------------------

缓存目前是标配之一（互联网开发三剑客：RPC/MQ/REDIS），凡是需要提速的地方，也许缓存就能排上用场，至少缓存的思想必然会被用上。

好处：服务提速
坏处：数据不一致风险，引入复杂度。

        原则，简单优先，能不用就不用，实在不行就需要好好考虑一番了

        缓存穿透怎么解决？缓存击穿怎么解决？缓存雪崩怎么解决？
        数据不一致性问题怎么解决？数据结构众多怎么选择合适的数据结构？
        缓存的key：value怎么设计？缓存怎么加载？过期时间怎么设置？补偿机制怎么设计？
        缓存具体选择什么方案？需不需要多层缓存？多层缓存的复杂度怎么控制？

    不过这些对于面试用处不大，面试会问各种底层结构？以及怎么优化的？怎么选择某种数据结构的？所有的一切，都是为了高性能而存在。


    是的
    缓存使用简单，但是深入难

------------------------------------------------------------------------------------------------------------------------
没有达到需要引用缓存需要的情况下，尽量不要过早使用缓存。
    缓存的坑很多，并且维护成本极高。在处理缓存的适合需要多考虑很多问题。

曾经碰到这样的情况：
    调用别人写的查询服务，但是查找到的数据却迟迟无法更新为最新数据。最后，重新写了直接查库的接口，才解决问题。

并且，缓存如果频繁更新，频繁失效 反而会带来性能的消耗。

再带上杨晓峰老师的一句话：“过早的优化是万恶之源"


    是的

------------------------------------------------------------------------------------------------------------------------
方便面那个比喻好评
缓存和缓冲区对应的英语是cache和buffer

    buffer的存在  ->  是为了解决 数据不能一次性读写完成，或某次的数据量太小 io成本又太高， 的折中方案


------------------------------------------------------------------------------------------------------------------------
老师，热点本地缓存使用组件 Guava Cache ，这个东西能存多大量呢，感觉像一个数据库

    guava cache本身没有限制，注意看存大量是否对gc有影响

------------------------------------------------------------------------------------------------------------------------
像股票之类的app页面数据实时刷新，这个是怎么做到的，是否用了缓存如何使用的缓存呢，希望老师能给解答，谢谢~


    股票的话，应该有分布式缓存，但是这个缓存更新频率高，需要用队列削峰填谷

    还有一点：

        相对来说，股票数量级  ->    是非常非常小  且固定的          // A股 4000只    美股 7000只


------------------------------------------------------------------------------------------------------------------------
热点缓存是存在本地内存之中吗
后台的列表数据有很多查询条件还有分页这种，能用缓存吗，如果能用，有什么好的缓存方案吗

    有做过这种keylist的存储
    一般要么缓存整体，要么缓存前几页的热点数据



------------------------------------------------------------------------------------------------------------------------
缓存的读写策略
------------------------------------------------------------------------------------------------------------------------


小结一下：
    1、用缓存目的是为了提速，之所以能提速 -> 关键在于使用内存来存储

        不过内存的特点是 -> 掉电后时会丢数据
        所以，一份数据会放在 DB和缓存 两个地方
        那么对于变化频繁的数据 -> 就会存在数据不一致的问题

        缓存最适合的是  ->  静态资源 或 变化低频的资源

    2、后面这个那个策略，都是为了解决变化频繁的数据的数据一致性问题的
        在解决缓存一致性问题时会引入别的问题，比如：性能问题，复杂度问题。

    3、具体怎么权衡看业务场景，不过数据一致性问题在分布式环境中是很经典和头疼的问题
        因为缓存数据会引入，别的情况也会引入，比如：主从延迟

------------------------------------------------------------------------------------------------------------------------
缓存  ->  一定会引入不一致

    所以解决的办法 需要权衡一致性和性能

------------------------------------------------------------------------------------------------------------------------
文中提到的第一个第一个缓存和数据不一致的问题，
我认为这个问题的原因是，多个客户端更新缓存和数据库之间是无序的、并发的操作，这样必然导致数据不一致的问题，
因此我们采用了监听binlog的方式，把Binlog扔到消息队列中
由一个leader来消费，负责更新缓存，保证了写缓存操作之间的顺序性，保证了缓存的准确性，避免了频繁读库。


    这样确实是一个比较好的方式，只是会稍微复杂

    监听binlong，可以使用阿里开源 Canal


------------------------------------------------------------------------------------------------------------------------
我理解WriteBack策略 相当于缓存和缓冲区合二为一了
据我所知，MySQL 的buffer pool 使用了WriteBack策略，
但为了防止系统崩溃后数据丢失，MySQL使用了WAL（Write-Ahead Logging）机制，写先日志。
好像WAL在HBase等系统也在用

    是的


------------------------------------------------------------------------------------------------------------------------
write back策略其实不算 数据库和cache 之间的策略，
而是计算机体系结构中的策略，比如：磁盘文件的缓存

它的完整读策略是这样的：
    如果缓存命中，则直接返回；
    如果缓存不命中，则重新找一个缓存块，如果这个缓存块是脏的，那么写入后端存储，并且把后端存储中的数据加载到缓存中；
    如果不是脏的，那么就把后端存储中的数据加载到缓存，然后标记缓存非脏。



------------------------------------------------------------------------------------------------------------------------
Cache Aside对缓存命中率两种解决方案中的1,可能是我没看懂，感觉没解决问题啊？

这里说在“更新数据时 也更新缓存”

我理解就是：
    先更新DB再更新缓存，这样除非在更新DB之前加分布式锁
    否则在更新DB之后加分布式锁，再更新缓存，依然较高可能出现不一致的情况。

实际中
    我们确实用在更新缓存时 用分布式锁或本地锁，只不过是发现缓存为空而去读DB时，为了解决穿透问题。

纯个人见解：
    除了cache aside，另外两种 -> 更贴近底层系统开发、而不是商业应用开发

因为我们大多数人做的系统
    都是低速存储都是数据库
    是有复杂的业务逻辑约束的，比如唯一性等，不是那种简单的 Page/CPU Cache

我们经常的写操作
    一般都要借助数据库来检验这些约束 并且在出错之后返回给用户。
    而如果直接与缓存打交道，且不论有些缓存的实现并不保证数据可靠性，也不能依靠缓存检验这些约束。

其实现在很多系统用的一种缓存模式是：

    类似CQRS
        写直接修改DB，异步更新到缓存
        读只从缓存读数据

    适合 对数据不一致窗口 可以容忍的场景



    1、是在更新数据库前加锁，锁的粒度是大了一些
    2、确实是更偏重底层开发

------------------------------------------------------------------------------------------------------------------------
Cache Aside（旁路缓存）策略，对于读多写少场景，当一个写操作更新db后同时删除缓存。然后多个读就会回源，这不会造成db压力么？

    会的
    这就是狗桩效应嘛~


------------------------------------------------------------------------------------------------------------------------
使用写回策略，如果在缓存更新到数据库之前设备掉电了，那这样数据岂不是丢失了，请问这是怎么解决的呢，通过主备机制吗，缓存数据写两份？

    是有这个问题
    比如 Page Cache在机器掉电之后就都是数据了

    一个办法是 将写入缓存的操作写入log里，类似lsm树的 write ahead log


------------------------------------------------------------------------------------------------------------------------
主从延迟场景

    其实如果是使用 Cache Aside方式的话。
    在写的时候时候因为更新数据后，删除了缓存

    在高并发情况下。那么可能会出现以下情况：

        主从同步的情况下，从库没来得及同步
        大量的读请求返回的是从库的旧数据，而这个时候读的数据会被动写入缓存

        那就存在很大的问题！这种应该怎么处理！
        如果是这样的话？是不是只能依靠分布式锁来实现了！



    是的
    这样只能更新缓存，然后使用 分布式锁来控制


------------------------------------------------------------------------------------------------------------------------
在cache aside策略中，如果先更新数据库，再删除缓存。
这样如果读请求访问量很大，会短时间出现大量请求穿透到数据库，这里有好的办法优化吗？


    如果更新不频繁的话，其实还OK

    如果更新频繁
        可以加分布式锁，让单一线程可以更新这条数据；
        或者设置短的过期时间，让可能出现的不一致数据尽快过期





------------------------------------------------------------------------------------------------------------------------
缓存的高可用
------------------------------------------------------------------------------------------------------------------------

高可用的设计思路  ->  没有其他的   ==>   核心就是  ->  增加副本

    针对数据  ->  就增加数据副本
    针对服务  ->  就增加服务副本
    针对机房  ->  就增加机房副本


增加副本引入的新问题是  ==>  数据不一致性


下面各种算法什么的，都是为了解决：

    1、因增加副本     ->   而带来的  数据不一致性问题

    2、或者 节点挂了  ->   怎么使服务 继续可用的策略

        比如：
            数据怎么迁移？故障怎么隔离？故障节点恢复后怎么是否加入？怎么加入？

    ------------------------------------------------------------------------
    最近在看火影：

        影分术  ->  就是鸣人的高可用方式，其他的高可用思路和这个如出一辙

        从动漫中也可以看出  ->  这个需要更多能量

        对公司而言  ->  需要更多机器和存储空间，技术复杂度也会增加一些

    幸好有现成的组件，避免人人都重复造轮子的资源浪费。


------------------------------------------------------------------------------------------------------------------------
关于客户端模式:

    1、客户端模式是指用户代码的服务端(以我们的电商系统为例)吗?
        是的，对于缓存来说，应用就是它的客户端

    2、客户端模式的缓存存放在何处?
        单独部署


------------------------------------------------------------------------------------------------------------------------
Client(客户端) 和 Server(服务端) 的理解：

    老实说一开始我也一脸懵逼，以为 客户端就是用户端


    但是后来想通了：

        应用服务器 为 用户 提供数据接口
            用户就是客户端，应用就是服务端


        但是 缓存 为 应用服务器 提供缓存服务

            这时候 对于缓存服务器来说   ==>   应用服务器  ->  就是客户端  ， 而缓存  ->  就是服务端


------------------------------------------------------------------------------------------------------------------------
主从会有延迟，写入主库，但延迟同步到从库，在同步完成前去从库读数据，读不到，这如何解决呢？

    1、写入的时候更新缓存，这样从缓存里面读就实时了
    2、直接读主库


------------------------------------------------------------------------------------------------------------------------
您之前说4核8G的机器上，MySQL最高支撑QPS为1万，怎样本文开头又说MySQL读峰值才1500/s呢

    1万是基准测试的结果，在实际中sql更复杂，达不到这个性能

------------------------------------------------------------------------------------------------------------------------
多副本你可以认为是缓存的缓存，也就是在缓存之上再加一组缓存，可以解决单组缓存的带宽瓶颈

而在实践过程中，因为大部分流量都会被副本承担，master和slave的数据有可能会变得不热，所以可以把master和slave当做副本


------------------------------------------------------------------------------------------------------------------------
缓存和数据库的同步机制，有没有开源的框架实现呢？

    有一些 从DB 同步数据到cache  的组件

------------------------------------------------------------------------------------------------------------------------
一致性哈希算法的不同实现
    1、哈希环法
    2、google的jump consistent hash
    3、Maglev一致性哈希法

------------------------------------------------------------------------------------------------------------------------
关于一致性hash有几点疑问，请老师解答：
1、为了防止hash环的倾斜，由实际节点虚拟出来的一部分虚拟节点 是如何保证虚拟节点能均匀排列呢？还是说增加虚拟节点的算法 可以自定义虚拟节点插入的位置呢？
2、假设一个真实节点宕机了，那是不是那个真实节点相对应的虚拟节点也“宕机”(不可用)了？
3、一致性hash算法在java里或者.net里面有现成的实现吗？若有是对应那个类呢？


1、从概率学上保证，如果你虚拟出来的节点足够多，你无法保证绝对的均匀
2、是的 虚拟节点也宕机了
3、貌似是没有的，不过在一些 memcached客户端代码 中都会有实现


========================================================================================================================
消息队列
========================================================================================================================


------------------------------------------------------------------------------------------------------------------------


------------------------------------------------------------------------------------------------------------------------





========================================================================================================================
分布式服务
========================================================================================================================


------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------





========================================================================================================================
维护
========================================================================================================================



------------------------------------------------------------------------------------------------------------------------


------------------------------------------------------------------------------------------------------------------------





========================================================================================================================
实战
========================================================================================================================



------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------