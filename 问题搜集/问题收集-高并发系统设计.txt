========================================================================================================================
基础
========================================================================================================================



========================================================================================================================
数据库
========================================================================================================================



------------------------------------------------------------------------------------------------------------------------
NoSQL
------------------------------------------------------------------------------------------------------------------------
之前单表热点数据接近亿级，查询时间达到了8秒左右

    后来进行了分表，按照ID取模分了一百张表，历史数据取模插入到分表中
    新建了一张表用来保存全局唯一ID，每次新建热点都会更新全局唯一ID，保证分表之后ID唯一性

    查询使用es
    GitHub上找了一个开源的MySQL数据同步到es的工具，模拟的从库，保证了数据同步的实时性

    热点数据查询性能降低到了1秒内
    性能得到很大提升

------------------------------------------------------------------------------------------------------------------------
业务中有个模块写极多读少，这种情况下是不是直接把这个数据拆分出来用单独nosql存储比较好？还是先写到nosql，再慢写到mysql好？
如果慢写到mysql
一是可能会出现数据不一致问题，
二是写请求会积累很多，内存型nosql支撑不住，可能要用leveldb之类的，磁盘多了一份数据，等要迁移的时候又增加了运维管理成本

    如果是长期都是写多读少，那么可以考虑nosql
    如果是瞬时峰值的话，还是用消息队列削峰填谷


------------------------------------------------------------------------------------------------------------------------




------------------------------------------------------------------------------------------------------------------------
数据的迁移                                                                                       // 双写、级联同步
------------------------------------------------------------------------------------------------------------------------



数据迁移目前还没做过，不过面试时曾经被问到过，给的是双写的方案。

关键点：
    1、数据校验怎么做？
    2、什么时候换成读新库？
    3、什么时候不再写旧库？

允许停服务，怎么都好说。不能停服务，选择的时机非常关键。
哪一个时刻判断出数据已经一致了，怎么判断太关键了，线上有可能做不到完全一致，需要做一些线下补偿。

怎么判断数据是否一致呢？
记录条数加抽样比对，允许一定的误差，不如总有误差就永远切不了啦！然后再去线下补偿。


    数据校验好了就可以切新库


------------------------------------------------------------------------------------------------------------------------
基于双写的方案，我总结了2种实现：

    1、基于同步写新库和旧库方案
        在写入数据的时候，同步写入旧库数据，异步写入新库数据。
        数据校验，对部分数据进行校验（最容易出问题的地方，需要提前准备好脚本）。
        使用灰度发布方式将读流量切换到新库。
        观察几天没问题后，可以改成只写新库。

    2、基于Canal的迁移方案
        将新库配置为源库的从库，同步数据。比如使用Canal同步数据。
        数据校验，对部分数据进行校验（最容易出问题的地方，需要提前准备好脚本）。
        使用灰度发布方式将读流量切换到新库。
        暂停应用，将新库作为主库写入，使用Canal同步到旧库。
        观察几天没问题后，撤销Canal的同步。


------------------------------------------------------------------------------------------------------------------------
数据具体怎么校验呢，简单说下常用的校验方式吧

    比如说从源库随机抽取一定量的数据，从新库中查询看是否一致


------------------------------------------------------------------------------------------------------------------------
老师说的迁移是指的旧库迁移到新库，新旧库的表结构基本一样。
但是如果系统重构后的迁移就很难做了。我以前遇到一个大的系统，整个db里面有几千张表。
重构后采用微服务的方式，原来的一个db分成了10多个db，还做了分表。有些原来旧库的表也做了拆分，合并，字段的增加、减少等。
旧库表中的有些字段名字都重新命名了。这样的数据迁移都是狗血的数据迁移。
整个公司组建一个数据迁移团队，包括开发，架构师，技术总监，dba，运维等几百人，数据校验也基本都是人工校验。
耗几个月才完成了数据迁移。而且问题一大推。面对如何奇葩的数据迁移，老师有什么好的方案？


    数据的同步可以考虑解析binlog来同步

    校验就真的没辙了，我之前经历的大的数据迁移都是以月为单位的

------------------------------------------------------------------------------------------------------------------------
关于双写方案有两个疑问需要请教一下：
    1、数据同时写入两个数据库怎么做对代码的改动比较小呢？有成熟的工具或中间件来做吗？
    2、新库在同步追上旧库的binlog后，在开始双写时需要断开吗？
        不然对于新库会有重复的数据。如果新库需要停止对旧库的binlog同步，和双写的开启时机这里怎么做协调呢？


    1、还真没有，其实改动代码也简单
    2、需要断开的，可以在双写的时候加开关，断开同步时立刻打开开关


------------------------------------------------------------------------------------------------------------------------
迁移redis 有分布式锁的存在，该怎样处理好

    我会想在业务低峰期迁移

------------------------------------------------------------------------------------------------------------------------
目前正在经历的上云，物理机与云都不互通，需要跳板机一层层的跳，乱七八糟一大堆的东西，就几个人搞，太坑了。

    我目前也在经历上云
    物理机和云的互通应该是上云的第一步吧

------------------------------------------------------------------------------------------------------------------------
如果数据的迁移还伴随着领域模型的重构
库表结构差异巨大，单纯基于binlog主从同步无法达到目标，必须业务上实现数据的模型变更，存量迁移和增量同步双写
有没有一些好的实践经验？


    先迁移，后重构，不香吗

    如果一起做的话，可以基于消息来做，只是会复杂很多

------------------------------------------------------------------------------------------------------------------------
双写需要考虑分布式一致性吗，如何比较简单的实现？

    其实很难做到一致性~
    所以要做数据校验，校验迁移源和目的是否一致

------------------------------------------------------------------------------------------------------------------------
想问下双写的具体实现是怎么做呢？

我的理解是：
    1、同步写入旧库和新库，同时需要保证原子性是吗？
    2、是对原本应用服务器中的所有sql语句进行拦截，如果是写操作，则做多一个写入新库的逻辑？


    1、很难保证原子性，所以要做数据校验
    2、是的


------------------------------------------------------------------------------------------------------------------------
我做过两次数据同步迁移，使用的双写方案，灰度发布，没有问题后全量切，一般需要半个月。

    那你们确实比较谨慎

------------------------------------------------------------------------------------------------------------------------
级联方案中 备库的作用是什么？
文中说是用于回滚，防止云上环境与机房环境不一致。
但是，回滚直接回滚到旧库不行吗？三个库的数据是同步的啊？


    迁移上云之后，旧库和云上的数据库之间的同步就断掉了，因为数据已经往云上写了


------------------------------------------------------------------------------------------------------------------------
关于"双写之前要将同步断掉", 什么时候将同步断掉呢？这个是怎样判断 同步差不多、可以关闭同步、打开双写的？

    主从延迟是可以监控的，可以看主从没有延迟了就可以断掉同步了

------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------


------------------------------------------------------------------------------------------------------------------------




========================================================================================================================
缓存
========================================================================================================================

------------------------------------------------------------------------------------------------------------------------
数据查询瓶颈
------------------------------------------------------------------------------------------------------------------------

缓存目前是标配之一（互联网开发三剑客：RPC/MQ/REDIS），凡是需要提速的地方，也许缓存就能排上用场，至少缓存的思想必然会被用上。

好处：服务提速
坏处：数据不一致风险，引入复杂度。

        原则，简单优先，能不用就不用，实在不行就需要好好考虑一番了

        缓存穿透怎么解决？缓存击穿怎么解决？缓存雪崩怎么解决？
        数据不一致性问题怎么解决？数据结构众多怎么选择合适的数据结构？
        缓存的key：value怎么设计？缓存怎么加载？过期时间怎么设置？补偿机制怎么设计？
        缓存具体选择什么方案？需不需要多层缓存？多层缓存的复杂度怎么控制？

    不过这些对于面试用处不大，面试会问各种底层结构？以及怎么优化的？怎么选择某种数据结构的？所有的一切，都是为了高性能而存在。


    是的
    缓存使用简单，但是深入难

------------------------------------------------------------------------------------------------------------------------
没有达到需要引用缓存需要的情况下，尽量不要过早使用缓存。
    缓存的坑很多，并且维护成本极高。在处理缓存的适合需要多考虑很多问题。

曾经碰到这样的情况：
    调用别人写的查询服务，但是查找到的数据却迟迟无法更新为最新数据。最后，重新写了直接查库的接口，才解决问题。

并且，缓存如果频繁更新，频繁失效 反而会带来性能的消耗。

再带上杨晓峰老师的一句话：“过早的优化是万恶之源"


    是的

------------------------------------------------------------------------------------------------------------------------
方便面那个比喻好评
缓存和缓冲区对应的英语是cache和buffer

    buffer的存在  ->  是为了解决 数据不能一次性读写完成，或某次的数据量太小 io成本又太高， 的折中方案


------------------------------------------------------------------------------------------------------------------------
老师，热点本地缓存使用组件 Guava Cache ，这个东西能存多大量呢，感觉像一个数据库

    guava cache本身没有限制，注意看存大量是否对gc有影响

------------------------------------------------------------------------------------------------------------------------
像股票之类的app页面数据实时刷新，这个是怎么做到的，是否用了缓存如何使用的缓存呢，希望老师能给解答，谢谢~


    股票的话，应该有分布式缓存，但是这个缓存更新频率高，需要用队列削峰填谷

    还有一点：

        相对来说，股票数量级  ->    是非常非常小  且固定的          // A股 4000只    美股 7000只


------------------------------------------------------------------------------------------------------------------------
热点缓存是存在本地内存之中吗
后台的列表数据有很多查询条件还有分页这种，能用缓存吗，如果能用，有什么好的缓存方案吗

    有做过这种keylist的存储
    一般要么缓存整体，要么缓存前几页的热点数据



------------------------------------------------------------------------------------------------------------------------
缓存的读写策略
------------------------------------------------------------------------------------------------------------------------


小结一下：
    1、用缓存目的是为了提速，之所以能提速 -> 关键在于使用内存来存储

        不过内存的特点是 -> 掉电后时会丢数据
        所以，一份数据会放在 DB和缓存 两个地方
        那么对于变化频繁的数据 -> 就会存在数据不一致的问题

        缓存最适合的是  ->  静态资源 或 变化低频的资源

    2、后面这个那个策略，都是为了解决变化频繁的数据的数据一致性问题的
        在解决缓存一致性问题时会引入别的问题，比如：性能问题，复杂度问题。

    3、具体怎么权衡看业务场景，不过数据一致性问题在分布式环境中是很经典和头疼的问题
        因为缓存数据会引入，别的情况也会引入，比如：主从延迟

------------------------------------------------------------------------------------------------------------------------
缓存  ->  一定会引入不一致

    所以解决的办法 需要权衡一致性和性能

------------------------------------------------------------------------------------------------------------------------
文中提到的第一个第一个缓存和数据不一致的问题，
我认为这个问题的原因是，多个客户端更新缓存和数据库之间是无序的、并发的操作，这样必然导致数据不一致的问题，
因此我们采用了监听binlog的方式，把Binlog扔到消息队列中
由一个leader来消费，负责更新缓存，保证了写缓存操作之间的顺序性，保证了缓存的准确性，避免了频繁读库。


    这样确实是一个比较好的方式，只是会稍微复杂

    监听binlong，可以使用阿里开源 Canal


------------------------------------------------------------------------------------------------------------------------
我理解WriteBack策略 相当于缓存和缓冲区合二为一了
据我所知，MySQL 的buffer pool 使用了WriteBack策略，
但为了防止系统崩溃后数据丢失，MySQL使用了WAL（Write-Ahead Logging）机制，写先日志。
好像WAL在HBase等系统也在用

    是的


------------------------------------------------------------------------------------------------------------------------
write back策略其实不算 数据库和cache 之间的策略，
而是计算机体系结构中的策略，比如：磁盘文件的缓存

它的完整读策略是这样的：
    如果缓存命中，则直接返回；
    如果缓存不命中，则重新找一个缓存块，如果这个缓存块是脏的，那么写入后端存储，并且把后端存储中的数据加载到缓存中；
    如果不是脏的，那么就把后端存储中的数据加载到缓存，然后标记缓存非脏。



------------------------------------------------------------------------------------------------------------------------
Cache Aside对缓存命中率两种解决方案中的1,可能是我没看懂，感觉没解决问题啊？

这里说在“更新数据时 也更新缓存”

我理解就是：
    先更新DB再更新缓存，这样除非在更新DB之前加分布式锁
    否则在更新DB之后加分布式锁，再更新缓存，依然较高可能出现不一致的情况。

实际中
    我们确实用在更新缓存时 用分布式锁或本地锁，只不过是发现缓存为空而去读DB时，为了解决穿透问题。

纯个人见解：
    除了cache aside，另外两种 -> 更贴近底层系统开发、而不是商业应用开发

因为我们大多数人做的系统
    都是低速存储都是数据库
    是有复杂的业务逻辑约束的，比如唯一性等，不是那种简单的 Page/CPU Cache

我们经常的写操作
    一般都要借助数据库来检验这些约束 并且在出错之后返回给用户。
    而如果直接与缓存打交道，且不论有些缓存的实现并不保证数据可靠性，也不能依靠缓存检验这些约束。

其实现在很多系统用的一种缓存模式是：

    类似CQRS
        写直接修改DB，异步更新到缓存
        读只从缓存读数据

    适合 对数据不一致窗口 可以容忍的场景



    1、是在更新数据库前加锁，锁的粒度是大了一些
    2、确实是更偏重底层开发

------------------------------------------------------------------------------------------------------------------------
Cache Aside（旁路缓存）策略，对于读多写少场景，当一个写操作更新db后同时删除缓存。然后多个读就会回源，这不会造成db压力么？

    会的
    这就是狗桩效应嘛~


------------------------------------------------------------------------------------------------------------------------
使用写回策略，如果在缓存更新到数据库之前设备掉电了，那这样数据岂不是丢失了，请问这是怎么解决的呢，通过主备机制吗，缓存数据写两份？

    是有这个问题
    比如 Page Cache在机器掉电之后就都是数据了

    一个办法是 将写入缓存的操作写入log里，类似lsm树的 write ahead log


------------------------------------------------------------------------------------------------------------------------
主从延迟场景

    其实如果是使用 Cache Aside方式的话。
    在写的时候时候因为更新数据后，删除了缓存

    在高并发情况下。那么可能会出现以下情况：

        主从同步的情况下，从库没来得及同步
        大量的读请求返回的是从库的旧数据，而这个时候读的数据会被动写入缓存

        那就存在很大的问题！这种应该怎么处理！
        如果是这样的话？是不是只能依靠分布式锁来实现了！



    是的
    这样只能更新缓存，然后使用 分布式锁来控制


------------------------------------------------------------------------------------------------------------------------
在cache aside策略中，如果先更新数据库，再删除缓存。
这样如果读请求访问量很大，会短时间出现大量请求穿透到数据库，这里有好的办法优化吗？


    如果更新不频繁的话，其实还OK

    如果更新频繁
        可以加分布式锁，让单一线程可以更新这条数据；
        或者设置短的过期时间，让可能出现的不一致数据尽快过期





------------------------------------------------------------------------------------------------------------------------
缓存的高可用
------------------------------------------------------------------------------------------------------------------------

高可用的设计思路  ->  没有其他的   ==>   核心就是  ->  增加副本

    针对数据  ->  就增加数据副本
    针对服务  ->  就增加服务副本
    针对机房  ->  就增加机房副本


增加副本引入的新问题是  ==>  数据不一致性


下面各种算法什么的，都是为了解决：

    1、因增加副本     ->   而带来的  数据不一致性问题

    2、或者 节点挂了  ->   怎么使服务 继续可用的策略

        比如：
            数据怎么迁移？故障怎么隔离？故障节点恢复后怎么是否加入？怎么加入？

    ------------------------------------------------------------------------
    最近在看火影：

        影分术  ->  就是鸣人的高可用方式，其他的高可用思路和这个如出一辙

        从动漫中也可以看出  ->  这个需要更多能量

        对公司而言  ->  需要更多机器和存储空间，技术复杂度也会增加一些

    幸好有现成的组件，避免人人都重复造轮子的资源浪费。


------------------------------------------------------------------------------------------------------------------------
关于客户端模式:

    1、客户端模式是指用户代码的服务端(以我们的电商系统为例)吗?
        是的，对于缓存来说，应用就是它的客户端

    2、客户端模式的缓存存放在何处?
        单独部署


------------------------------------------------------------------------------------------------------------------------
Client(客户端) 和 Server(服务端) 的理解：

    老实说一开始我也一脸懵逼，以为 客户端就是用户端


    但是后来想通了：

        应用服务器 为 用户 提供数据接口
            用户就是客户端，应用就是服务端


        但是 缓存 为 应用服务器 提供缓存服务

            这时候 对于缓存服务器来说   ==>   应用服务器  ->  就是客户端  ， 而缓存  ->  就是服务端


------------------------------------------------------------------------------------------------------------------------
主从会有延迟，写入主库，但延迟同步到从库，在同步完成前去从库读数据，读不到，这如何解决呢？

    1、写入的时候更新缓存，这样从缓存里面读就实时了
    2、直接读主库


------------------------------------------------------------------------------------------------------------------------
您之前说4核8G的机器上，MySQL最高支撑QPS为1万，怎样本文开头又说MySQL读峰值才1500/s呢

    1万是基准测试的结果，在实际中sql更复杂，达不到这个性能

------------------------------------------------------------------------------------------------------------------------
多副本你可以认为是缓存的缓存，也就是在缓存之上再加一组缓存，可以解决单组缓存的带宽瓶颈

而在实践过程中，因为大部分流量都会被副本承担，master和slave的数据有可能会变得不热，所以可以把master和slave当做副本


------------------------------------------------------------------------------------------------------------------------
缓存和数据库的同步机制，有没有开源的框架实现呢？

    有一些 从DB 同步数据到cache  的组件

------------------------------------------------------------------------------------------------------------------------
一致性哈希算法的不同实现
    1、哈希环法
    2、google的jump consistent hash
    3、Maglev一致性哈希法

------------------------------------------------------------------------------------------------------------------------
关于一致性hash有几点疑问，请老师解答：
1、为了防止hash环的倾斜，由实际节点虚拟出来的一部分虚拟节点 是如何保证虚拟节点能均匀排列呢？还是说增加虚拟节点的算法 可以自定义虚拟节点插入的位置呢？
2、假设一个真实节点宕机了，那是不是那个真实节点相对应的虚拟节点也“宕机”(不可用)了？
3、一致性hash算法在java里或者.net里面有现成的实现吗？若有是对应那个类呢？


1、从概率学上保证，如果你虚拟出来的节点足够多，你无法保证绝对的均匀
2、是的 虚拟节点也宕机了
3、貌似是没有的，不过在一些 memcached客户端代码 中都会有实现




------------------------------------------------------------------------------------------------------------------------
缓存穿透
------------------------------------------------------------------------------------------------------------------------

解决缓存穿透的方案：

    1、回种空值
        是一种最常见的解决思路，实现起来也最简单，如果评估空值缓存占据的缓存空间可以接受，那么可以优先使用这种方案

    2、布隆过滤器

        会引入一个新的组件，也会引入一些开发上的复杂度和运维上的成本

        所以只有在存在海量查询数据库中，不存在数据的请求时才会使用，在使用时也要关注布隆过滤器对内存空间的消耗


    3、对于极热点缓存数据穿透造成的“狗桩效应”

        可以通过设置分布式锁 或者 后台线程定时加载的方式来解决


------------------------------------------------------------------------------------------------------------------------
核心思想

    数据库是一个脆弱的资源，它无论是在扩展性、性能还是承担并发的能力上，相比缓存都处于绝对的劣势

    所以我们解决缓存穿透问题的核心目标   ->   在于 减少对于数据库的并发请求


------------------------------------------------------------------------------------------------------------------------
那么这个过滤器在微服务环境下如何部署比较好？部署好了以后是否也可以起到防止洪水攻击带来的缓存穿透问题？

    以单独服务部署比较好
    我之前团队曾经改了一下redis源码来实现这个功能，利用redis的存储机制

------------------------------------------------------------------------------------------------------------------------
布隆过滤器可以用在一些资讯app的新闻展示中，给用户推送新的资讯用来过滤掉那些用户已经浏览过的记录

    是的

------------------------------------------------------------------------------------------------------------------------
缓存穿透跟缓存雪崩是一个概念么？如果不是那缓存雪崩的解决方案是什么呢？谢谢

    穿透  指的是缓存中没有数据，需要到数据库中去取；

    雪崩  指的是一个缓存节点的故障导致全局故障，两者是不同的
          之前提的一致性hash中实现虚拟节点就是一种避免雪崩的方法

------------------------------------------------------------------------------------------------------------------------
分布式锁和一般锁的有什么区别呢？

    一般的锁  是进程中的锁，可以同步一个进程中的多个线程；
    分布式锁  可以同步多个进程

------------------------------------------------------------------------------------------------------------------------
缓存适合存放什么样子的数据呢？还是数据库里面的所有数据都可以放入缓存呢？

    缓存适合放经常访问的热数据，不能放全量数据，而且也放不下

------------------------------------------------------------------------------------------------------------------------
bloom filter这个用法会在各种文章教程中提出
但我一直有个疑问想求教一下：基于redis这种存储的bloom filter有没有成熟的方案？
    因为我觉得毕竟redis不是一种完全可靠的存储，一旦crash理论上有可能丢数据
    在用户的那个案例中，一旦应该出现在bloom filter中的数据丢失了，就意味着永远也查不出这个用户来了。
    那是否我们还应该启动一个监控进程，一旦发现redis crash了，要重新构建bloom filter呢？

回到思考题，我现在觉得文章中的三个方案都只能解决部分场景的问题，有时候需要配合使用。
    除此之外，合理的数据库连接池大小以及服务限流也能起到最后防线的作用吧？


    bloom filter需要自研，可以基于redis持久化存储到硬盘上

    连接池应该解决不了问题，因为链接不是无限的；限流是有损的


------------------------------------------------------------------------------------------------------------------------
分布式锁的方案中，有问题吧？如果第二个线程发现有这个key说明有别的线程在加载数据，但是还没有加载完，这个时候读缓存是没有的。

    可以返回失败 不从数据库读取，或者重试


------------------------------------------------------------------------------------------------------------------------
布隆过滤器单独部署服务，服务启动时需要初始化数据，将数据库中数据初始化到过滤器中。后续将布隆过滤器定期写到磁盘中，防止服务重启导致丢失。
请教下，文中说的布隆过滤器是否如上上面说部署？

    是的






------------------------------------------------------------------------------------------------------------------------
CDN                                                                         // 静态资源 缓存   ->  图片、视频、音频...
------------------------------------------------------------------------------------------------------------------------

CDN是广义上缓存的一种
不过假设你现在所用的CDN出现问题了，你能做什么？


    你自己无力自建只能使用他人的，除了配置配置监控一下，真出问题了，猜测也只能打电话过去，其他无能为力。
    当然，关心还是需要的。

------------------------------------------------------------------------------------------------------------------------
配置CDN后，无论请求静态的还是动态的资源，都是流量先走CDN节点，然后动态资源，CDN再请求源站吗

    是的

------------------------------------------------------------------------------------------------------------------------
大神我想问下CDN缓存怎么更新呢？
比如商品的图片在cdn上面，但是某个时候后台运营人员对商品修改了图片，如果没有更新cdn信息的话，那用户访问的还是老的图片

    CDN家会提供一些删除和更新的接口的

------------------------------------------------------------------------------------------------------------------------
高效的抢购秒杀场景是下在秒杀前(几分钟)在CDN放一个很小的脚本(这个需要跟CDN厂商谈)，
逻辑是根据当前人数产生一个概率，只允许 x% 的人真正进入秒杀，其他的直接返回秒杀结束(当然对这部分人是不合理的)。
这样到后端的流量是我们可以接受的流量。


    可以在客户端做一些丢弃，或者lb上做一些限流策略

------------------------------------------------------------------------------------------------------------------------
1、CDN不是运维干的事情吗？作为程序员或架构师，只需要了解一下，没必要深入吧？
2、CDN命中率是厂商可以监控的到吗？

    1、嘿嘿，不是的，我们在做点播系统的时候非常关注CDN的数据
    2、厂商有提供，不过更多要靠我们自己来监控回源的信息

------------------------------------------------------------------------------------------------------------------------
自己保证可用的话 我的想法是多个厂商？ 或者自己做cdn节点管理 不可用的赶紧切到最近可用的cdn

    没错，要监控CDN的运行状态，有问题随时切换

------------------------------------------------------------------------------------------------------------------------
还可以通过网络运营商来提高可用性，电信用户走电信网络，网通用户走网通网络

------------------------------------------------------------------------------------------------------------------------
CDN回源是由CDN触发的还是用户触发的？具体过程是什么

    CDN触发，配置CDN的时候需要配置源站地址。

------------------------------------------------------------------------------------------------------------------------
视频类这种非静态的，如何利用cdn呢？

    视频是静态资源

------------------------------------------------------------------------------------------------------------------------
一个域名可以同时使用2个CDN厂家吗？

    可以的
    不过要在代码中控制用哪一家

------------------------------------------------------------------------------------------------------------------------
DNS对应多个IP地址的时候，这种情况APP该怎么缓存呢，该怎么保证它原来的轮训呢？

    可以缓存一个ip的列表

------------------------------------------------------------------------------------------------------------------------
CDN分发呢？同步静态资源到所有CDN节点上的？

    是的

------------------------------------------------------------------------------------------------------------------------
Cdn一般用第三方的就行了

    是的
    但是我们也要关注CDN的质量

------------------------------------------------------------------------------------------------------------------------
第三方CDN厂商提供我们的是域名还是IP地址?

    是域名
------------------------------------------------------------------------------------------------------------------------
最近正好在用CDN。用的腾讯云的！
我在公司访问某一个资源的时候，大部分情况能命中！
但是有时候即使资源没有变化，但是有概率还是会发生回源！不知道这种现象发生的原因是什么？怎么排查！


    是cdn数据过期了吗
    可以抓包看看请求包中的header信息

------------------------------------------------------------------------------------------------------------------------






========================================================================================================================
消息队列
========================================================================================================================

------------------------------------------------------------------------------------------------------------------------
秒杀场景
------------------------------------------------------------------------------------------------------------------------

秒杀场景，使用消息队列的话，怎么保证秒杀产品不超卖，这块计算逻辑是怎么处理的？


    其实方法无非有几种：
        1、使用锁的方式，比如分布式锁，也可以利用redis本身操作原子性的特点
        2、写入消息队列，在消息队列中做减库存的操作，做异步校验


------------------------------------------------------------------------------------------------------------------------
请教个问题：
就是前面同学提到的在秒杀场景一下保证产品不超卖方案用redis的原子性特点，
那么这个原子性在客户下单的时候用还是支付成功后才能用到？
如果客户下单的时候用了redis的原子性减库存了，一旦客户不支付后取消订单了，这时候怎么处理已减掉的库存问题呢？


    我理解是在下单的时候，如果不支付，一般可以设置一个定时器
    定时器时间一到，就把库存加上，同时定义订单失败

------------------------------------------------------------------------------------------------------------------------
据说，有一种处理秒杀方式，是随机让一部分用户直接失败   23333....

    你道破了天机...

------------------------------------------------------------------------------------------------------------------------
比如1000件商品，系统生成1000个令牌，拿到令牌的用户可以进入消息队列，其他未拿到令牌的直接返回已抢完，这种方式是否可以？

    可以的

------------------------------------------------------------------------------------------------------------------------
用户如何知道结果轮循查询，还是长连接通知？

    轮询查询 -> 耗费系统资源
    简单的思路是电商系统一般会支持  ->  系统通知功能 或 私信功能，可以给用户发一个私信

------------------------------------------------------------------------------------------------------------------------
秒杀场景使用了，消息队列，那么前端如何得获得秒杀结果呢？
消息队列的消费者，如何编写比较好，是一个死循环监听程序吗？


    1、可以使用产品的功能来通知，比如私信
    2、是的，是一个死循环的程序

------------------------------------------------------------------------------------------------------------------------
看过很多都是关于设计如何缓存，如何hash一致性分库分表等
但是没有见过讲解服务容器等需求，希望解答下...

比如说：
    10000QPS 需要多少台服务器？需要多少tomcat类似等容器等？
我的理解是：
    请求最终都会到达容器吧，容器扛不住，上层设计的完美好像也不能完全解决？？？


    这个要看业务的复杂度：
        业务比较轻量的话，单台服务器抗2000qps没有问题
        如果因为中有大量io请求，可能也就300-400qps，不能一概而论

------------------------------------------------------------------------------------------------------------------------
请教两个问题：
1、由于秒杀数量是有限的，那么假如商品只有1000，队列满1000后，其他请求是否应该直接拒流呢？
2、用户请求入队列后，为了及时反馈结果，前端会不断轮训查询结果。
    查询的读求情也会消耗资源，这部分是采用缓存策略还是结合读写访问不同业务机器来处理呢？


    两个思路：
        1、通过限流模块直接失败一部分请求；设定一个失败请求的比率，通过后再根据队列是否已满去做后续的操作；

        2、请求成功后，客户端建立ws连接，服务端通过ws连接将处理结果主动通知客户端。
            这里不建议采用轮询的方式，会给服务端造成压力
            ws只是一种方式，其他类似的服务端通知方案都可以考虑

------------------------------------------------------------------------------------------------------------------------
消息队列的长度如何设计，以什么为参考？如果请求数量超过了所有消息队列的长度，怎么处理，直接丢弃就可以了吗？

    一般消息队列中间件  ->  支持堆积  ->  也就是长度可以非常大

------------------------------------------------------------------------------------------------------------------------
若系统要将所有请求（如:接口的请求与返回报文）的日志入库,如何实现,提高性能且不影响交易？

    一般用类似es的方案






------------------------------------------------------------------------------------------------------------------------
消息幂等
------------------------------------------------------------------------------------------------------------------------

消息发送的三种语义：
    1、至少发送一次，存在重复发送，但不会丢消息
    2、之多发送一次，存在丢失消息，但不会重复发
    3、仅且发送一次，最理想情况，但是很难做到

所以，大部分消息中间件都会采用1，这样就会出现重复发生消息的风险，需要做幂等处理，做幂等处理就必须有全局唯一值。
    1、利用消息的全局唯一值来做处理，比如：消息的key
    2、利用业务的全局唯一值来做处理，比如：数据库的主键或唯一键

怎么处理？
    1、要么先查询，判断是否重复，然后再做处理
    2、要么利用存储系统的特点，吞掉重复异常，比如：DB
    3、或者加锁，加乐观锁，视具体业务来定


不过全局唯一值是少不了的
    具体是什么？存储在哪里？是先查还是吞异常也看具体业务。
    如果是数据库，先查再判断性能堪忧，最好采用唯一键，重复吞异常的方式。



------------------------------------------------------------------------------------------------------------------------
mq这块儿有一个很重要的方面没有设计，mq消息乱序的问题，想知道老师工作中是怎么处理这个问题的。

    有一个办法是 可以把相关的数据写入到同一个partition

------------------------------------------------------------------------------------------------------------------------
有一种场景
    消息发出后因为网络问题没有得到ack响应，此时服务挂掉
    重启之后内存中的消息就丢失了，无法完成消息队列客户端提供的重试机制，这种情况是不是就丢失消息了。

如果要解决这个问题，思路是在发消息前需要记录消息发送记录，发送完成后标记完成，重启服务后查看发送消息，确无响应的消息，进行重发。
但是我觉得这样是不是性能影响太大，不仅消费者需要日志检查，生产者也需要日志检查


    是有这种情况。
    不过因为有ack的机制，所以发送端是可以知道哪条消息有丢失的，如果发送端对要发送的消息有记录就好

------------------------------------------------------------------------------------------------------------------------

处理方式：

    网络抖动处理              重发
    消息队列服务器宕机         集群
    消息重复                  使用唯一ID，保证消息唯一性

------------------------------------------------------------------------------------------------------------------------

联想到MongoDB在写策略中有w和j两个参数：
    w对应 ->  同步多个从节点
    j是   ->  刷journal到磁盘

    看来存储系统的技术都差不多。一般设置w=majority就可以，j=false。

跟kafka中老师的建议一样。
Redis中也有AOF，不同存储系统解决问题不一样，但共性还是很多的。


    因为都要保证  ->  高性能、高可用、数据一致性

    只是每个存储系统侧重点不一样：
        Kafka是             写性能
        Redis是             读性能
        普通关系数据库是      事务（一致性）

------------------------------------------------------------------------------------------------------------------------
我们目前是在消费消息后，将消息id（业务上定义的唯一标识）放入redis
消费前，先去redis查找，也算是业务上的一种防重复吧

    这个也是的


------------------------------------------------------------------------------------------------------------------------
清除Redis中的 msg_id：

1、check

    我们之前使用rocketmq消费订单信息时也是通过redis校验msgID是否存在。
    保持在redis中的msg_id不设置超时时间，每天我们有一个和商城（生产者）的对账任务，如果对账没有问题，再将msg_id删除


2、合理 过期时间

    自动del


------------------------------------------------------------------------------------------------------------------------
消费端 消息处理的幂等性：

    1、增加去重表（通用）

    2、根据业务数据状态来判断（例如：订单支付后变更状态为已支付，如果订单当前状态已经为已支付则忽略此消息）


------------------------------------------------------------------------------------------------------------------------
我们在生产环境中，为了避免重复消费使用了全局唯一ID的方式，每次业务逻辑前都会从库中查一下。

但是会出现两条消息瞬时并发处理问题，这时事务都没提交所以都查不到。

    这时可以用老师说的 版本乐观锁来解决，我们目前的方式是 增加了分布式锁


------------------------------------------------------------------------------------------------------------------------
关于幂等性方案，上面先查后写操作，也不是绝对的，会有并发问题
同时多个线程，一个正在进行插入操作，一个正在进行查询操作，正好查不到，会有两条重复数据，当然概率比较小

解决方法：
    1、有没有业务数据加唯一索引，插入失败抛异常，异常可能会MQ重试，所以还需要catch异常处理
    2、插入加入写锁，for update 影响会比较大
    3、当有唯一索引实，insert ignore 忽略重复插入问题，replace into 和 insert ... on duplicate key update，都是替换原有的重复数据



------------------------------------------------------------------------------------------------------------------------
推荐使用多副本而不是每次刷盘，我不太理解。
难道每次都刷盘（flush），性能应该比每次都要多次网络调用要强得多啊（备份同步）

    从性能数据看，网络调用耗时要比磁盘写入耗时低

------------------------------------------------------------------------------------------------------------------------
如果消息在处理之后，还没有来得及写入数据库，消费者宕机了重启之后发现数据库中并没有这条消息，还是会重复执行两次消费逻辑
这时你就需要引入事务机制，保证消息处理和写入数据库必须同时成功或者同时失败，但是这样消息处理的成本就更高了。


    基本上还是分布式事务，比如两阶段提交的办法

------------------------------------------------------------------------------------------------------------------------
生产者判重          交给消息中间件自行处理，加判重表。
消费端的重复消费    通过分布式锁控制，过期时间可以放长些。


------------------------------------------------------------------------------------------------------------------------
消息队列会满吗？满了怎么办

    一般消息队列会支持堆积，如果堆积太多，可能会写入失败，也可能会挂，看你用的什么队列

------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------
消息延迟                                                                                    // 提升消息队列的性能
------------------------------------------------------------------------------------------------------------------------

总结

我们使用消息队列，一般会关注消息的延迟，然后优化消息的读写性能，但首先要做的是对延迟的监控，那怎么监控消息队列的延迟呢？

    1、通过消息队列的工具，来监控消息的堆积情况
        一般消息队列都会提供这种工具，拿kafka来说，可以通过 kafka-consumer-groups.sh 这个命令来监控堆积情况。
        主要关注【lag】列
        同时kafka会通过JMX暴露消息的堆积情况，所以可以使用jconsole这种工具进行查看

    2、通过生成监控消息的方式来监控延迟情况

        这种方式就是启动一个监控程序，并生产一种特殊的消息

        比如是消息生成的时间戳，然后循环的写入到消息队列中，真正的消费者对于这种特殊的消息不进行处理
        而监控程序会处理这种特殊的消息，通过比较 消费到该消息的时间 和 生成该消息的时间差 来判断是否有延迟

那如何减少消息的延迟呢？
    1、优化消费端的代码以提升性能
    2、增加消费者的数量

    关于第2点对消息队列会有限制
        比如对于kafka来说，在同一个分区下，消息只能被一个消费者【同一个consumer_group】消费
        这样也是考虑到了多个消费者消费需要抢占锁，会有性能损失

        【topic的分区数量决定了消费者的并行度】
            所以想提高并行度就得增加分区，那如何不增加分区呢？

            可以在处理消息上做工作了，比如可以在一个消费者中增加处理消息的并行度   ->   比如多线程的方式去处理，或者加入异步处理

消费消息时 消费线程的 空转问题

    消费端通过不断轮训去topic中拉取消息，就存在线程空转的情况

    这里可以增加一个策略就是拉取不到消息，等待一会儿        // 20ms~100ms

    等待时间可以是固定的，也可以是阶梯的

    当然这样可能加重消息消费延迟情况



消息队列本身 读取性能提升：
    1、消息的存储         ->  存储介质
    2、零拷贝技术


------------------------------------------------------------------------------------------------------------------------
sendfile 在linux 2.1的时候

    是  文件 与 socket两个内核缓冲区   之间 copy

在更高linux版本

    已经能从  文件缓冲区   ->  直接写到 网卡      // socket缓冲区 省掉了


------------------------------------------------------------------------------------------------------------------------
消息的顺序性
    比如 订单消息 要在 配送消息  之前消费

对于这种问题，由因果一致性来解决。
因果一致性的一种做法是：消息的编号递增，小的号码先消费，大的号码要在后消费。

但是在分布式集群环境下，每个消费者消费的时候要把它的最大消息编号广播给其他消费者。
这样一来的话，代价就比较高了。


    比如在Kafka下，把要求严格顺序的消息 写到一个partition


------------------------------------------------------------------------------------------------------------------------
消息顺序性问题
    rabbitmq   一个队列 对应  一个消费者消费
    kafka      相同key的消息放到同一个partition中，使用同一个内存队列进行消费

    还要保证消费端是单线程的

------------------------------------------------------------------------------------------------------------------------
并行消费那块有数据丢失的风险吧
server接收到数据返回给队列ack，然后丢给线程池，数据还没处理完这个节点挂掉了，数据不就全丢了。

    会有这个风险，不过为了提升性能，需要一些权衡

------------------------------------------------------------------------------------------------------------------------
数据库数据本身也是属于磁盘数据啊，为什么在存储的时候将数据库换成本地磁盘，qps会提高一个数量级呢？

    数据库是随机写，磁盘是顺序写

------------------------------------------------------------------------------------------------------------------------
MQ的一个核心卖点就是削峰填谷
    一般来讲生产和消费必然存在速度差，而且常常把慢动作放在消费侧，有短暂积压应该是正常现象
    如果长时间有大量积压，那就是消费侧过慢了，需要优化

具体怎么优化，也需要看具体性能瓶颈在哪里
    不过核心思路在于，有性能瓶颈的地方最好可以通过水平扩展的方式来解决
    否则就只能沿着网线找性能瓶颈点，然后把具体瓶颈给优化掉啦！
    一般应用是IO密集型，找有IO的地方基本能找到性能瓶颈。

------------------------------------------------------------------------------------------------------------------------
Kafka 通过 JMX 暴露了消息堆积的数据，我在本地启动了一个 console consumer，然后使用 jconsole 连接 consumer 就可以看到 consumer 的堆积数据了
意思是说，生产环境Linux服务器上的kafka服务，我可以在本地写一个consumer订阅broker，然后能通过jconsole可以看到kafka里的消息队列情况？

    可能生产环境不好用jconsole，不过可以写代码获取jmx的数据

------------------------------------------------------------------------------------------------------------------------
消息处理时 避免耗时操作 也很重要

    是的，必要时可以做 多级队列

------------------------------------------------------------------------------------------------------------------------
我们用的ActiveMQ隔一段时间后会出现，生产后消费时隔半个小时到一个小时的延迟，并大量也不大，这个怎么检查能找到根源？


    可以监控一下写入消息和处理消息的速率，看看是不是写入消息量级过大还来不及处理，还是说处理的慢了

    如果处理的慢了，那么要排查处理的链路上哪处是瓶颈


------------------------------------------------------------------------------------------------------------------------
请问怎么采集每个consumer的JMX数据呢，让监控程序去连接每一个consumer应用吗？

    是的

------------------------------------------------------------------------------------------------------------------------
请教老师kafka新版本offset通过主题方式替换zk，能提升多大的吞吐量，有没有量化的数据参考


    李玥老师的消息队列课程里做过极限压测

        在一台配置比较高的服务器上，部署单节点Kafka

        极限情况下   ->   每秒数据吞吐量高达600M，几乎跑满带宽

------------------------------------------------------------------------------------------------------------------------
消息中间件的读写都是顺序的吗？
写文件的时候会先写到Page Cache中，然后往文件写，写文件的时候应该是一行一行写的吧？
如果不写入Page Cache直接写入文件，也是一行一行写的吧，有什么区别吗？
比如说写到文件的第1万行了，再往后继续写，是可以快速定位到这个位置的吗？
读取消息的时候也是读的文件，假如说读到了第1万行，再往后读，也是可以很快定位到这个1万行吗？
Java有没有API，比如说，我直接读取第1万行的数据？


    写文件   ->   一般都会写入 Page Cache   ->   由操作系统 来刷盘


------------------------------------------------------------------------------------------------------------------------
零拷贝时，从内核缓冲区到socket缓冲区是否可以通过共享缓冲区的方式，再减少一次拷贝？谢谢！

    可以将描述符传输到socket缓冲区，这样可以减少一次拷贝


------------------------------------------------------------------------------------------------------------------------
生成 特殊消息 做监测

这里有个问题，你这么做是问了判断当前队列的延迟时间对吧
但是这样子做的话定时生成的时间戳和正式的任务混在一起了，这样不就给正常执行的业务处理程序增加了很多负担
会不会导致太多的这种任务导致对于正式的业务延迟更大？


    不会的
    这样比如1分钟发送一个消息，相比于业务动辄每秒几十或者几百次请求来说太微不足道了

------------------------------------------------------------------------------------------------------------------------
经过排查发现客户端发的消息，服务器收的比较晚，造成超时，双方之间tcp socket通讯局域网，这种情况怎样解决啊？

    局域网应该不会有大量超时
    可以抓包看一下，也可以看看网卡是否有丢包

------------------------------------------------------------------------------------------------------------------------
kafka的消费者端也是通过轮询的形式拉取消息？kafka是否也会出现空转的情况？

    kafka本身不会空转
    因为kafka消费端是拉模式，所以kafka是接受读取请求的

------------------------------------------------------------------------------------------------------------------------


------------------------------------------------------------------------------------------------------------------------




------------------------------------------------------------------------------------------------------------------------














========================================================================================================================
分布式服务
========================================================================================================================


------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------





========================================================================================================================
维护
========================================================================================================================



------------------------------------------------------------------------------------------------------------------------


------------------------------------------------------------------------------------------------------------------------





========================================================================================================================
实战
========================================================================================================================



------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------