========================================================================================================================
基础
========================================================================================================================



========================================================================================================================
数据库
========================================================================================================================



------------------------------------------------------------------------------------------------------------------------
NoSQL
------------------------------------------------------------------------------------------------------------------------
之前单表热点数据接近亿级，查询时间达到了8秒左右

    后来进行了分表，按照ID取模分了一百张表，历史数据取模插入到分表中
    新建了一张表用来保存全局唯一ID，每次新建热点都会更新全局唯一ID，保证分表之后ID唯一性

    查询使用es
    GitHub上找了一个开源的MySQL数据同步到es的工具，模拟的从库，保证了数据同步的实时性

    热点数据查询性能降低到了1秒内
    性能得到很大提升

------------------------------------------------------------------------------------------------------------------------
业务中有个模块写极多读少，这种情况下是不是直接把这个数据拆分出来用单独nosql存储比较好？还是先写到nosql，再慢写到mysql好？
如果慢写到mysql
一是可能会出现数据不一致问题，
二是写请求会积累很多，内存型nosql支撑不住，可能要用leveldb之类的，磁盘多了一份数据，等要迁移的时候又增加了运维管理成本

    如果是长期都是写多读少，那么可以考虑nosql
    如果是瞬时峰值的话，还是用消息队列削峰填谷


------------------------------------------------------------------------------------------------------------------------




------------------------------------------------------------------------------------------------------------------------








========================================================================================================================
缓存
========================================================================================================================

------------------------------------------------------------------------------------------------------------------------
数据查询瓶颈
------------------------------------------------------------------------------------------------------------------------

缓存目前是标配之一（互联网开发三剑客：RPC/MQ/REDIS），凡是需要提速的地方，也许缓存就能排上用场，至少缓存的思想必然会被用上。

好处：服务提速
坏处：数据不一致风险，引入复杂度。

        原则，简单优先，能不用就不用，实在不行就需要好好考虑一番了

        缓存穿透怎么解决？缓存击穿怎么解决？缓存雪崩怎么解决？
        数据不一致性问题怎么解决？数据结构众多怎么选择合适的数据结构？
        缓存的key：value怎么设计？缓存怎么加载？过期时间怎么设置？补偿机制怎么设计？
        缓存具体选择什么方案？需不需要多层缓存？多层缓存的复杂度怎么控制？

    不过这些对于面试用处不大，面试会问各种底层结构？以及怎么优化的？怎么选择某种数据结构的？所有的一切，都是为了高性能而存在。


    是的
    缓存使用简单，但是深入难

------------------------------------------------------------------------------------------------------------------------
没有达到需要引用缓存需要的情况下，尽量不要过早使用缓存。
    缓存的坑很多，并且维护成本极高。在处理缓存的适合需要多考虑很多问题。

曾经碰到这样的情况：
    调用别人写的查询服务，但是查找到的数据却迟迟无法更新为最新数据。最后，重新写了直接查库的接口，才解决问题。

并且，缓存如果频繁更新，频繁失效 反而会带来性能的消耗。

再带上杨晓峰老师的一句话：“过早的优化是万恶之源"


    是的

------------------------------------------------------------------------------------------------------------------------
方便面那个比喻好评
缓存和缓冲区对应的英语是cache和buffer

    buffer的存在  ->  是为了解决 数据不能一次性读写完成，或某次的数据量太小 io成本又太高， 的折中方案


------------------------------------------------------------------------------------------------------------------------
老师，热点本地缓存使用组件 Guava Cache ，这个东西能存多大量呢，感觉像一个数据库

    guava cache本身没有限制，注意看存大量是否对gc有影响

------------------------------------------------------------------------------------------------------------------------
像股票之类的app页面数据实时刷新，这个是怎么做到的，是否用了缓存如何使用的缓存呢，希望老师能给解答，谢谢~


    股票的话，应该有分布式缓存，但是这个缓存更新频率高，需要用队列削峰填谷

    还有一点：

        相对来说，股票数量级  ->    是非常非常小  且固定的          // A股 4000只    美股 7000只


------------------------------------------------------------------------------------------------------------------------
热点缓存是存在本地内存之中吗
后台的列表数据有很多查询条件还有分页这种，能用缓存吗，如果能用，有什么好的缓存方案吗

    有做过这种keylist的存储
    一般要么缓存整体，要么缓存前几页的热点数据



------------------------------------------------------------------------------------------------------------------------
缓存的读写策略
------------------------------------------------------------------------------------------------------------------------


小结一下：
    1、用缓存目的是为了提速，之所以能提速 -> 关键在于使用内存来存储

        不过内存的特点是 -> 掉电后时会丢数据
        所以，一份数据会放在 DB和缓存 两个地方
        那么对于变化频繁的数据 -> 就会存在数据不一致的问题

        缓存最适合的是  ->  静态资源 或 变化低频的资源

    2、后面这个那个策略，都是为了解决变化频繁的数据的数据一致性问题的
        在解决缓存一致性问题时会引入别的问题，比如：性能问题，复杂度问题。

    3、具体怎么权衡看业务场景，不过数据一致性问题在分布式环境中是很经典和头疼的问题
        因为缓存数据会引入，别的情况也会引入，比如：主从延迟

------------------------------------------------------------------------------------------------------------------------
缓存  ->  一定会引入不一致

    所以解决的办法 需要权衡一致性和性能

------------------------------------------------------------------------------------------------------------------------
文中提到的第一个第一个缓存和数据不一致的问题，
我认为这个问题的原因是，多个客户端更新缓存和数据库之间是无序的、并发的操作，这样必然导致数据不一致的问题，
因此我们采用了监听binlog的方式，把Binlog扔到消息队列中
由一个leader来消费，负责更新缓存，保证了写缓存操作之间的顺序性，保证了缓存的准确性，避免了频繁读库。


    这样确实是一个比较好的方式，只是会稍微复杂

    监听binlong，可以使用阿里开源 Canal


------------------------------------------------------------------------------------------------------------------------
我理解WriteBack策略 相当于缓存和缓冲区合二为一了
据我所知，MySQL 的buffer pool 使用了WriteBack策略，
但为了防止系统崩溃后数据丢失，MySQL使用了WAL（Write-Ahead Logging）机制，写先日志。
好像WAL在HBase等系统也在用

    是的


------------------------------------------------------------------------------------------------------------------------
write back策略其实不算 数据库和cache 之间的策略，
而是计算机体系结构中的策略，比如：磁盘文件的缓存

它的完整读策略是这样的：
    如果缓存命中，则直接返回；
    如果缓存不命中，则重新找一个缓存块，如果这个缓存块是脏的，那么写入后端存储，并且把后端存储中的数据加载到缓存中；
    如果不是脏的，那么就把后端存储中的数据加载到缓存，然后标记缓存非脏。



------------------------------------------------------------------------------------------------------------------------
Cache Aside对缓存命中率两种解决方案中的1,可能是我没看懂，感觉没解决问题啊？

这里说在“更新数据时 也更新缓存”

我理解就是：
    先更新DB再更新缓存，这样除非在更新DB之前加分布式锁
    否则在更新DB之后加分布式锁，再更新缓存，依然较高可能出现不一致的情况。

实际中
    我们确实用在更新缓存时 用分布式锁或本地锁，只不过是发现缓存为空而去读DB时，为了解决穿透问题。

纯个人见解：
    除了cache aside，另外两种 -> 更贴近底层系统开发、而不是商业应用开发

因为我们大多数人做的系统
    都是低速存储都是数据库
    是有复杂的业务逻辑约束的，比如唯一性等，不是那种简单的 Page/CPU Cache

我们经常的写操作
    一般都要借助数据库来检验这些约束 并且在出错之后返回给用户。
    而如果直接与缓存打交道，且不论有些缓存的实现并不保证数据可靠性，也不能依靠缓存检验这些约束。

其实现在很多系统用的一种缓存模式是：

    类似CQRS
        写直接修改DB，异步更新到缓存
        读只从缓存读数据

    适合 对数据不一致窗口 可以容忍的场景



    1、是在更新数据库前加锁，锁的粒度是大了一些
    2、确实是更偏重底层开发

------------------------------------------------------------------------------------------------------------------------
Cache Aside（旁路缓存）策略，对于读多写少场景，当一个写操作更新db后同时删除缓存。然后多个读就会回源，这不会造成db压力么？

    会的
    这就是狗桩效应嘛~


------------------------------------------------------------------------------------------------------------------------
使用写回策略，如果在缓存更新到数据库之前设备掉电了，那这样数据岂不是丢失了，请问这是怎么解决的呢，通过主备机制吗，缓存数据写两份？

    是有这个问题
    比如 Page Cache在机器掉电之后就都是数据了

    一个办法是 将写入缓存的操作写入log里，类似lsm树的 write ahead log


------------------------------------------------------------------------------------------------------------------------
主从延迟场景

    其实如果是使用 Cache Aside方式的话。
    在写的时候时候因为更新数据后，删除了缓存

    在高并发情况下。那么可能会出现以下情况：

        主从同步的情况下，从库没来得及同步
        大量的读请求返回的是从库的旧数据，而这个时候读的数据会被动写入缓存

        那就存在很大的问题！这种应该怎么处理！
        如果是这样的话？是不是只能依靠分布式锁来实现了！



    是的
    这样只能更新缓存，然后使用 分布式锁来控制


------------------------------------------------------------------------------------------------------------------------
在cache aside策略中，如果先更新数据库，再删除缓存。
这样如果读请求访问量很大，会短时间出现大量请求穿透到数据库，这里有好的办法优化吗？


    如果更新不频繁的话，其实还OK

    如果更新频繁
        可以加分布式锁，让单一线程可以更新这条数据；
        或者设置短的过期时间，让可能出现的不一致数据尽快过期





------------------------------------------------------------------------------------------------------------------------
缓存的高可用
------------------------------------------------------------------------------------------------------------------------

高可用的设计思路  ->  没有其他的   ==>   核心就是  ->  增加副本

    针对数据  ->  就增加数据副本
    针对服务  ->  就增加服务副本
    针对机房  ->  就增加机房副本


增加副本引入的新问题是  ==>  数据不一致性


下面各种算法什么的，都是为了解决：

    1、因增加副本     ->   而带来的  数据不一致性问题

    2、或者 节点挂了  ->   怎么使服务 继续可用的策略

        比如：
            数据怎么迁移？故障怎么隔离？故障节点恢复后怎么是否加入？怎么加入？

    ------------------------------------------------------------------------
    最近在看火影：

        影分术  ->  就是鸣人的高可用方式，其他的高可用思路和这个如出一辙

        从动漫中也可以看出  ->  这个需要更多能量

        对公司而言  ->  需要更多机器和存储空间，技术复杂度也会增加一些

    幸好有现成的组件，避免人人都重复造轮子的资源浪费。


------------------------------------------------------------------------------------------------------------------------
关于客户端模式:

    1、客户端模式是指用户代码的服务端(以我们的电商系统为例)吗?
        是的，对于缓存来说，应用就是它的客户端

    2、客户端模式的缓存存放在何处?
        单独部署


------------------------------------------------------------------------------------------------------------------------
Client(客户端) 和 Server(服务端) 的理解：

    老实说一开始我也一脸懵逼，以为 客户端就是用户端


    但是后来想通了：

        应用服务器 为 用户 提供数据接口
            用户就是客户端，应用就是服务端


        但是 缓存 为 应用服务器 提供缓存服务

            这时候 对于缓存服务器来说   ==>   应用服务器  ->  就是客户端  ， 而缓存  ->  就是服务端


------------------------------------------------------------------------------------------------------------------------
主从会有延迟，写入主库，但延迟同步到从库，在同步完成前去从库读数据，读不到，这如何解决呢？

    1、写入的时候更新缓存，这样从缓存里面读就实时了
    2、直接读主库


------------------------------------------------------------------------------------------------------------------------
您之前说4核8G的机器上，MySQL最高支撑QPS为1万，怎样本文开头又说MySQL读峰值才1500/s呢

    1万是基准测试的结果，在实际中sql更复杂，达不到这个性能

------------------------------------------------------------------------------------------------------------------------
多副本你可以认为是缓存的缓存，也就是在缓存之上再加一组缓存，可以解决单组缓存的带宽瓶颈

而在实践过程中，因为大部分流量都会被副本承担，master和slave的数据有可能会变得不热，所以可以把master和slave当做副本


------------------------------------------------------------------------------------------------------------------------
缓存和数据库的同步机制，有没有开源的框架实现呢？

    有一些 从DB 同步数据到cache  的组件

------------------------------------------------------------------------------------------------------------------------
一致性哈希算法的不同实现
    1、哈希环法
    2、google的jump consistent hash
    3、Maglev一致性哈希法

------------------------------------------------------------------------------------------------------------------------
关于一致性hash有几点疑问，请老师解答：
1、为了防止hash环的倾斜，由实际节点虚拟出来的一部分虚拟节点 是如何保证虚拟节点能均匀排列呢？还是说增加虚拟节点的算法 可以自定义虚拟节点插入的位置呢？
2、假设一个真实节点宕机了，那是不是那个真实节点相对应的虚拟节点也“宕机”(不可用)了？
3、一致性hash算法在java里或者.net里面有现成的实现吗？若有是对应那个类呢？


1、从概率学上保证，如果你虚拟出来的节点足够多，你无法保证绝对的均匀
2、是的 虚拟节点也宕机了
3、貌似是没有的，不过在一些 memcached客户端代码 中都会有实现




------------------------------------------------------------------------------------------------------------------------
缓存穿透
------------------------------------------------------------------------------------------------------------------------

解决缓存穿透的方案：

    1、回种空值
        是一种最常见的解决思路，实现起来也最简单，如果评估空值缓存占据的缓存空间可以接受，那么可以优先使用这种方案

    2、布隆过滤器

        会引入一个新的组件，也会引入一些开发上的复杂度和运维上的成本

        所以只有在存在海量查询数据库中，不存在数据的请求时才会使用，在使用时也要关注布隆过滤器对内存空间的消耗


    3、对于极热点缓存数据穿透造成的“狗桩效应”

        可以通过设置分布式锁 或者 后台线程定时加载的方式来解决


------------------------------------------------------------------------------------------------------------------------
核心思想

    数据库是一个脆弱的资源，它无论是在扩展性、性能还是承担并发的能力上，相比缓存都处于绝对的劣势

    所以我们解决缓存穿透问题的核心目标   ->   在于 减少对于数据库的并发请求


------------------------------------------------------------------------------------------------------------------------
那么这个过滤器在微服务环境下如何部署比较好？部署好了以后是否也可以起到防止洪水攻击带来的缓存穿透问题？

    以单独服务部署比较好
    我之前团队曾经改了一下redis源码来实现这个功能，利用redis的存储机制

------------------------------------------------------------------------------------------------------------------------
布隆过滤器可以用在一些资讯app的新闻展示中，给用户推送新的资讯用来过滤掉那些用户已经浏览过的记录

    是的

------------------------------------------------------------------------------------------------------------------------
缓存穿透跟缓存雪崩是一个概念么？如果不是那缓存雪崩的解决方案是什么呢？谢谢

    穿透  指的是缓存中没有数据，需要到数据库中去取；

    雪崩  指的是一个缓存节点的故障导致全局故障，两者是不同的
          之前提的一致性hash中实现虚拟节点就是一种避免雪崩的方法

------------------------------------------------------------------------------------------------------------------------
分布式锁和一般锁的有什么区别呢？

    一般的锁  是进程中的锁，可以同步一个进程中的多个线程；
    分布式锁  可以同步多个进程

------------------------------------------------------------------------------------------------------------------------
缓存适合存放什么样子的数据呢？还是数据库里面的所有数据都可以放入缓存呢？

    缓存适合放经常访问的热数据，不能放全量数据，而且也放不下

------------------------------------------------------------------------------------------------------------------------
bloom filter这个用法会在各种文章教程中提出
但我一直有个疑问想求教一下：基于redis这种存储的bloom filter有没有成熟的方案？
    因为我觉得毕竟redis不是一种完全可靠的存储，一旦crash理论上有可能丢数据
    在用户的那个案例中，一旦应该出现在bloom filter中的数据丢失了，就意味着永远也查不出这个用户来了。
    那是否我们还应该启动一个监控进程，一旦发现redis crash了，要重新构建bloom filter呢？

回到思考题，我现在觉得文章中的三个方案都只能解决部分场景的问题，有时候需要配合使用。
    除此之外，合理的数据库连接池大小以及服务限流也能起到最后防线的作用吧？


    bloom filter需要自研，可以基于redis持久化存储到硬盘上

    连接池应该解决不了问题，因为链接不是无限的；限流是有损的


------------------------------------------------------------------------------------------------------------------------
分布式锁的方案中，有问题吧？如果第二个线程发现有这个key说明有别的线程在加载数据，但是还没有加载完，这个时候读缓存是没有的。

    可以返回失败 不从数据库读取，或者重试


------------------------------------------------------------------------------------------------------------------------
布隆过滤器单独部署服务，服务启动时需要初始化数据，将数据库中数据初始化到过滤器中。后续将布隆过滤器定期写到磁盘中，防止服务重启导致丢失。
请教下，文中说的布隆过滤器是否如上上面说部署？

    是的






------------------------------------------------------------------------------------------------------------------------
CDN                                                                         // 静态资源 缓存   ->  图片、视频、音频...
------------------------------------------------------------------------------------------------------------------------

CDN是广义上缓存的一种
不过假设你现在所用的CDN出现问题了，你能做什么？


    你自己无力自建只能使用他人的，除了配置配置监控一下，真出问题了，猜测也只能打电话过去，其他无能为力。
    当然，关心还是需要的。

------------------------------------------------------------------------------------------------------------------------
配置CDN后，无论请求静态的还是动态的资源，都是流量先走CDN节点，然后动态资源，CDN再请求源站吗

    是的

------------------------------------------------------------------------------------------------------------------------
大神我想问下CDN缓存怎么更新呢？
比如商品的图片在cdn上面，但是某个时候后台运营人员对商品修改了图片，如果没有更新cdn信息的话，那用户访问的还是老的图片

    CDN家会提供一些删除和更新的接口的

------------------------------------------------------------------------------------------------------------------------
高效的抢购秒杀场景是下在秒杀前(几分钟)在CDN放一个很小的脚本(这个需要跟CDN厂商谈)，
逻辑是根据当前人数产生一个概率，只允许 x% 的人真正进入秒杀，其他的直接返回秒杀结束(当然对这部分人是不合理的)。
这样到后端的流量是我们可以接受的流量。


    可以在客户端做一些丢弃，或者lb上做一些限流策略

------------------------------------------------------------------------------------------------------------------------
1、CDN不是运维干的事情吗？作为程序员或架构师，只需要了解一下，没必要深入吧？
2、CDN命中率是厂商可以监控的到吗？

    1、嘿嘿，不是的，我们在做点播系统的时候非常关注CDN的数据
    2、厂商有提供，不过更多要靠我们自己来监控回源的信息

------------------------------------------------------------------------------------------------------------------------
自己保证可用的话 我的想法是多个厂商？ 或者自己做cdn节点管理 不可用的赶紧切到最近可用的cdn

    没错，要监控CDN的运行状态，有问题随时切换

------------------------------------------------------------------------------------------------------------------------
还可以通过网络运营商来提高可用性，电信用户走电信网络，网通用户走网通网络

------------------------------------------------------------------------------------------------------------------------
CDN回源是由CDN触发的还是用户触发的？具体过程是什么

    CDN触发，配置CDN的时候需要配置源站地址。

------------------------------------------------------------------------------------------------------------------------
视频类这种非静态的，如何利用cdn呢？

    视频是静态资源

------------------------------------------------------------------------------------------------------------------------
一个域名可以同时使用2个CDN厂家吗？

    可以的
    不过要在代码中控制用哪一家

------------------------------------------------------------------------------------------------------------------------
DNS对应多个IP地址的时候，这种情况APP该怎么缓存呢，该怎么保证它原来的轮训呢？

    可以缓存一个ip的列表

------------------------------------------------------------------------------------------------------------------------
CDN分发呢？同步静态资源到所有CDN节点上的？

    是的

------------------------------------------------------------------------------------------------------------------------
Cdn一般用第三方的就行了

    是的
    但是我们也要关注CDN的质量

------------------------------------------------------------------------------------------------------------------------
第三方CDN厂商提供我们的是域名还是IP地址?

    是域名
------------------------------------------------------------------------------------------------------------------------
最近正好在用CDN。用的腾讯云的！
我在公司访问某一个资源的时候，大部分情况能命中！
但是有时候即使资源没有变化，但是有概率还是会发生回源！不知道这种现象发生的原因是什么？怎么排查！


    是cdn数据过期了吗
    可以抓包看看请求包中的header信息

------------------------------------------------------------------------------------------------------------------------



========================================================================================================================
消息队列
========================================================================================================================


------------------------------------------------------------------------------------------------------------------------


------------------------------------------------------------------------------------------------------------------------





========================================================================================================================
分布式服务
========================================================================================================================


------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------





========================================================================================================================
维护
========================================================================================================================



------------------------------------------------------------------------------------------------------------------------


------------------------------------------------------------------------------------------------------------------------





========================================================================================================================
实战
========================================================================================================================



------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------