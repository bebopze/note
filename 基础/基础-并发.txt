========================================================================================================================
java 基础，jdk 中常用集合源码,线程池,aqs 等 concurrent 工具类
========================================================================================================================

1、Collection

    List    //  有序（存取 顺序一致）

            ArrayList              数组

            LinkedList             链表

            Vector                 数组                           synchronized


    Set     // 无序 + 去重


            HashSet                散列表（HashMap）

            TreeSet                红黑树（TreeMap）


    Map     // 无序 + 去重


            HashMap                散列表 + 链表/红黑树

            TreeMap                红黑树

            Hashtable              散列表 + 链表                   synchronized


    Queue   // 有序 -> 先进先出


        单端队列

            ArrayBlockingQueue          数组（有界）

            LinkedBlockingQueue         链表（无界）

            PriorityBlockingQueue       数组（无界 优先队列）

        双端队列

            Deque




========================================================================================================================
并发理论、JVM内存模型
========================================================================================================================

------------------------------------------------------------------------------------------------------------------------
1、并发编程 bug 的 源头

    1、可见性       // 共享变量

        缓存

            均衡 CPU 与 内存 的 速度差


            导致新bug：

                不同线程 操作的是 各自的CPU缓存

    2、有序性

        编译优化
            指令重排 -> 提高 缓存 利用率


            导致新bug：

                拿到 未初始化完成 的 对象

    3、原子性

        线程切换

            分时复⽤ CPU，均衡 CPU 与 I/O设备 的速度差

            导致新bug：

                1条 高级语言 语句 = N条 CPU 指令

                CPU 执行单位 -->  1条 CPU 指令

                操作系统 做 线程切换，可以发⽣在 任何⼀条 CPU指令 执⾏完



    -- 再次证明：没有完美的技术，引入一门新的技术 来解决问题的同时，必定会带来新的问题


------------------------------------------------------------------------------------------------------------------------
2、Java 的 解决方案
------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------------------
1、Java内存模型                                        // 可见性 、 有序性

    Java 中 提出了 JMM（Java内存模型）解决方案

        来解决 可见性 和 有序性 问题

            -- 再次证明：计算机科学领域 的 任何问题 都可以通过 增加一个间接的中间层 来解决

                - Java 的 内存模型 是 并发编程领域 的 ⼀次重要创新

    JMM的 功能：

        按需 禁用 缓存(可见性) 和 编译优化(有序性)

    提供的 工具：

        volatile、synchronized 和 final 三个关键字，Happens-Before 规则



        Happens-Before 规则：                  // 可见性❗❗❗

            1、Happens-Before 语义 的 现实理解：

                如果 A事件 是导致 B事件 的 起因，那么 A事件 ⼀定是先于（Happens-Before） B事件 发⽣的


            2、在 Java 语⾔⾥⾯：

                Happens-Before 的语义：

                    本质上是一种  可见性❗❗❗


                A Happens-Before B

                    意味着 A事件 对 B事件 来说是可⻅的，⽆论 A事件 和 B事件 是否发⽣在同⼀个线程⾥

                    例如 A事件 发⽣在 线程1 上，B事件 发⽣在 线程2 上，Happens-Before 规则保证 线程2 上也能看到	A事件 的发⽣




        1、volatile：             // 禁用 缓存、编译优化

            1、原始语义：禁用 缓存                                        // 禁用 缓存

                强制 CPU缓存 刷新至 主内存，并强制 使 其他CPU的缓存 失效，使用时 从主内存 重新读取.


            2、Java 内存模型 在 1.5 版本对 volatile 语义进⾏了增强         // 禁用 缓存、编译优化

                Happens-Before 规则 增强语义

                    对⼀个 volatile变量 的 写操作， Happens-Before 于 后续对这个 volatile变量 的 读操作   // 可见、顺序

                        ==>  volatile变量 - 写 Happens-Before 后续 读


        2、synchronized          // 禁用 缓存、编译优化                       - https://blog.csdn.net/qq_30118563/article/details/90106667

            1、原始语义：
                互斥锁

            2、Happens-Before 规则 增强语义

                对 一个锁 的 解锁 Happens-Before 于 后续对 这个锁 的 加锁            // 可见、顺序

                // 保证了执行顺序、同时保证了 可见性


                语义类似 volatile

                    锁释放、获取锁 的 内存语义：
                        释放 锁 时，会将 持锁线程 的 工作内存的共享变量值 刷到 主内存
                        获取 锁 时，JMM 将 持锁线程的 工作内存 置为无效，临界区代码 必须从 主内存中 读共享变量

                        锁释放 与 volatile写，锁获取 与 volatile读 有相同内存语义！！！

                    小结：
                        A释放锁    ->    即 A 向后来将拿锁的 线程B 发送 一个消息
                        B获取锁    ->    即 B 接收了 之前释放锁的 线程A 的一个消息
                        A释放锁、B获取锁，其实就是 A向B 发送个消息



                    synchronized关键字的内存语义            - 摘自：Java并发编程之美                 - https://www.cnblogs.com/cold-windy/p/11743013.html

                        加锁 和 释放锁 的语义：
                            当 获取 锁 以后， 会 清空 锁块内 本地内存中将会被用到的共享变量，在 使用这些共享变量时 从 主内存 进行加载
                            在 释放 锁 时，   将 本地内存中修改的 共享变量 刷新到 主内存 中

                    进入 synchronized块 的 内存语义：

                        把 在synchronized块内 使用到的变量 从 线程的工作内存中 清除，这样在 synchronized 块中 使用到该变量时，
                        就不会从线程的工作内存中获取，而是直接从主内存中获取

                    退出 synchronized块 的 内存语义：

                        把 在synchronized块内 对共享变量的修改 刷新到 主内存




        3、程序的顺序性规则       // 顺序、可见

            在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作             // 同一线程

            可以重排，但是要保证 符合 Happens-Before 规则
            Happens-Before 规则 关注的是 可见性❗❗❗

            所谓顺序：

                指的是 你可以用 顺序的方式 推演 程序的执行，但是 程序的执行 不一定是 完全顺序的

                编译器保证：

                    结果一定  ==  顺序方式 推演的结果


        4、传递性                // 顺序、可见

        5、线程 start() 规则     // 顺序、可见

        6、线程 join() 规则      // 顺序、可见

        7、中断法则

            一个线程调用另一个线程的interrupt()  happens-before  于  被中断的线程 发现中断

        8、终结法则

            一个对象的 构造函数的结束  happens-before  于  这个对象 finalize()的开始

            finalize()

                在GC回收对象之前  ->  被自动调用

                在GC之前  ->  做必要的清理工作

                    -------------------------------------------
                    GC ->  obj：

                        1、obj.finalize()  ->  给它一次真正被清理之前的 收尾操作      // GC之前 -> 处理后事  ==>  如：关闭流

                        2、gc -> obj

                    ------------------------------------------------------
                    1、一旦GC准备好释放对象占用的存储空间，它首先调用finalize()

                        如果使用finalize()  ->  就可以在垃圾收集期间  进行一些重要的清除或清扫工作(如：关闭流)

                        但JVM(Java虚拟机)不保证此方法总被调用

                    2、而且只有在下一次垃圾收集过程中，才会真正回收对象的内存


    final

        告诉编译器优化得更好一点

        final 修饰变量时，初衷是告诉编译器：  这个变量 ⽣⽽不变，可以 可劲儿优化

        final修饰符：

            final修饰的实例字段则是 涉及到 新建对象的发布问题

            当一个对象包含final修饰的实例字段时，其他线程能够看到已经初始化的final实例字段，这是安全的。


    实现：
        JVM内部实现   - JVM编程人员实现

            内存屏障                            // 隔离（无法 重排） 、 禁用缓存（可见）                 - https://baijiahao.baidu.com/s?id=1667840029586081215
                Load  Barrier  读 屏障
                Store Barrier  写 屏障


            内存屏障的两个作用：

                1、阻止屏障两侧的指令重排序              // 隔离（禁止 重排）

                2、写的时候，强制把 缓冲区/高速缓存中的数据 写回 主内存，并让 其他CPU缓冲中的数据 失效；读的时候 直接从 主内存 中读取        // 禁用缓存（可见）

                    对于Load  Barrier来说
                        在指令前插入Load  Barrier，可以让 高速缓存中的数据 失效，强制重新从 主内存 加载数据

                    对于Store Barrier来说
                        在指令后插入Store Barrier，可以让 写入缓存中的最新数据 更新写入 主内存，让 其他线程 可见


            Java的内存屏障 通常所谓的四种：

                LoadLoad、StoreStore、LoadStore、StoreLoad     // 实际上也是 上述两种的 组合，完成一系列的 屏障 和 数据同步 功能


                LoadLoad 屏障：

                    对于这样的语句
                        Load1; LoadLoad; Load2;

                        在 Load2 及 后续读取操作 要读取的数据被访问前，保证 Load1要读取的数据 被 读取完毕

                StoreStore 屏障：

                    对于这样的语句
                        Store1; StoreStore; Store2;

                        在 Store2 及 后续写入操作 执行前，保证 Store1的写入操作 对 其它处理器 可见

                LoadStore 屏障：

                    对于这样的语句
                        Load1; LoadStore; Store2;

                        在 Store2 及 后续写入操作 被刷出前，保证 Load1要读取的数据 被 读取完毕

                StoreLoad 屏障：                       // 万能屏障、开销也最大

                    对于这样的语句
                        Store1; StoreLoad; Load2;

                        在 Load2 及 后续所有读取 操作执行前，保证 Store1的写入 对 所有处理器 可见

                        它的 开销 是四种屏障中 最大的

                        在大多数处理器的实现中，这个屏障是个 万能屏障，兼具 其它三种内存屏障 的功能


            volatile 与 内存屏障

                在每个 volatile 写操作前 插入 StoreStore屏障，在 写操作后 插入 StoreLoad屏障

                在每个 volatile 读操作前 插入 LoadLoad屏障，  在 读操作后 插入 LoadStore屏障

                所以 volatile 防止了指令的重排序(保证有序性) 和 内存的一致性(保证可见性)


            - 当然用 synchronized 或者 Lock 给代码加锁，也是可以保证 有序性 与 可见性 的

                synchronized 释放锁/获取锁 语义  同 volatile

                Lock 语义 同 synchronized

                    - 实现 依赖 volatile


        ----------------------------
        Java内存模型底层怎么实现的？

            主要是通过 内存屏障(memory barrier) 禁止重排序的

            即时编译器 根据 具体的 底层体系架构：

                将这些 内存屏障 替换成 具体的 CPU 指令


            对于 编译器 而言：

                内存屏障 将限制 它所能做的 重排序 优化

            而对于 处理器 而言：

                内存屏障 将会导致 缓存的刷新 操作


            比如，对于volatile：

                编译器 将在volatile字段的 读写操作 前后 各插入一些 内存屏障


------------------------------------------------------------------------------------------------------------------------
2、互斥锁         // 原子性

    功能：

        按需 禁用 "线程切换"

            非真正的 禁用 "线程切换"，CPU 在 单位时间片 后 依然会做 线程切换

            只是切换时，锁 不会释放，其他线程拿到了CPU使用权，然后获取不到锁，就进不来临界区，继续被挂起

            然后继续 重新分配 CPU使用权

            持有 锁 的线程 重新争夺到 CPU使用权 后，程序可继续执行，就保证了 执行不中断  ---> 保证了 原子性

    工具：
        synchronized、Lock


        1、synchronized                                        - https://www.jianshu.com/p/e2054351bd95

            可以保证 方法或者代码块 在运行时，同一时刻 只有一个方法 可以进入到 临界区，同时它还可以保证 共享变量的内存可见性

            无锁、偏向锁、轻量级锁、重量级锁   的 加锁/解锁、   锁升级(不可逆)


            加锁、解锁：
                monitorenter 和 monitorexit指令



            重量级锁

                传统的锁机制（JDK 1.6 之前）

                    传统的锁（重量级锁）依赖于系统的同步函数，在linux上使用mutex互斥锁，最底层实现依赖于futex
                    这些同步函数都涉及到用户态和内核态的切换、进程的上下文切换，成本较高
                    对于 加了synchronized关键字 但运行时 并没有 多线程竞争，或 两个线程 接近于 交替执行 的情况，使用 传统锁机制 无疑 效率是会比较低的


            偏向锁 和 轻量级锁（JDK 1.6 引入）

                它们的引入是为了解决

                    在没有多线程竞争 或 基本没有竞争  的场景下，因使用 传统锁机制 带来的 性能开销问题


            对象头

                实现多种锁机制的基础


                对象 与 对应的锁信息 映射（当前哪个线程持有锁，哪些线程在等待）：

                    1、全局map
                        线程安全     ->  性能
                        加锁对象多时  ->  内存

                    2、对象头



        2、Lock

            功能：
                阻塞、非阻塞、可中断、可超时

            实现：
                AQS
                    volatile、传递性、happens-before、CAS


------------------------------------------------------------------------------------------------------------------------
锁升级机制（synchronized的优化机制）：                                   - https://www.cnblogs.com/myseries/p/12213997.html

    无锁   ->   偏向锁   ->   轻量级锁（自旋锁、自适应自旋锁）  ->   重量级锁

    ------------------------------------------------------------------------
    基于现实主义的经验总结（现实主义、经验主义）：

        提炼出 几种典型场景    针对性的优化 （主要是 锁竞争）

        ------------------------------------------------------------------------
        观察得出：

            1、多数情况下，只有一个线程抢占锁                                            ->      偏向锁（不解锁、加锁）

            2、竞争情况下，一般只有2个线程在竞争锁，且多数情况下，同步块 执行很快            ->      轻量级锁（自旋 - 非阻塞）

            3、最恶劣情况（激烈竞争、同步块 执行很慢）                                    ->      重量级锁（阻塞）



    ------------------------------------------------------------------------
    第一个线程抢占锁    ->  偏向锁      // 只有一个线程，退出同步 不释放锁

    第2+个线程抢占所   ->   锁撤销 + 锁升级    ->  轻量级锁（自旋锁、自适应自旋锁）          // CPU空转，不阻塞     ->  便于快速 获取锁（等待/唤醒 成本过高）

    自旋10次以上(默认)     ->    重量级锁   // 等待时间过长（锁竞争激烈、或 同步块执行 过慢）   -->  CPU长时间空转 获取不到锁  ->  升级  转换为 阻塞（等待、唤醒）

    ------------------------------------------------------------------------
    不可逆：

        锁升级 -> 不可逆！！！

        一旦升级为 重量级锁，就算并发下降 -> 1个线程，也永远是 -> 重量级锁!!!

------------------------------------------------------------------------------------------------------------------------
偏向锁         // 还未发生 锁竞争   ->  只有一个线程抢占锁    ->  退出同步 也不会释放锁      ->  避免 反复加锁、解锁 的性能损耗

    在实际应用运行过程中发现：

        “锁总是同一个线程持有，很少发生竞争”'

        也就是说 锁总是被 第一个占用他的线程 拥有，这个线程就是 锁的偏向线程

    只要发生竞争：

        偏向锁 就会升级 轻量锁


    在Jdk1.6中，偏向锁的开关是默认开启的，适用于 只有一个线程 访问同步块的场景


------------------------------------------------------------------------------------------------------------------------
轻量锁       // 只要发生 锁竞争（有第2+个线程 来抢占这个锁了）  ->  偏向锁 升级 轻量锁  ->  开始进入 锁竞争模式  ->  竞争失败：自旋  ->  退出同步 会释放锁   // 有竞争了，自然要释放锁了

    轻量级锁 每次退出同步块 都需要释放锁，而偏向锁 是在竞争发生时 才释放锁

    争夺轻量级锁失败时  ->  自旋 尝试抢占锁


    锁撤销：         // 锁撤销的开销 还是挺大的

        如果某些同步代码块，大多数情况下都是有 两个及以上 的线程竞争的话，那么偏向锁就会是一种累赘

        对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭

------------------------------------------------------------------------------------------------------------------------
轻量级锁主要有两种：          // 轻量级锁也被称为 非阻塞同步、乐观锁，因为这个过程 并没有把线程阻塞挂起，而是让线程空循环等待，串行执行
------------------------------------------------------------------------------------------------------------------------
1、自旋锁           // 自旋次数 固定   -> 默认自旋10次           同步块 执行很快很快的 场景     ->    自旋 取代 重量级的阻塞、唤醒

    所谓自旋

        就是指 当有另外一个线程 来竞争锁时，这个线程会在 原地循环等待（而不是 把该线程给阻塞挂起）

        直到那个获得锁的线程 释放锁之后，这个线程 就可以 马上抢占锁

    注意
        锁在原地循环的时候，是会消耗cpu的

        就相当于在 执行一个啥也没有的 for循环

    所以，轻量级锁适用于：

        那些 同步代码块执行的 很快的场景

        这样，线程原地等待 很短很短的时间 就能获得锁了

    经验表明：

        大部分同步代码块执行的时间 都是很短很短的

        也正是基于这个原因，才有了 轻量级锁 这么个东西


------------------------------------------------------------------------------------------------------------------------
2、自适应自旋锁                // 自旋次数 动态调整

    所谓自适应自旋锁

        就是线程空循环 等待的自旋次数 并非是固定的，而是会 动态的 根据实际情况 来改变自旋等待的次数


    其大概原理是这样的：

        假如一个线程1 刚刚成功获得一个锁，当它把锁释放了之后

            线程2获得该锁，并且线程2在运行的过程中，此时线程1又想来获得该锁了

            但线程2还没有释放该锁，所以线程1只能自旋等待，但是虚拟机认为，由于 线程1 刚刚获得过该锁

            那么虚拟机觉得 线程1这次自旋 也是很有可能 能够再次成功获得该锁的，所以 会延长 线程1自旋的次数


        另外，如果对于某一个锁，一个线程自旋之后，很少成功获得该锁

            那么以后这个线程要获取该锁时，是有可能直接忽略掉自旋过程，直接升级为重量级锁的，以免空循环等待浪费资源



------------------------------------------------------------------------------------------------------------------------
重量锁       // 互斥锁    ->  阻塞、悲观锁

    阻塞/唤醒：

        竞争失败后，线程阻塞

        释放锁后，唤醒阻塞的线程

    适用：
        竞争激烈

        同步块 执行时间长


    --------------------------------------
    重量级锁开销大：

        线程阻塞、唤醒：

            需要操作系统来帮忙，从  用户态 ->  内核态    // 巨耗时

            而转换状态 是需要消耗很多时间的，有可能比 用户执行代码的时间还要长


------------------------------------------------------------------------------------------------------------------------
锁升级                                                     - https://blog.csdn.net/qq_35688140/article/details/100527378


无锁：我们刚实例化一个对象                                   // Object lock = new Object();

偏向锁：单个线程的时候，会开启偏向锁。可以使用-XX:-UseBiasedLocking来禁用偏向锁。

轻量级锁：当多个线程来竞争的时候，偏向锁会进行一个升级，升级为轻量级锁（内部是自旋锁），因为轻量级锁认为，我马上就会拿到锁，所以以自旋的方式，等待线程释放锁。

重量级锁：由于轻量级锁过于乐观，结果迟迟拿不到锁，所以就不断地自旋，自旋到一定的次数，为了避免资源的浪费，就升级为我们最终的重量级锁。




------------------------------------------------------------------------------------------------------------------------
synchronized包含6个核心组件：

    Wait Set：           哪些调用 wait 方法被阻塞的线程被放置在这里；                                    // 阻塞   - wait()

    Contention List：    竞争队列，所有请求锁的线程首先被放在这个竞争队列中；                              // 阻塞   - all

    Entry List：         Contention List 中那些有资格成为候选资源的线程被移动到 Entry List 中；           // 阻塞   - 预挑选 部分有资格 竞争锁的

    OnDeck：             任意时刻，最多只有一个线程正在竞争锁资源，该线程被称为 OnDeck；                    // 持有🔐者

    Owner：              当前已经获取到所资源的线程被称为 Owner；                                        // 持有🔐者

    !Owner：             当前释放锁的线程。                                                            // 持有🔐者


JVM 每次从队列的尾部取出一个数据，用于锁竞争候选者（OnDeck），但是并发情况下，ContentionList 会被大量的并发线程进行 CAS 访问，
为了降低对尾部元素的竞争，JVM 会将一部分线程移动到 EntryList 中作为候选竞争线程。

Owner 线程会在 unlock 时，将 ContentionList 中的部分线程迁移到 EntryList 中，并指定 EntryList 中的某个线程为 OnDeck 线程（一般是最先进去的那个线程）。

Owner 线程并不直接把锁传递给 OnDeck 线程，而是把锁竞争的权利交给 OnDeck，OnDeck 需要重新竞争锁。
这样虽然牺牲了一些公平性，但是能极大的提升系统的吞吐量，在JVM 中，也把这种选择行为称之为“竞争切换”。

OnDeck 线程获取到锁资源后会变为 Owner 线程，而没有得到锁资源的仍然停留在 EntryList 中。
如果 Owner 线程被 wait 方法阻塞，则转移到 WaitSet 队列中，直到某个时刻通过 notify 或者notifyAll 唤醒，会重新进去 EntryList 中。

处于 ContentionList、EntryList、WaitSet 中的线程都处于阻塞状态，该阻塞是由操作系统来完成的（Linux 内核下采用 pthread_mutex_lock 内核函数实现的）。

Synchronized 是非公平锁。
    Synchronized 在线程进入 ContentionList 时，等待的线程会先尝试自旋获取锁，如果获取不到就进入 ContentionList，
    这明显对于已经进入队列的线程是不公平的，还有一个不公平的事情就是自旋获取锁的线程还可能直接抢占 OnDeck 线程的锁

每个对象都有个 monitor 对象，加锁就是在竞争 monitor 对象，代码块加锁是在前后分别加上 monitorenter 和 monitorexit 指令来实现的，
方法加锁是通过一个标记位来判断的

synchronized 是一个重量级操作，需要调用操作系统相关接口，性能是低效的（程序在操作系统的内核态与用户态之间切换），有可能给线程加锁消耗的时间比有用操作消耗的时间更多。


JDK 1.6 中默认是开启偏向锁和轻量级锁，可以通过-XX:-UseBiasedLocking 来禁用偏向锁。
这个过程也完完全全说明了synchronized是一个非公平的锁


========================================================================================================================
管程模型
========================================================================================================================

    所谓管程，指的是：

        管理  -->  共享变量（field）  以及  对共享变量的操作过程（method）

        让他们 支持并发（线程安全）


    翻译为 Java 领域的语言，就是：

        管理  -->  类的 成员变量 和 成员方法

        让这个类 是 线程安全 的


    那管程是 怎么管 的呢？

        1、synchronized

            Java 内置的 管程方案（synchronized）使用简单

            synchronized 关键字修饰的代码块，在 编译期 会自动生成 相关加锁和解锁 的代码

            但是 仅支持 一个条件变量

        2、Lock

            Java SDK 并发包实现的 管程 支持多个条件变量（Condition）

            不过并发包里的锁，需要开发人员 自己进行 加锁和解锁 操作



    管程解决

        并发编程里两大核心问题 ———— 互斥 和 同步


    管程 ———— 并发编程的万能钥匙：

        学好管程，理论上所有的并发问题你都可以解决
        且很多并发工具类底层都是管程实现的
        所以学好管程，就是相当于掌握了一把 并发编程的万能钥匙


========================================================================================================================
并发模型
========================================================================================================================


1、并发编程 核心

    落实到我们编程人员的这里，编写并发程序，主要就是三点：

        分工、同步、互斥

        所以，JDK 内容看起来繁杂，但是其实就是针对这三类问题，提供的各种对应 工具 而已


    分工

        拆解任务 -> 并分配给线程          // 串行 -> 并行


        分工工具：

            线程池、Fork/Join、Future、CompletableFuture、CompletionService


    同步

        线程协作                        // 任务依赖 -> 线程通信


        同步工具：
            CountDownLatch、CyclicBarrier
            synchronized                    ->  wait()、notify()、notifyAll()         // 条件变量
            Lock - Condition                ->  await()、signal()、signalAll()        // 条件变量


    互斥

        共享资源 竞争                   // 同一时刻 只允许 一个线程 访问


        互斥工具：
            synchronized、Lock



========================================================================================================================
分工 工具  -  线程池
========================================================================================================================

------------------------------------------------------------------------------------------------------------------------
1、线程池                           // 分工 、 生产(submit 任务提交方) + 消费(线程池 自身)

    ThreadPoolExecutor

        int corePoolSize                        表示线程池保有的最小线程数

        int maximumPoolSize                     表示线程池创建的最大线程数

        long keepAliveTime                      如果一个线程空闲了 keepAliveTime & unit 这么久，
        TimeUnit unit                           而且线程池的线程数大于 corePoolSize，那么这个空闲的线程就要被回收了

        BlockingQueue<Runnable> workQueue       工作队列

        ThreadFactory threadFactory             通过这个参数你可以自定义如何创建线程，可以给线程指定一个有意义的名字

        RejectedExecutionHandler handler        自定义任务的拒绝策略  // 可自扩展策略

             默认提供的4种策略：
                 AbortPolicy：         默认策略，直接拒绝，并抛出异常                                           // 拒绝 + 抛Ex
                 DiscardPolicy：       直接丢弃任务，没有任何异常抛出                                           // 丢弃
                 CallerRunsPolicy：    提交任务的线程自己去执行该任务                                           // 自执行
                 DiscardOldestPolicy： 丢弃最老的任务，把最早进入工作队列的任务丢弃，然后把新任务加入到工作队列      // LRU淘汰


------------------------------------------------------------------------------------------------------------------------
2、Future                            // result、阻塞

    功能：

        获取 异步任务 结果

    场景：

        任务之间 有 依赖 关系，可以用 Future 来解决

        ------------------------------------------------------------------------------
        对于 简单的 并行任务，可以通过 “线程池 + Future” 的方案来解决

            Future 可以很容易获得 异步任务的 执行结果

            任务之间有依赖关系，可以用 Future 来解决

            get 阻塞 获取结果  -->  （类似：Thread.join()、CountDownLatch、阻塞队列...）

    API：

        ThreadPoolExecutor    ->    AbstractExecutorService    ->    ExecutorService    ->    Executor

            void        execute  ->  Runnable


            Future      submit   ->  Runnable

            Future      submit   ->  Runnable + Result

            Future      submit   ->  Callable


------------------------------------------------------------------------------------------------------------------------
3、FutureTask                        // Future + Runnable

    FutureTask      ->      Future , Runnable


------------------------------------------------------------------------------------------------------------------------
4、CompletableFuture                 // JDK 1.8    异步编程

    功能：
        异步编程、链式操作

    场景：
        并行、串行、      聚合关系   ->   AND 、OR

    语义：
        runAsync()      // 异步运行
        supplyAsync()   // 提供异步
        thenCombine()   // 然后合并

    异步编程：
        RxJava、Flow（Java 9）

------------------------------------------------------------------------------------------------------------------------
5、CompletionService                 // 线程池 + Future结果 队列

    功能：

        将 线程池 Executor  和  阻塞队列 BlockingQueue  的 功能融合在了一起

            让 批量异步任务 的 管理 更简单

            CompletionService 能够让 异步任务的执行 结果 有序化，先执行完的 先进入 阻塞队列

    场景：

        批量的 并行任务

    实现：

        ExecutorCompletionService

            ExecutorCompletionService(Executor executor, BlockingQueue<Future<V>> completionQueue);

                Executor                        任务执行 线程池

                BlockingQueue<Future<V>>        任务执行 结果Future 存储队列


                执行顺序：

                    submit          ->  提交task

                    Executor        ->  线程池 后台异步 自动执行任务

                    BlockingQueue<Future<V>>    ->  任务结果Future对象 存储到 completionQueue 队列

                    take/poll                   ->  从 结果队列 取 Future对象   ->  Future.get() 结果


------------------------------------------------------------------------------------------------------------------------
6、Fork/Join

    功能：

        单机版 的 MapReduce

        Fork/Join 并行计算框架 主要解决的是 分治任务

            分治 的核心思想是 “分而治之”：

                将一个 大的任务 拆分成 小的子任务 去解决，然后再把 子任务的结果 聚合起来，从而得到 最终结果


            核心组件 ForkJoinPool

                支持 任务窃取 机制

                能够让 所有线程的 工作量基本均衡，不会出现 有的线程很忙、而有的线程很闲 的状况

                所以 性能很好

    场景：

        分治任务


    分治任务模型：

        1、模型：
            1、任务分解
                将 任务 迭代地分解为 子任务，直至 子任务 可以 直接计算出结果

            2、结果合并
                逐层合并 子任务的 执行结果，直至 获得 最终结果

        2、模型实现：

            Fork/Join 是一个 并行计算 的框架，主要就是用来支持

                分治任务模型

                    Fork    ->   任务分解       ->      将 任务 迭代地分解为 子任务，直至 子任务 可以 直接计算出结果
                    Join    ->   结果合并       ->      逐层合并 子任务的 执行结果，直至 获得 最终结果

            Fork/Join 计算框架 主要包含两部分：

                分治任务的线程池     ForkJoinPool
                分治任务            ForkJoinTask

                    类似于 ThreadPoolExecutor 和 Runnable 的关系

                    都可以理解为 提交任务 到 线程池，只不过 分治任务 有自己独特类型 ForkJoinTask

------------------------------------------------------------------------------------------------------------------------
1、ForkJoinTask：                 // Runnable、Callable、Future
------------------------------------------------------------------------------------------------------------------------
1、本质 —— Task

    包装了 Runnable、Callable

        包装类
            AdaptedCallable
            AdaptedRunnable


2、fork / join

    fork                        // 异步执行

        过程：
            将 任务(自身) 放到 线程池 的 工作队列

            ForkJoinPool.workQueue -> push(this)        // this -> ForkJoinTask

        现象结果：
            异步地 执行 一个子任务

        角色：
            生产者


    join                        // 阻塞获取

        阻塞 当前线程    -->    等待 子任务的 执行结果


3、Future                    // 吸收了 Future     -->  获取异步任务结果

    ForkJoinTask implements Future

        目的：
            吸收 Future 获取异步任务结果 的能力


4、分治实现                   // 递归

    底层实现：

        递归

    ForkJoinTask 有两个 抽象子类：

        RecursiveAction 和 RecursiveTask        --> recursive：递归

        通过名字你就应该能知道，它们都是用 递归 的方式来处理 分治任务


        这两个子类都定义了 抽象方法 compute()，不过区别是：

            RecursiveAction     void compute()          ->  无 返回值
            RecursiveTask       V    compute()          ->  有 返回值

                这两个子类也是 抽象类，在使用的时候，需要你定义子类去扩展

------------------------------------------------------------------------------------------------------------------------
2、ForkJoinPool
------------------------------------------------------------------------------------------------------------------------
本质：             // 线程池 -> 消费者
    消费者


核心组件：

    Fork/Join 并行计算 的核心组件是：ForkJoinPool


ForkJoinPool  VS  ThreadPoolExecutor：

    1、生产者 - 消费者

        ThreadPoolExecutor

            本质上是一个 生产者 - 消费者模式 的实现

        ForkJoinPool

            本质上也是一个 生产者 - 消费者 的实现



    2、任务队列            // 生产者-消费者 通信的媒介

        ThreadPoolExecutor      一个 任务队列          多个工作线程
        ForkJoinPool            多个 任务队列          多个工作线程


    3、extends AbstractExecutorService

        ThreadPoolExecutor  extends AbstractExecutorService
        ForkJoinPool        extends AbstractExecutorService


API：

    1、直接处理 ForkJoinTask         // externalPush(task)

        T invoke(ForkJoinTask<T> task);                     阻塞       -> task.join()

        ForkJoinTask<T> submit(ForkJoinTask<T> task);       异步


    2、间接处理 ForkJoinTask         // r/c -> adapt -> task  ===>  externalPush(task)

        Runnable、Callable   -->  AdaptedRunnable、AdaptedCallable   -->  ForkJoinTask   -->  externalPush(task)



执行：

    invoke(task) / submit(task)

    -->  externalPush(task)     ->  路由  -> workQueues[i]  -> workQueue

        ForkJoinPool 根据一定的 路由规则 把 任务 提交到 一个任务队列

        如果任务 在执行过程中 会创建出 子任务

        那么 子任务 会提交到 工作线程对应的 任务队列中       // 子父 同一队列     --> 工作线程 — workQueue  存在绑定 映射关系


任务窃取 机制：

   ForkJoinPool 支持一种叫做 “任务窃取” 的机制

   如果工作线程空闲了，那它可以 “窃取” 其他工作任务队列里的任务

   线程T2 对应的 任务队列空了，它可以 “窃取” 线程T1 对应的 任务队列 的任务

   如此一来，所有的工作线程 都不会闲下来了


双端 任务队列：

   ForkJoinPool 中的 任务队列 采用的是 双端队列

   工作线程正常 获取任务 和 “窃取任务” 分别是从 任务队列 不同的端 消费

   这样能 避免 很多不必要的 数据竞争



应用：

    Java1.8 提供的 Stream 的 并行流 是以 ForkJoinPool 为基础


注意：     // 默认 共享一个ForkJoinPool          -->  各任务 自定义自己的 ForkJoinPool

    默认情况下 所有的并行流计算 都共享一个 ForkJoinPool，这个共享的 ForkJoinPool 默认的线程数是 CPU的核数

    如果所有的并行流计算都是 CPU 密集型计算的话，完全没有问题

    但是如果存在 I/O 密集型 的 并行流计算，那么很可能会因为 一个很慢的 I/O 计算 而 拖慢整个系统的性能

    所以建议用 不同的 ForkJoinPool 执行 不同类型 的 计算任务





========================================================================================================================
同步/互斥 工具  -  JUC 高级 并发工具         -> 经过高度封装       Lock/...  ->  AQS  ->  final + volatile + CAS
========================================================================================================================

1、分工                    // 任务拆分         串行 -> 并行

    Fork/Join

    线程池


2、同步                    // 线程通信

    CountDownLatch        ->    AQS    ->  final + volatile + CAS

    CyclicBarrier         ->    ReentrantLock -> AQS


    Condition

        synchronized - 内置同步器        // wait()、 notify() / notifyAll()

        Lock.Condition                  // await()、signal() / signalAll()



3、互斥                    // 锁

    synchronized        - 互斥                 阻塞 / 不支持 超时 / 不可中断                   管程模型

        同步
            wait()

            notify()

            notifyAll()


    Lock                - 互斥                                                              管程模型

        lock()                                  阻塞

        tryLock()                               非阻塞

        tryLock(long time, TimeUnit unit)       超时

        lockInterruptibly()                     可中断



        同步
            Condition newCondition()

                await()

                signal()

                signalAll()



        ReentrantLock                          ->    AQS

        ReentrantReadWriteLock

            ->    StampedLock（1.8 读写锁  乐观读[无锁]）



    Semaphore                   信号量模型

                                               ->    AQS

        没对外暴露 Condition


        信号量模型：

            红绿灯


            信号量 可以实现的 独特功能

                同时允许 多个线程 进入临界区

            信号量 只能

                唤醒一个 阻塞中的线程            // 无Condition，无法同时 唤醒多个 线程去争抢锁


            信号量模型 没有 Condition 的概念

                即 阻塞线程 被唤醒了 直接就运行了，而 不会去检查 此时 临界条件 是否已经不满足了

                基于此考虑 信号量模型 才会设计出 只能让一个线程被唤醒，否则就会出现 因缺少Condition检查 而带来的 线程安全问题

                正因为 缺失了Condition，所以 用信号量来实现阻塞队列 就很麻烦，因为 要自己实现类似Condition 的逻辑




========================================================================================================================
并发容器、原子类
========================================================================================================================

------------------------------------------------------------------------------------------------------------------------
并发容器/原子类              // JUC工具 的 基石

    上述 JUC 高级并发工具 的底层实现 依赖于：

        -> 并发容器、原子类

------------------------------------------------------------------------------------------------------------------------
1、并发容器：             // 有锁 工具    ->  依然有锁，只是依靠 空间换时间  --> 减小 锁粒度，并未 消除锁   -->  减小 竞争 程度

                        // 也有部分是 无锁 工具   ->  未用到 Lock，直接基于 最底层 CAS 实现
------------------------------------------------------------------------------------------------------------------------
    1、同步容器：

        JDK 1.5 之前 - 基于 synchronized 的 同步容器：

            Collections.synchronizedList、Collections.synchronizedMap、Collections.synchronizedSet ...

            Vector、Stack、Hashtable


    2、并发容器：         // final + volatile + CAS

        List
            CopyOnWriteArrayList                        - Lock      ->  final + volatile + CAS

        Map
            ConcurrentHashMap                           - Lock / synchronized + CAS

            ConcurrentSkipListMap                       - final + volatile + CAS

        Set
            CopyOnWriteArraySet                         - Lock                          -> CopyOnWriteArrayList


            ConcurrentSkipListSet                       - final + volatile + CAS        -> ConcurrentSkipListMap

        Queue/Deque

            ArrayBlockingQueue、LinkedBlockingQueue      - Lock + Condition
            ConcurrentLinkedQueue                        - final + volatile + CAS

            LinkedBlockingDeque                          - Lock + Condition
            ConcurrentLinkedDeque                        - final + volatile + CAS


    Fail-Fast：

        Java容器 的 快速失败机制（Fail-Fast）

------------------------------------------------------------------------------------------------------------------------
2、原子类           // 无锁工具     ->  硬件支持  -->  CPU 提供了 CAS指令（语义：比较 并 交换） -->  单条CPU指令（完成 比较并交换） ->  原子性
------------------------------------------------------------------------------------------------------------------------

    无锁 VS 有锁：


        互斥锁方案

            为了保证互斥性，需要执行加锁、解锁操作，而加锁、解锁操作本身就消耗性能；

            同时拿不到锁的线程还会进入阻塞状态，进而触发线程切换，线程切换对性能的消耗也很大。

        无锁方案

            相比之下，无锁方案 则完全没有 加锁、解锁的性能消耗，同时 还能保证 互斥性

            既解决了问题，又没有带来新的问题，可谓绝佳方案（怎么可能有完美方案呢....  自旋等待[饥饿、活锁]、ABA问题）



    无锁方案 的 实现原理：

        硬件支持

        基于 CAS 指令，实现 原子类 容器

        ------------------------------------------------------------------------------------------------
        CPU 为了解决并发问题，提供了 CAS 指令（CAS，全称是 Compare And Swap，即“比较并交换”）

            CAS 指令 包含 3 个参数：

                共享变量的 内存地址A、用于 比较的值B 、共享变量的 新值C

              【CAS语义】  ====>  只有当 内存中地址A处 的值 等于 B 时，才能将 内存中地址A处的 值 更新为 新值C

       【作为 一条CPU指令】，CAS指令 本身 是能够保证 原子性 的

        ------------------------------------------------------
        -> 利用 CAS，我们也可以自己实现一个 无锁原子类 数据结构.


    用法：

        CAS + 自旋（循环尝试）


    新问题：

        饥饿、活锁：

            自旋 会 反复重试

        ABA：
            版本号


    场景：

        一个 共享变量      ->  原子类                - 简单 场景


        多个 共享变量      ->  互斥锁                - 复杂 场景

        --------------------------------------------------------------
        Java 提供的 原子类 能够解决一些 简单的 原子性问题，

        但你可能会发现，上面我们 所有原子类的方法 都是针对 一个共享变量 的，

        如果你需要解决 多个变量 的 原子性问题，建议还是使用 互斥锁方案。

        原子类虽好，但使用要慎之又慎。




    ------------------------------------
    有锁

        Concurrent

    无锁

        Atomic




========================================================================================================================
并发 设计模式         -> 并发 常用解决方案
========================================================================================================================

------------------------------------------------------------------------------------------------------------------------
多线程设计模式

    1、避免共享的设计模式

        Immutability模式、Copy-on-Write模式 和 线程本地存储模式

        本质上都是为了避免共享，只是实现⼿段不同⽽已


        这 3 种设计模式的实现都很简单，但是实现过程中有些细节还是需要格外注意的：

            使⽤ Immutability模式  需要注意 对象属性的不可变性

            使⽤	Copy-on-Write模式  需要注意 性能问题

            使⽤ 线程本地存储模式    需要注意 异步执行问题


    2、多线程版本 IF 的设计模式：

        Guarded Suspension 模式 和 Balking 模式，都可以简单地理解为“多线程版本的 if”


        区别：

            前者会 等待 if 条件变为真，⽽后者则 不需要等待


        Guarded Suspension 模式 的经典实现是 使用管程

            很多初学者会简单地⽤线程 sleep 的⽅式实现

            但不推荐你使⽤这种⽅式，最重要的原因是

                性能

                    sleep 的时间太长，会影响 响应时间

                    sleep 的时间太短，会导致 线程频繁地被唤醒，消耗系统资源


        实现 Balking模式 最容易忽视的就是 竞态条件 问题：

            因此，在多线程场景中使⽤ if语句 时，⼀定要多问⾃⼰⼀遍：是否存在 竞态条件


    3、三种最简单实用的 分⼯模式：

        Thread-Per-Message 模式、Worker Thread 模式、⽣产者 - 消费者 模式



------------------------------------------------------------------------------------------------------------------------
1、生产者-消费者模式

    解耦

    异步
        平衡 生产者和消费者 的 速度差

------------------------------------------------------------------------------------------------------------------------
2、不变

    享元模式

    无状态

------------------------------------------------------------------------------------------------------------------------
3、Copy-on-Write         // 写时复制

------------------------------------------------------------------------------------------------------------------------
4、线程本地存储模式        // 避免共享 -> 没有共享，就没有伤害

    ThreadLocal          // 只是个 中间工具类 --> 真正的 实现在 ThreadLocalMap  （Thread -> ThreadLocal.ThreadLocalMap）

        内存泄露 问题：        // 无法 自动释放

            线程池 -> Thread     // 线程池中 线程的存活时间 太长，往往都是 和程序 同生共死的，
                                // 这就意味着 Thread 持有的 ThreadLocalMap 一直都不会被回收

            -> ThreadLocalMap   // K （ThreadLocal） - 弱引用        只要 ThreadLocal 结束了自己的生命周期，K 就可以被回收
                                // V （Value）       - 强引用        即便 Value 的生命周期结束了，      Value 也无法被回收

                                ===>  JVM 不能做到自动释放对 Value 的强引用

                                --> 从而导致 内存泄露


        如何正确使用 ThreadLocal      // 手动释放

            既然 JVM 不能做到自动释放对 Value 的强引用，那我们 手动释放 就可以了


            手动释放：

                tl.set(obj);

                // 线程逻辑 结束前
                tl.remove(obj);     // 手动清理 ThreadLocal  --> map.remove  K/V

------------------------------------------------------------------------------------------------------------------------
5、Guarded Suspension

    等待唤醒 机制的实现

    多线程版本的 if               // 单线程下 if条件结果 不会改变，多线程下 会变

------------------------------------------------------------------------------------------------------------------------
6、两阶段终止模式           // 有始有终   开启/终止 Thread


    1、优雅 终止 线程：                // 非暴力    -> stop() - 被废弃的原因

        让 Java线程 ⾃⼰执⾏完run()⽅法


        Thread 方法：

            interrupt() 、 isInterrupted()


        线程状态：

            休眠 -> 运行 -> 终止

                休眠 -> 唤醒   ==>   设置终止标志 -> 线程检测 终止标志 -> 执行/放弃 run()   ==>   线程终止


        将终止过程分成两个阶段：                    // T1 终止 T2，非 自己终止自己

            1、线程T1 向 线程T2 发送终止指令

            2、线程T2 响应终止指令


        终止指令：

            1、发出 终止指令    ->  设置 终止标志位

                interrupt()⽅法

            2、执行 终止       -> 根据 终止标志位 isInterrupted() 判断

                线程 会在合适的时机 检查这个标志位

                如果发现符合终止条件，则⾃动退出 run() ⽅法



        1、JDK自带的 终止标志                              // 多线程条件下（可能被其他线程修改）  ->  不可控

            // Thread标志位判断
            while (!Thread.currentThread().isInterrupted()) {
                ...

                try	{
                    Thread.sleep(2000);
                }	catch	(InterruptedException	e){
                    // 重新设置线程中断状态
                    Thread.currentThread().interrupt();
                }

                ...
            }


        2、自定义 终止标志                                 // 强烈建议！！！          自定义（只有自己使用）  ->  可控

            // 自定义线程终⽌标志位
            volatile boolean terminated	= false;

            // 自定义标志位判断
            while (!terminated){
                ...
            }


    2、优雅地 终止 线程池：

        线程池提供了两个⽅法：

            shutdown()                          // 已有的继续执行，新来的reject

                shutdown()⽅法是 ⼀种 很保守的 关闭线程池的⽅法

                线程池 执⾏ shutdown() 后，就会 拒绝 接收新的任务

                但是会等待线程池中 正在执行的任务 和 已经进入阻塞队列的任务 都执行完之后 才最终关闭线程池


            shutdownNow()                       // 已有的中断，新来的reject

                ⽽ shutdownNow() ⽅法，相对 激进

                线程池 执⾏ shutdownNow() 后，会 拒绝 接收新的任务

                同时还会 中断 线程池中 正在执行的任务，已经进入阻塞队列的任务 也被 剥夺了执行的机会

                不过这些 被剥夺执行机会的任务 会作为 shutdownNow() 方法的 返回值返回



------------------------------------------------------------------------------------------------------------------------
线程状态：
    新建、就绪、运行、阻塞、死亡
