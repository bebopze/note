========================================================================================================================
1、SQL执行过程
========================================================================================================================

    客户端     ->      Server端
                                   ↗   查缓存（MySQL8 已废弃）

    ->  连接器（管理连接，权限验证）  ->  分析器（词法分析，语法分析）  ->  优化器（执行计划生成，索引选择） ->  执行器（操作引擎，返回结果）

    ->  存储引擎（存储数据，提供读写接口）



    MySQL：

        1、Server层                                   // 不同的存储引擎共用一个 Server 层，也就是 从连接器到执行器 的部分

            连接器、查询缓存、分析器、优化器、执行器等

            涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等）

            所有 跨存储引擎的功能 都在这一层实现，比如 存储过程、触发器、视图 等


        2、存储引擎层                                  // 存、取     插件式

            负责 数据的存储和提取

            其架构模式是 插件式 的，支持 InnoDB、MyISAM、Memory 等多个存储引擎

            最常用的存储引擎是 InnoDB



========================================================================================================================
2、日志                        // CRUD 记录
========================================================================================================================

    1、redo log              // 引擎层 日志、InnoDB 引擎特有的日志、crash-safe、事务

        作用：
            crash-safe、事务


        目的：         // 提高写入性能   ->  通过  crash-safe、事务  保证数据完整性

            每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高

                // 写入磁盘的操作 成本过高：   ->  写入 log file  ->  就直接返回 insert/update/delete 成功    ->  后台再从 log file  --同步至-->  磁盘

                    // 所有我们先写入 一个日志file，做个缓冲        // 类似 log4j日志

                    // 读取 log  ->  解析 log  ->  写入磁盘

                    // 再从log file中，更新到磁盘（ 因为直接写入磁盘，需要经过 IO、查询、更新、维护索引...  --->  成本较高  ->  时间较慢  -->  系统效率慢 ）


        优化：

            先写日志，再写磁盘       // 也就是先写粉板，等不忙的时候再写账本

                InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了

                同时，InnoDB 引擎会在适当（空闲）的时候，将这个操作记录更新到磁盘里面


        日志 存储：  // 本质就是一个  循环队列

            本质就是一个  循环队列
            --------------------------------------
            日志存储空间有限，记满了    ->  停下来（阻塞），批量处理一批  ->  写入磁盘  ->  清除已处理日志，腾出空间


        理解：

            类比 掌柜记账

                黑板记账(日志，写入内存)    ->  打烊后（系统空闲）核算，记入账本（写入磁盘）

            -----------------------------
            黑板写满了   ->  停下来，先核算一批，写入账本  ->  擦除 -> 重新黑板记账



        crash-safe：                 // redo log 记录在 file，也是一种物理存储   并不是在内存中   ->   只要记录在了粉板上，就具备 crash-safe 的能力了

            只要写入了 log file，就已经持久化到 file 文件中了

            log file 类似 log4j 的日志文件，也是 持久化存储在 物理磁盘 上

            就算 数据库异常崩溃，log file 依然存在

            重启后，继续 读取、解析log，再持久化到磁盘



        实现：

            "循环队列"   ->  本质上是一个 循环file

            checkpoint  ->  游标



    2、binlog            // Server层 日志   ->  所有引擎 公有     、  归档

        归档日志    ->  数据恢复




------------------------------------------------------------------------------------------------------------------------
redo log、binlog 区别：         // 都是log file   ->  类似 log4j 业务日志   ->   均存储在 物理磁盘

    redo log    ->  引擎层   日志       InnoDB 独有        固定大小   循环写入(覆盖)       ->  crash-safe

    binlog      ->  Server层 日志      公有日志            无大小限制 追加写入             ->  归档


------------------------------------------------------------------------------------------------------------------------
物理日志 redo log

    保证 crash-safe 能力

    innodb_flush_log_at_trx_commit 参数设置成 1      ->      表示每次事务的 redo log 都直接持久化到磁盘

        // 这个参数建议设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失



------------------------------------------------------------------------------------------------------------------------
两阶段提交：      // InnoDB

    redo log（prepare）   ->  binlog  ->  redo log（commit）

    为了 让两份日志 之间的逻辑一致    // 两类日志 职责功能 各不相同   ->  破坏一个，就会破坏 现有 逻辑 和 功能

    -----------------------
    本质上，从设计目的：

        redo log    负责 事务

        binlog      负责 归档恢复

        各司其职，相互配合，才提供(保证)了 Mysql InnoDB 的现有功能（crash-safe、事务、归档恢复）

        如果 你非要破坏 其中一个log，自然也就破坏了 现有逻辑和功能（crash-safe、事务、归档恢复）

    ---------------------------
    两阶段提交

        是  跨系统 维持数据逻辑一致性  时常用的一个方案

        即使你不做数据库内核开发，日常开发中也有可能会用到

        如：分布式事务 的 两阶段提交

------------------------------------------------------------------------------------------------------------------------
crash-safe、两阶段提交、事务

    InnoDB 为了 提高写入效率     ->  设计了 redo log

    为了解决 redo log 和 磁盘 数据的一致性问题     ->  设计了 crash-safe

    具体：

        两阶段提交   +   事务(状态 标记)

------------------------------------------------------------------------------------------------------------------------
数据恢复：       // 依靠 ->  数据库备份  +  binlog（归档日志）

    1、找到 最近的一次全量备份（如：一天一备，也就是昨晚），恢复至临时库

    2、从 备份的时间点 开始，将binlog取出来，重放到误删表的时刻

        binlog（归档日志） -> 恢复数据：
            从 最近一次全量备份 的时间点 开始；
            到 删库跑路        的时间点 结束；

    应用：

        1、误删恢复

        2、扩容    ->  搭建备库（全量备份 + binlog）


------------------------------------------------------------------------------------------------------------------------
binlog为什么说是逻辑日志呢？它里面的内容 也是 存储成物理文件，怎么说是逻辑 而不是物理

    这样理解：

        逻辑日志 可以给别的数据库，别的引擎使用，以及大家都 讲得通（认可）这个“逻辑”

        物理日志 就只有“我”自己能用，别人没有共享我的“物理格式”


------------------------------------------------------------------------------------------------------------------------
redo log 的持久化

    实际上，为了提高 redo log 的写入效率：

        并没有 每次都直接将 redo log 写入file

        而是放到buffer中，积攒多条 再一次性刷盘

        --------------------------------------------
        innodb_flush_log_at_trx_commit      0/1

            设置成 1       ->     每次事务的 redo log  都直接 持久化 到磁盘

            建议设置成 1    ->    可以保证 MySQL 异常重启之后 redo log 不丢失     --> 保证了 数据不丢失


------------------------------------------------------------------------------------------------------------------------
binlog 的持久化

    sync_binlog     0/1

        参数设置成 1        ->    表示每次事务的 binlog  都直接 持久化 到磁盘

        也建议设置成 1      ->    可以保证 MySQL 异常重启之后 binlog 不丢失


------------------------------------------------------------------------------------------------------------------------
binlog 还不能去掉

    1、redo log 只有InnoDB有，别的引擎没有

    2、redo log是循环写的，不持久保存，binlog 的“归档”这个功能，redolog是不具备的


    3、binlog 是MySQL 原生支持的，很多三方扩展、或现有的数据同步方案  强依赖了 binlog    // 主从同步 即是通过 解析 binlog 实现的


------------------------------------------------------------------------------------------------------------------------
redo log 和 binlog  两套日志 共存

    归根结底是 历史原因

        binlog      MySQL 原生自带

        redo log    InnoDB 三方公司开发   后来被 MySQL 合并到官方版本   造成了 二者共存的局面

                    // 既然是三方公司开发，也不好直接改你的 binlog

                    // 且改动原有的设计，成本也非常大，而且你还不是官方

                    // 最优方案自然是在 MySQL 提供的扩展插件（自定义 引擎扩展）上 重新开发一套log   ->  redo log


------------------------------------------------------------------------------------------------------------------------
crash-safe 崩溃恢复：

    redo log 里面的数据有两种状态，分别是：prepare、commit

    宕机后重启

        如果 redo log 里面的数据是 commit 状态，则commit

        如果是 prepare 状态，则需要根据 binlog 来确定  数据 是回滚还是提交

            如果 binlog 中已经记录了 prepare 状态数据的逻辑修改，则commit，否则需要回滚

            // 如果 修改 已经归档到 binlog，则此数据 必须提交，因为 下游的从库 会用到binlog，这样才能保证主从一致



------------------------------------------------------------------------------------------------------------------------
crash-safe 可能情况分析：

    1-prepare阶段     2-写binlog       3-commit

    当在 2之前 崩溃

        重启恢复：没有commit，且 binlog 中不完整/不存在            ->  rollback

        备份恢复：没有 binlog

        一致

    当在 3之前 崩溃

        重启恢复：虽没有commit，但满足 prepare 和 binlog完整       ->  commit

        备份恢复：有 binlog

        一致


------------------------------------------------------------------------------------------------------------------------
redo log 和 binlog 是如何关联的

    xid     ->      通过 事务ID 关联



========================================================================================================================
3、事务隔离
========================================================================================================================

    事务：
        ACID（Atomicity、Consistency、Isolation、Durability，即 原子性、一致性、隔离性、持久性）


------------------------------------------------------------------------------------------------------------------------
隔离级别：       // 主导权在自身，这里所说的隔离指  -->  自己的变更（A会话 的更新）  未提交前 是否允许  被别人看到（B会话）

                // 隔离级别 -> 全局共享参数     ====>  实际开发中，所有线程(会话) 都为同一个隔离级别      A事务 与 B事务 彼此是否可见  -->  可类比 多线程 下的可见性问题


    读未提交（read un-committed）：                // 非阻塞、能看到

        一个事务还没提交时，它做的变更就能被别的事务看到


    读已提交（read committed）：                   // 非阻塞、看不到

        一个事务提交之后，它做的变更才会被其他事务看到


    可重复读（repeatable read）：                  // 非阻塞、看不到      实现：缓存第一次读取的值（别人改数据的事务 已经提交，我在我的事务中也 根本不去读）

        一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的

        当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的


    串行化（serializable）：                       // 阻塞

        顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”

        当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行

        ----------------------------
        串行化只有在加『读锁』或者「写锁」的时候 才会尝试加锁，而 不是事务一开始 就进行加锁


------------------------------------------------------------------------------------------------------------------------
隔离级别 - 效率  的平衡：

    隔离 得越严实     ->      效率 就会越低

    因此很多时候，我们都要在 二者之间 寻找一个平衡点

------------------------------------------------------------------------------------------------------------------------
默认隔离级别：

    官方MySQL     ->   可重复读（repeatable read）

    阿里云RDS     ->   读已提交（read committed）

    --------------------------------------------
    Oracle       ->   读已提交（read committed）

------------------------------------------------------------------------------------------------------------------------
隔离级别的理解

    读未提交：   别人改数据的事务 尚未提交，我在我的事务中 也能读到
    读已提交：   别人改数据的事务 已经提交，我在我的事务中 才能读到
    可重复读：   别人改数据的事务 已经提交，我在我的事务中 也不去读      // 类似 快照

    串行：      我的事务尚未提交，别人就别想改数据                    // 读写、写写 -> 互斥     读读并行

    这4种隔离级别，并行性能依次降低，安全性依次提高

------------------------------------------------------------------------------------------------------------------------
隔离级别的理解

    对于我读别人

        1、判断依据  ->  commit

        2、我可以改变 自身隔离级别          ->  来获取别人 未提交的数据    （牺牲 安全性）


    对于别人读我

        1、我能主导的就是   ->  要不要commit

        2、他可以改变 自身隔离级别          ->  来获取我  未提交的数据     （牺牲 安全性）


    --------------------------------
    而最终拿到的结果，都是视图给(维护)的

------------------------------------------------------------------------------------------------------------------------
关于隔离级别的理解：

    1、read uncommitted

        可以看到未提交的数据（脏读），举个例子：别人说的话你都相信了，但是可能他只是说说，并不实际做。

    2、read committed

        读取提交的数据。但是，可能多次读取的数据结果不一致（不可重复读，幻读）。用读写的观点就是：读取的行数据，可以写。

    3、repeatable read（MySQL 默认隔离级别）

        可以重复读取，但有幻读

        读写观点：

            读取的数据行不可写，但是可以往表中新增数据

            在MySQL中，其他事务新增的数据，看不到，不会产生幻读

            采用多版本并发控制（MVCC）机制解决幻读问题

    4、serializable

        可读，不可写

        像Java中的锁，写数据必须等待另一个事务结束

------------------------------------------------------------------------------------------------------------------------
事务的可见性      // 目标事务 所在会话的隔离级别

    A 能否看到 B   ->   主导权在B （即：B 想不想让 A 看到）

        A事务 能否看到 B事务的变更   ->  取决于 B事务所在会话的 事务隔离级别

------------------------------------------------------------------------------------------------------------------------
视图          // select == 视图中的结果值   ->  数据库里面会创建一个视图，访问的时候 以视图的逻辑结果 为准

    1、读未提交（read un-committed）

        无视图 一说，直接返回记录的最新值


    2、读已提交（read committed）

        每次执行sql语句之前 创建新视图
        -----------------------------------------------
            RC级别下，MVCC视图会在 每一个语句前 创建一个

                所以在RC级别下，一个事务 是可以看到另外一个事务 已经提交的内容

                因为它在每一次查询之前 都会重新给数据 创建一个新的MVCC视图


    3、可重复读（repeatable read）

        每次创建事务的时候 创建视图
        -----------------------------------------------
            RR级别下，MVCC视图 在开始事务的时候 就创建好了

            这个视图 会一直使用，直到该事务结束


    4、串行化（serializable）

        加锁  ->  阻塞  ->  禁止并行访问


------------------------------------------------------------------------------------------------------------------------
undo log        // 每个 insert/update/delete  -->  redo log  +  undo log  +  binlog

    可回溯（历史版本 undo log）：     // 类似 git

        实际上每条记录 在更新的时候 都会同时记录一条 回滚操作

        记录上的最新值，通过回滚操作，都可以得到前一个状态的值


    关联倒推（版本）：

        1 -> 2 -> 3 -> ... -> 100

    rollback：

        1 <- 2 <- 3 <- ... <- 100

    删除：

        在不需要的时候才删除

        也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除

        什么时候才不需要了呢？就是当系统里 没有比这个回滚日志更早的 read-view 的时候

        ------------------------------
        当与此相关的所有事务 都已commit


    所以不要用长事务

        长事务意味着 系统里面会存在 很老的事务视图

            由于这些事务 随时可能访问 数据库里面的任何数据

            所以这个事务提交之前，数据库里面 它可能用到的回滚记录 都必须保留，这就会导致 大量占用存储空间

        长事务还占用锁资源

------------------------------------------------------------------------------------------------------------------------
在可重复读的隔离级别下，如何理解

当系统里没有比这个回滚日志更早的 read-view 的时候，这个 回滚日志 就会被删除？

- 这也是尽量 不要使用长事务 的主要原因

    比如：

    1、在某个时刻（今天上午9:00）开启了 一个事务A（对于可重复读隔离级别，此时一个视图 read-view A 也创建了）

        这是一个 很长很长 的事务...

    2、事务A 在今天上午 9:20 的时候，查询了一个记录 R1的一个字段f1 的值为 1

    3、今天上午9:25的时候，一个事务B（随之而来的read-view B）也被开启了，它更新了 R1.f1 的值为 2（同时也创建了一个 由2到1 的回滚日志）

        这是一个短事务，事务随后就被 commit 了

    4、今天上午9:30的时候，一个事务C（随之而来的read-view C）也被开启了，它更新了 R1.f1 的值为 3（同时也创建了一个 由3到2 的回滚日志）

        这是一个短事务，事务随后就被 commit 了

    ...

    5、到了下午3:00了，长事务A 还没有 commit

        为了保证事务 在执行期间看到的数据 在前后必须是一致的，那些 老的事务视图、回滚日志 就必须一直存在，这就占用了大量的存储空间


    源于此，我们应该尽量不要使用长事务


------------------------------------------------------------------------------------------------------------------------
多版本并发控制（MVCC）：

    不同时刻启动的事务 会有不同的 read-view


    把 一致性读、当前读和行锁 串起来


========================================================================================================================
InnoDB 的 事务和锁                                                        全部默认 auto_commit = 1
========================================================================================================================
------------------------------------------------------------------------------------------------------------------------
事务的启动 时机

    begin / start transaction 命令
        并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动

        这个操作不会启动一个事务，这个操作之后，第一个对数据库的操作才会真正启动一个事务

    start transaction with consistent snapshot
        马上启动一个事务

------------------------------------------------------------------------------------------------------------------------
一致性视图（事务启动）

    begin/start transaction
        一致性视图 是在执行 第一个快照读语句 时创建

    start transaction with consistent snapshot
        一致性视图 是在执行 start transaction with consistent snapshot 时创建

------------------------------------------------------------------------------------------------------------------------
begin / start transaction

    begin                   开启事务，标识一个事务的开始。
    start transaction       开启事务


    在MySQL里，begin 和 start transaction 是等价的

------------------------------------------------------------------------------------------------------------------------
auto_commit = 1
    MySQL会 隐式的 开启和关闭事务

显式使用 begin / start + 多条语句 + commit
    是为了表明  ->  这“多条语句”是 1 个事务


------------------------------------------------------------------------------------------------------------------------
定义一个事务                                // 全部默认 autocommit = 1

    显示  ->  手动 begin/commit

        可以聚合 多条SQL 在同一个事务中，原子操作


    隐式  ->  自动 begin/commit             // 默认 autocommit = 1

        这时候，一条SQL，就是一个事务

        执行SQL 之前，InnnoDB 自动添加 begin
        执行SQL 之后，InnnoDB 自动添加 commit

------------------------------------------------------------------------------------------------------------------------
MySQL的两个“视图”
    1、查询语句 虚拟表(view)
    2、InnoDB实现MVCC时的 一致性读视图(consisitent read view)


它没有物理结构

作用：
    事务执行期间，用来定义“我能看到什么数据”

------------------------------------------------------------------------------------------------------------------------
MVCC  快照

    1、快照 范围：
        整库

    2、快照 时间基点：
        事务 启动时      ->      就 “拍了个快照”

    3、快照 怎么实现的：

         transaction id

------------------------------------------------------------------------------------------------------------------------
行记录 多版本

    undo_log 实现
        V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的
        比如，需要 V2 的时候，就是通过 V4 依次执行 U3(undo_log 3)、U2(undo_log 2) 算出来

    每行数据都有多个版本，每次事务更新数据的时候，都会生成一个 新的数据版本
    并把 transaction_id 赋值给这个数据版本的 事务ID，记为 row trx_id

    版本中包含
        本次数据的值
        事务id
        还有一个引用（指向 上一个数据版本）

    表中的一行记录
        可能有多个版本 (row)，每个版本有自己的 row trx_id


------------------------------------------------------------------------------------------------------------------------
数据版本可见性规则

    事务视图数组  ->  未提交事务集合（活跃）

        InnoDB 每个事务启动时，都会 创建一个数组
            用来保存    ->      这个事务启动瞬间，当前正在 “活跃”的 所有事务ID
            “活跃”     ->      启动了，但还没提交


    row trx_id
        根据 事务启动的时间 递增        ->  有序

                                                             低水位           高水位
                                                               ↓               ↓
    trx_id 视图数组： 保存 trx_id（有序）      ->      已提交事务  | 未提交事务集合  | 未开始事务


    低水位：
        数组内 事务ID最小值

    高水位：
        当前系统 已创建的 事务id的最大值 + 1

------------------------------------------------------------------------------------------------------------------------
秒级 创建快照

    InnoDB 利用了 “所有数据都有多个版本” 的这个特性

    根据  时间基点  切割  ->   trx_id 数组            // trx_id 根据 时间先后，递增生成

        当前事务启动时 的 时刻基点  ->  定义 低水位   + 高水位      =====> 生成当前事务的 "快照"   ->  秒级

------------------------------------------------------------------------------------------------------------------------
快照 访问逻辑         // 一致性 读！！！


                   低水位- 11                                    高水位- 21

     已提交事务                  未提交事务(视图数组 - 活跃事务)                  未开始事务

     [0,...,10]                  [3,...,20]                                 [21,+∞)

     ---------------

    --------------------------------------------------------------------------------------------------------------------
    当前事务内，select 行记录k：                  // 访问快照    ->  这里都是在说 可见性  --> 读      ==> update，是不受此约束的

        1、当前事务 trx_id = 15

        2、获取到 行记录k 当前最新版本v4 的 row trx_id = 25

            25 > 21(高水位)，v4版本 是由 将来(快照时刻 之后) 启动的事务生成的    ==>  不可见

        3、获取 v4 的上一历史版本 v3，v3 的 row trx_id = 5                  // undo_log 倒推

            低水位 < 5 < 高水位
            -------------------------------------------
            判断，trx_id=5 是否在 视图数组[3,...,20]中

                1、在     ->    v3版本 是由 还没提交 的事务生成的    ==>  不可见            // 存在于 活跃事务   [3,...,20] 中

                2、不在   ->    v3版本 是由 已经提交 的事务生成的    ==>  可见              // 存在于 已提交事务 [0,...,10] 中

------------------------------------------------------------------------------------------------------------------------
事务可见性 分析规则                  // 上面👆是原理，实际分析太绕、太复杂

一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：

    1、版本未提交，不可见；
    2、版本已提交，但是是在视图创建后提交的，不可见；
    3、版本已提交，而且是在视图创建前提交的，可见。

------------------------------------------------------------------------------------------------------------------------
事务数组        // 活跃trx_id 列表

    事务ID，虽然在生成的时候保证了 有序递增，但每个事务的 生命周期 是不确定的。
    所以，可能出现，虽然落在高、低水位之间，但却是已经提交了的事务。
    这时，就需要根据，活跃数组中具体的值(row trx_id)，来判断 行记录当前版本 是否是在当前事务之前，就已经提交的事务。

------------------------------------------------------------------------------------------------------------------------
Innodb 要保证这个规则：事务启动以前所有还没提交的事务，它都不可见。

    但是只存一个 已经提交事务的最大值(低水位) 是不够的。
        因为存在一个问题，那些比 最大值(低水位) 小的事务，之后也可能更新

    所以事务启动的时候还要保存“现在正在执行的所有事物ID列表”，如果一个row trx_id 在这列表中，也要不可见。

------------------------------------------------------------------------------------------------------------------------
当开启事务时，需要 保存活跃事务的数组（A），然后获取高水位（B）。
我的疑问就是，在这两个动作之间（A和B之间）会不会产生新的事务？如果产生了新的事务，那么这个新的事务相对于当前事务就是可见的，不管有没有提交。

    是加了锁的，保证了A-B的原子操作：

        代码实现上，获取视图数组 和 高水位，
        是在 事务系统的 锁保护下 做的，可以认为是原子操作，期间不能创建新事务。



========================================================================================================================
update 逻辑               // update -> 读最新值               ==> "当前读"
========================================================================================================================

事务视图的 update
    更新数据的时候，不再理会 "历史版本"， 直接读取 -> 当前(最新)值


“当前读”（current read）

    直接读取 -> 当前(最新)值
    ------------------------------
    update 都是 先读后写
    而这个读，只能读 当前(最新)值，称为“当前读”（current read）

========================================================================================================================
当前读             // 当前读  ->  读当前(最新)值        ==> 本质上是 加锁 -> 获取 被lock保护的资源 的最新值    -> 表现上，就是 不再理会 "历史版本"
========================================================================================================================

触发 当前读              // 读 最新值

    1、update 自动触发                   // 本质上也是 加锁 触发（默认加了 写锁）

    2、select 手动加锁 主动触发

        1、读锁（S 锁，共享锁）           ->  lock in share mode
        2、写锁（X 锁，排他锁）           ->  for update


除 update 外，select 加锁，也是 当前读

    select k from t where id=1   lock in share mode;
    select k from t where id=1   for update;

------------------------------------------------------------------------------------------------------------------------
当前读 的本质是 加锁(行锁)保护

    根据 两阶段锁协议
        如果A线程 持有锁 没释放
        而B线程 通过update/select 加锁(竞争锁)，触发 当前读 -> 想要获取 最新值

        此时，B线程-会阻塞，等待 A线程-释放锁

        ----------------------
        加锁角度分析：
            锁没释放    ->  锁等待

        结果角度：
            想要获取 最新值，而最新值还没提交，只能等待其提交

            --------------------------------------
            A线程 可commit，也可 rollback
            B线程，只能等待 A事务结束(commit/rollback)


------------------------------------------------------------------------------------------------------------------------
一致性读、当前读、行锁

    一致性读        快照值(历史值)       事务视图 快照                      ->  事务启动，创建 事务视图 快照 - trx_id 事务数组 快照

    当前读          当前值(最新值)       update、select加锁(行锁)           ->  update 行锁  、 select 行锁

    --------------------------------------------------------------------------------------------------------------------
    行锁            当前值(最新值)       通过加 update/select 行锁   ->   触发 "当前读"   -> 具体实现：通过(行)锁竞争，触发 lock wait



------------------------------------------------------------------------------------------------------------------------
一致性读


------------------------------------------------------------------------------------------------------------------------
事务隔离的本质实现: 两阶段锁协议 + MVCC

------------------------------------------------------------------------------------------------------------------------
事务的可重复读的能力是怎么实现的？

    可重复读 的核心就是 一致性读（consistent read）

    而事务 更新数据 的时候，只能用 当前读，如果 当前记录的行锁 被其他事务占用 的话，就需要进入 锁等待

------------------------------------------------------------------------------------------------------------------------
可重复读    ->      一致性读        - 快照

读已提交    ->      类似 可重复读
------------------------------------------------------------------------------------------------------------------------
区别：
    1、可重复读      ->      1个  一致性视图
    2、读已提交      ->      N个  视图                  // 1个SQL    ->  1个视图

    --------------------------------------------------------
    可重复读
        只需在 事务开始的时候 创建一致性视图，之后事务里的其他查询 都共用 这个一致性视图

    读已提交
        每一个语句执行前 都会重新 算出一个新的视图

------------------------------------------------------------------------------------------------------------------------
读已提交
    start transaction with consistent snapshot  ==  start transaction           // just事务标识，真正的事务 -> SQL执行前开启

    --------------------------------------------------------
    start transaction with consistent snapshot
        启动事务，并创建一个 持续整个事务的 一致性快照
        在[读已提交]隔离级别下，这个用法就没意义了
        等效于普通的 start transaction


------------------------------------------------------------------------------------------------------------------------
读已提交 VS 可重复读                        // 注意，这里都是在说 "读" - select

    从资源利用(视图维护)角度，就清晰了             // 维护视图，肯定是有成本的

        读已提交        每次select   ->  开启1个  新视图                 ->  数据安全(拿到最新 持久化值)      并发度 低

        可重复读        事务启动时    ->  开启1个  全局 一致性视图         ->  数据脏读(拿到 历史值)           并发度 高

------------------------------------------------------------------------------------------------------------------------
总结
    一致性读，会根据 row trx_id 和 一致性视图(<低水位) 确定数据版本的 可见性                   // 普通select语句，非加锁(当前读)

    可重复读    ->  查询只承认   在事务启动前 就 已经提交 完成的数据；    // 事务 前 提交
    读已提交    ->  查询只承认   在语句启动前 就 已经提交 完成的数据      // SQL 前 提交


    当前读，总是读取 已经提交完成 的 最新版本


------------------------------------------------------------------------------------------------------------------------
思考

    行数据，支持 可重复度(快照读) + 当前读

    表结构，为什么 不支持 可重复度(快照读)？

        因为，表结构，没有对应的 行数据，也没有 row trx_id（表结构 版本 -> undo_log），因此只能遵循 当前读 的逻辑.

        ------------------------------------------------------------
        想要实现，其实也是可以的，参考 行记录 的 一致性视图(快照)

            MySQL 8.0 已经可以把 表结构 放在 InnoDB 字典里了，也许以后会支持表结构的可重复读.





========================================================================================================================
4、索引
========================================================================================================================
功能：
    快速查询

------------------------------------------------------------------------------------------------------------------------
数据结构模型：

    1、hash表

        无序  ->      不支持区间查

            等值查     ->  O(1)

            区间查    ->  遍历 O(n)



    2、有序数组

        有序：

            等值、区间   ->  二分查找O(logN)

        缺点：

            插删 数组全量移位


    3、二叉搜索树                     // 瘦高

        有序：
            支持区间查

        ---------------
        查：
            O(logN)

        插、删：

            O(logN)

        ---------------
        缺点：

            二叉  ->  分支少     ===>  树高 过高（树高 = IO 次数）   ->  IO 多


    4、N叉树       // B+树                矮胖

        1200^3 ≈ 17亿

            root 数据块 总是在内存中，且第二层 也大概率在内存中

            最多只需要 3次 IO   ->   就能查找到 17亿表中的数据

------------------------------------------------------------------------------------------------------------------------
索引模型

    索引在 引擎层实现   ->      不同引擎，实现各不相同，并没有统一规范

------------------------------------------------------------------------------------------------------------------------
InnoDB 的索引模型

    主键索引    ->  整行数据

    普通索引    ->  ID（回表）


    尽量使用主键ID查询：

        普通索引查询  ->  拿到ID   --回表-->   主键索引 获取整行数据


    尽量使用 整型自增主键：

        从性能和存储空间方面考量，自增主键往往是更合理的选择

            bigint -> 8个字节

            主键ID 占用空间越小  ->  非主键索引 占用的空间越小


------------------------------------------------------------------------------------------------------------------------
覆盖索引
    索引上的信息足够满足查询请求，不需要再回表。
------------------------------------------------------------------------------------------------------------------------
联合索引

    节省空间    ->  多棵树（多个B+索引树），合并一颗树                       // 索引合并

    回表优化    ->  idx__idCard_name：  根据 id_card -> name  不用回表     // 索引冗余

------------------------------------------------------------------------------------------------------------------------
索引的维护

    页分裂/页合并：

        一个数据页满了

        按照B+Tree算法，新增加一个数据页，叫做页分裂，会导致性能下降      // 新增一层树高  ->  原有树结构 会重新排列

        空间利用率降低大概50%

        当相邻的两个数据页 利用率很低的时候 会做数据页合并，合并的过程 是分裂过程的逆过程


------------------------------------------------------------------------------------------------------------------------
“N叉树”的N值在MySQL中是可以被人工调整的么
              count
    N = ——————————————————
            page size

    1、数据总数一定，页越大，一页存放的索引越多，N越大，树高越低

    2、页大小固定，索引(key值)占用空间越小，存储索引越多，N越大，树高越低


    ------------------------------------------
    1、通过改变key值来调整
        N叉树中非叶子节点存放的是索引信息，索引包含Key和Point指针。
        Point指针固定为6个字节，假如Key为10个字节，那么单个索引就是16个字节。
        如果B+树中页大小为16K，那么一个页就可以存储1024个索引，此时N就等于1024。
        我们通过改变Key的大小，就可以改变N的值
    2、改变页的大小
        页越大，一页存放的索引就越多，N就越大。

------------------------------------------------------------------------------------------------------------------------
索引树的理解：

    每一张表 其实就是几个B+树    // 1个主键树  +  n个普通索引树

        树结点的 key值 就是某一行的主键，value是该行的其他数据

        新建索引 就是新增一个B+树


    查询 走主键索引   ->  就是查询 主键索引B+树

    查询 走普通索引   ->  就是查询 普通索引B+树       ==>  回表 -> 走主键索引

    查询不走索引      ->  就是 遍历主B+树


------------------------------------------------------------------------------------------------------------------------
搜索树

    搜索树 已经很接近 跳表

        跳表 其实就是站在 平衡搜索树的肩膀上 改进而来的


    B+树 类比 跳表  的存、取：

        上层的结点         ->  为了查询 而建立的多级索引

        最底层的 叶子结点   ->  才是 数据(行)

------------------------------------------------------------------------------------------------------------------------
联合索引

    覆盖索引

    最左前缀原则

    索引下推优化




========================================================================================================================
change buffer           // 索引更新         ->      change buffer + 异步merge(刷盘)
========================================================================================================================

------------------------------------------------------------------------------------------------------------------------
索引 查询过程
------------------------------------------------------------------------------------------------------------------------

数据页内部
    数据页(结点)内部是单链表，通过二分法来定位记录(元素)

读写的单位
    数据页(结点)         // InnoDB 数据页大小 默认 16KB

    读一条记录的时候，并不是将 这个记录本身 从磁盘读出来，而是以页为单位，将其整体读入内存.

------------------------------------------------------------------------------------------------------------------------
索引 更新过程
------------------------------------------------------------------------------------------------------------------------

索引更新    ->  更新 B+树的 数据页

    1、数据在内存中
        直接更新内存

    2、不在内存中
        change buffer 记录更新操作，然后定时刷新到磁盘， 或读取数据的时候，更新读入内存的数据并刷新.

change buffer

    记录更新操作，相当于redo_log，做更新的中介，延缓更新，减少IO.

    ----------------------------------------------
    因为更新，涉及到数据页从 磁盘->内存 的IO

    同步操作
        性能差，频繁IO，还会造成系统阻塞.

    改为异步
        减少随机磁盘访问，提升 更新性能.


索引 insert/update/delete           // 类似 行数据的写入  ->  redo_log + 异步刷盘

    change buffer(写内存) + 异步merge（写磁盘  ->  后台线程 异步merge、访问数据页 主动触发merge、数据库正常shutdown 触发merge）

    更新 的时候，将 更新操作写入 change buffer 中，就算更新完成.



------------------------------------------------------------------------------------------------------------------------
将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。
而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。

    数据读入内存，是会将数据所在的 内存页 加载进内存，
    而更新操作记录只是 将该记录 存起来，内存占用相对较小.


------------------------------------------------------------------------------------------------------------------------

索引的更新方式

    1、同步更新                  // 唯一索引

        唯一索引        需校验唯一性，而这必须要将数据页读入内存才能判断

    2、异步更新                  // 普通索引

        写内存(change buffer)  +   异步merge


异步目的

    更新数据，是需要将数据先加载到内存，才能做更新的。



    异步merge -> 数据页 加载到内存，和 change buffer 记录 merge，再刷盘.



适用 普通索引，不适用 唯一索引

    1、数据页在内存中：

        性能相差无几.

        唯一索引，只多一步校验.

    2、数据页在不内存中：

        唯一索引，需要将数据页读取到内存，判断冲突

            走同步更新 -->  然后直接更新

        普通索引，无需读取内存

            走异步更新 -->  直接将更新记录在 change buffer


唯一索引
    由于要判断冲突，因此每次更新，都需要将数据页加载到内存，做唯一性校验。

    将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一.
    change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的.



性能
    一般，除非必要，最好不要用 唯一索引.

    唯一索引的唯一性校验，频繁读取数据页，频繁的磁盘IO，性能低下.

    如果是业务频繁写入操作的话，唯一索引，频繁IO，会造成数据库的阻塞.



------------------------------------------------------------------------------------------------------------------------
innodb 普通索引 修改成 唯一索引，产生的生产事故

    写多读少使用 change buffer，可以加快执行速度(减少数据页磁盘 io);
    但是，如果业务模型是 写后立马会做查询，则会触发 change buffer 立即 merge 到磁盘。
    这样 的场景磁盘 io 次数不会减少，反而会增加 change buffer 的维护代价。



------------------------------------------------------------------------------------------------------------------------
普通索引和唯一索引

    查询性能几乎一样, 但是写性能是普通索引快, 因为可以用到change buffer, 唯一索引会导致内存命中率下降


------------------------------------------------------------------------------------------------------------------------
change buffer 用的是 buffer pool 里的内存，因此不能无限增大


------------------------------------------------------------------------------------------------------------------------
merge之前变更得越多，收益越大

对于写多读少的业务场景，比如日志，账单的业务，change buffer能提升性能
------------------------------------------------------------------------------------------------------------------------
写入后立马查询
    更新操作 先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程

    样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价

    change buffer 反而起到了副作用

------------------------------------------------------------------------------------------------------------------------
redo log 与 change buffer(含磁盘持久化) 这2个机制，

    不同之处在于 ———— 优化了 整个变更流程 的 不同阶段

    ---------------------------------------------------------------
    先不考虑redo log、change buffer机制，

    简化抽象一个变更（insert、update、delete）流程：
        1、从磁盘读取 待变更的行 所在的数据页，读取至内存页中；
        2、对内存页中的行，执行变更操作；
        3、将变更后的数据页，写入至磁盘中；

        ------------------------------
        步骤1，涉及 随机读 磁盘IO；
        步骤3，涉及 随机写 磁盘IO；

    ---------------------------------------------------------------
    change buffer 机制， 优化了步骤1    —>  避免了随机读磁盘IO

    redo log      机制， 优化了步骤3    —>  避免了随机写磁盘IO，将随机写磁盘，优化为了顺序写磁盘(写redo log，确保crash-safe)


    ---------------------------------------------------------------
    在我们 innodb 中， change buffer 机制不是一直会被应用到，
    仅当 待操作的数据页 当前不在内存中，需要先读磁盘加载数据页时，change buffer 才有用武之地。
    redo log 机制，为了保证crash-safe，一直都会用到。

    ---------------------------------------------------------------
    有无用到change buffer机制，对于redo log这步的区别在于：
        用到了 change buffer 机制时，在redo log中记录的本次变更，是记录 new change buffer item 相关的信息，
        而不是直接的记录物理页的变更。


========================================================================================================================
analyze table t                                                 // 重新统计 索引信息
========================================================================================================================

1、MySQL 选错索引

    我们认为使用K索引检索的速度会更快的，但是MySQL没有使用，决定使用什么索引是 由Server层的优化器 来决定的.
    它也是想选择最佳的方案来检索数据的，不过它也是人写的程序也是存在bug的。

2、MySQL为啥会选错索引？

    优化器认为使用那个索引检索数据的速度比较快是一个需要各种因素综合评估的事情，

    比如：
        是否使用临时表、是否排序、扫描的行数多少、回表的次数等，
        文中的例子优化器判断失误的主要原因是扫描行数的判断存在误差，因为这个信息是采样评估得到的。

        索引的创建是非常的耗时的，因为需要真正的建索引的过程，
        但是删除索引却不需要耗费太多时间，因为是标记删除，这个是以空间换时间的思路。

        优化器采用采样评估出现误差的原因也在于，索引的标记删除影响的。

3、MySQL选错索引怎么破？
    1、force index 强行选择一个索引，不常用、不推荐用
    2、调整SQL语句，使优化器选择的和我们想的一样，不具有通用性
    3、新建更合适的索引或者删除不合适的索引，是一个思路
    4、使用 analyze table 可以解决 索引统计信息不准确 导致的索引选错的问题




-------------------------------
show table status

analyze table t

force index(idx)

    select * from t force index(a) where a between 10000 and 20000;


========================================================================================================================
5、锁
========================================================================================================================

1、全局锁     ->  Server层 实现

    锁整个数据库

    加全局读锁
        FTWRL   ->  Flush tables with read lock

    让整个库处于只读状态
        阻塞增删改、阻塞表结构变更

    场景：
        全库逻辑备份


    全库只读：

        1、set global readonly=true

            readonly 的值会被用来做其他逻辑，如：判断主库还是备库

            修改 global 变量的方式影响面更大，不建议你使用

        2、FTWRL

            异常风险：
                FTWRL 命令之后由于客户端发生异常断开，MySQL 会自动释放这个全局锁

                将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，
                这样会导致整个库长时间处于不可写状态，风险较高

------------------------------------------------------------------------------------------------------------------------
2、表级锁     ->  Server层 实现

    1、表锁                                         // 显示锁  ->  主动 加/释放锁

        加/释放 锁：
            手动加锁  ->  lock   tables ... read/write
            手动解锁  ->  unlock tables

        ---------------------------------------------------------
        read     ->  线程 获取到 read 的权限
        write    ->  线程 获取到 write + read（能写就能 读） 的权限

        ---------------------------------------------------------
        lock tables t1 read, t2 write

            当前线程 可以 read          表t1
            当前线程 可以 write + read  表t2

            其他线程  write          表t1  -> 阻塞
            其他线程  read + write   表t2  -> 阻塞


    --------------------------------------------------------------------------------------------------------------------
    2、MDL（meta data lock）  元数据锁                 //  隐式锁                  表级锁  ->   增删改查 时，不允许修改表结构

        加/释放 锁：
            增删改查   时     ->  自动加  MDL read锁     ==>  禁止 修改表结构
            修改表结构 时     ->  自动加  MDL write锁    ==>  禁止 增删改查

        -----------------------------------------------------------------
            A 申请 MDL read锁
            B 申请 MDL read锁
            C 申请 MDL write锁     ->  阻塞          ===>    加等待时间，不要阻塞后面的 MDL read锁 线程
            D 申请 MDL read锁      ->  阻塞
            ...

        -----------------------------------------------------------------
        长事务
            MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。

------------------------------------------------------------------------------------------------------------------------
3、行锁      ->  引擎层 实现                                          InnoDB 行锁   ->  隐式锁

    InnoDB 行锁
        隐式锁     ->      自动 加锁，锁住行记录

    细粒度锁
        减少锁冲突，提升业务并发度

    两阶段锁
        执行SQL时加锁，事务结束(commit/rollback) 才释放
    -----------------------------------------------------------
    加/释放 锁：
        加锁：
            update t set k=k+1 where id = 1;        // InnoDB 自动加行锁，锁住 id=1 的行记录

        释放锁：
            commit/rollback                         // 事务结束时，才(自动)释放锁
    -----------------------------------------------------------
    死锁
        资源循环依赖

    死锁检测（耗CPU资源）
        锁资源申请不到时，触发死锁检测                                         // 即：他要 加锁访问的行 上有锁，他才要检测
            检测 获取锁的线程，是否有 资源循环依赖 问题
            是 -> 则判定为死锁，随机释放一个线程持有的资源，破解僵局！

    解决死锁：
        1、超时自动释放
        2、开启 死锁检测（吃CPU）

    预防死锁：
        1、转移 上层/中间件 限流/排队
        2、业务上优化





========================================================================================================================
6、order by      // 排序                                                       内存/磁盘
========================================================================================================================

内存/临时文件 排序：

    1、在内存中排序                        // 低于内存限制，在内存中排序

        1、线程安全

            为每个线程 独立分配一份 sort_buffer 内存空间

        2、字段排序

            要排序的字段，加载入 sort_buffer，排序，返回结果集


    2、在磁盘中排序                        //  超过内存限制，在磁盘文件排序      ->  文件 归并排序

        超过 sort_buffer 空间限制，走外部排序 ————> 磁盘临时文件排序

        磁盘文件会 分为多份，采用 归并排序



排序方式：

    1、全字段排序

        1、一次性加载所有字段，放入 内存/临时文件

        2、排序完成，直接返回结果集                  // 无需 回表

    2、rowid排序

        1、只加载 需要参与排序的字段 + 主键ID，放入 内存/临时文件

        2、排序完成，再根据 主键ID，回表  ->  返回所有字段



========================================================================================================================
7、高可用、主备、读写分离
========================================================================================================================



========================================================================================================================
8、join
========================================================================================================================



========================================================================================================================
9、临时表
========================================================================================================================



========================================================================================================================
10、InnoDB、Memory
========================================================================================================================





========================================================================================================================
10、自增主键
========================================================================================================================




========================================================================================================================
11、insert 锁
========================================================================================================================




========================================================================================================================
12、复制一张表
========================================================================================================================


========================================================================================================================
13、分区表
========================================================================================================================


========================================================================================================================
函数
========================================================================================================================
函数  ->  索引失效

    1、条件字段函数操作
    2、隐式类型转换
    3、隐式字符编码转换

-----------------------------------------------------------------------
对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。

-----------------------------------------------------------------------
索引字段不能进行函数操作，但是索引字段的参数可以玩函数，一言以蔽之

    ---------------------------------------------
    不能      ->      fun(a) =  1
    可以      ->        a    =  fun(1)
    ---------------------------------------------
    表中索引字段 不能加函数，可能会导致索引丢失有序性
    但是 入参  可以随便操作，因为不影响索引有序性

========================================================================================================================
Extra                            - https://dev.mysql.com/doc/refman/5.7/en/explain-output.html#explain-extra-information
========================================================================================================================
Using where                 ->  WHERE                           ->  筛选过滤
Using index                 ->  index                           ->  索引覆盖
Using index condition       ->  index                           ->  索引下推
Using filesort              ->  排序                            ->  非自然有序，需借助外部排序(内存/文件)
Using temporary             ->  临时表





【Using filesort】    本次查询语句中有order by，且排序依照的字段不在本次使用的索引中，不能自然有序。需要进行额外的排序工作。

【Using index】       使用了覆盖索引 ———— 即本次查询所需的所有信息字段，都可以从利用的索引上取得。无需回表。

                      - 当查询仅使用属于单个索引的列时，可以使用此策略

【Using index condition】 使用了索引下推技术ICP


                        虽然本次查询所需的数据，不能从利用的索引上完全取得，还是需要回表去主索引获取。
                        但在回表前，充分利用索引中的字段，根据where条件进行过滤，提前排除了不符合查询条件的列。
                        这样就减少了回表的次数，提高了效率。


 【Using where】 表示本次查询要进行筛选过滤。

