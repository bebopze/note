========================================================================================================================
基础
========================================================================================================================


------------------------------------------------------------------------------------------------------------------------
高并发系统设计 的三种通用方法：

    1、Scale-out（扩展）
        数据库一主多从、分库分表、存储分片

    2、缓存

        整个计算机体系中  ->  磁盘是最慢的一环

    3、异步

        调用方法执行完毕后，再通过  回调、事件通知 等方式   ->   将结果 反馈给调用方

        场景：
            12306  ->   查询余票、下单、退票

            1、接收请求 ==> 先返回suc（告知用户请求成功，系统正在后台处理）

            2、后台处理成功后   ==>   微信/短信/邮箱/站内信 通知用户    或    用户自己刷新页面 获取异步处理结果


------------------------------------------------------------------------------------------------------------------------
架构分层


应用：
    OSI网络模型  七层
    TCP/IP协议   四层
    Linux 文件系统


好处：
    1、简化系统设计   ->   让不同的人专注做某一层次的事情
    2、高复用
    3、横向扩展


缺点：
    复杂度



------------------------------------------------------------------------------------------------------------------------
如何提升系统性能


三高：
    高并发、高性能、高可用


高并发系统设计的 三大目标：

    高性能、高可用、可扩展



========================================================================================================================
数据库
========================================================================================================================

池化技术：
    减少频繁创建数据库连接的性能损耗

------------------------------------------------------------------------------------------------------------------------
数据库优化方案：

    查询请求增加时     ->   做主从分离

    写入数据量增加时   ->   如何实现分库分表

------------------------------------------------------------------------------------------------------------------------
发号器：

    保证分库分表后ID的全局唯一性

        1、Snowflake 算法落地

        2、美团Leaf（分布式ID生成系统）             // QPS近5万

        3、微信序列号生成器                        // QPS1000万以上

        4、百度开源的UidGenerator                 // （仅支持单机部署）使用Snowflake算法，单机QPS可达600万


        ------------------------------------
        2、UUID的问题：

            1、生成的ID 最好具有单调递增性，也就是有序的     // 在系统设计时，ID 有可能成为排序的字段

            2、ID 有序也会提升数据的写入性能               // B+树 是有序的    ==>   有序 ->  顺序写

            3、它不具备业务含义                           // ID按自定义规则   ==>   生成时间、哪个机房的发号器、哪个业务服务...

            4、32位 字符串  ->  耗费空间


            场景：
                生成 Request ID 来标记单次请求           // 不依赖于任何第三方系统，所以在性能和可用性上都比较好




------------------------------------------------------------------------------------------------------------------------
NoSQL                   在高并发场景下，数据库 和 NoSQL 互补



    存储服务：

        1、读写能力

            提升它的读写性能  ->   尤其是 读性能   ==>  NoSQL

        2、扩展能力

            增强它在存储上的扩展能力  ->  从而应对 大数据量 的存储需求


    ---------------------------------------------------------------------
    NoSQL

        读多写少的产品：
            微信朋友圈、微博、淘宝                 // 查询QPS 远远大于 写入QPS



        分类：

            KV存储
                Redis、LevelDB

            列式存储
                Hbase、Cassandra

            文档型
                 MongoDB、CouchDB

            Elasticsearch
                倒排索引


        优点：

            性能
                将对磁盘的随机写转换成顺序写

                内存数据库 高效读

            在某些场景下
                全文搜索

            扩展性
                天生支持分布式、支持数据冗余、数据分片

        缺点：


------------------------------------------------------------------------------------------------------------------------
数据的迁移                                                                                       // 双写、级联同步
------------------------------------------------------------------------------------------------------------------------

1、双写

    步骤：
        1、新库配置为源库的从库   ->   用来同步数据

            如果需要将数据同步到多库多表，那么可以使用一些第三方工具获取 Binlog 的增量日志（比如开源工具 Canal）
            在获取增量日志之后就可以按照分库分表的逻辑写入到新的库表中了。

        2、改造业务代码

            在数据写入的时候不仅要写入旧库也要写入新库。
            当然，基于性能的考虑，我们可以异步地写入新库，只要保证旧库写入成功即可。
            但是我们需要注意的是，需要将写入新库失败的数据记录在单独的日志中，这样方便后续对这些数据补写，保证新库和旧库的数据一致性。

        3、校验数据

            由于数据库中数据量很大，做全量的数据校验不太现实。
            可以抽取部分数据，具体数据量依据总体数据量而定，只要保证这些数据是一致的就可以。

        4、读流量 切换到 新库

            由于担心一次切换全量读流量可能会对系统产生未知的影响

            所以这里最好采用灰度的方式来切换

                比如开始切换 10% 的流量，如果没有问题再切换到 50% 的流量，最后再切换到 100%

        5、双写 切回旧库 故障保障

            由于有双写的存在，所以在切换的过程中出现任何的问题都可以将读写流量随时切换到旧库去，保障系统的性能

        6、前面切换 新库

            在观察了几天发现数据的迁移没有问题之后，就可以将数据库的双写改造成只写新库，数据的迁移也就完成了


    注意：
        最容易出问题的步骤   ->   就是 数据校验

        未开始迁移数据之前先写好数据校验的工具或者脚本，在测试环境上测试充分之后，再开始正式的数据迁移。


    场景：
        迁移 MySQL、Redis、MQ


    应用：
        将数据从自建机房迁移到云   ==>   带宽延迟  ->  减少 跨专线读



2、级联同步

    应用：
        将数据从自建机房迁移到云


    步骤：
        1、新库  配置为  旧库的从库   ->   用作数据   同步

        2、备库  配置为  新库的从库   ->   用作数据的 备份

        3、读流量 切换到 新库

            三个库的写入一致后  ->  将数据库的 读流量切换到新库

        4、写入流量 切换到 新库

            暂停应用的写入   ->   将业务的写入流量 切换到 新库

                // 由于这里需要 -> 暂停应用的写入    ==>    所以需要安排在  ->  业务的低峰期


        5、回滚

            1、读流量切换到备库，再暂停应用的写入

            2、将写流量切换到备库

            -----------------------------------------------------------------------
            所有的流量都切换到了备库，也就是又回到了自建机房的环境，就可以认为已经回滚了。


    场景：
        MySQL、Redis

    优势：
        简单易实施

    缺点：
        切写的时候需要短暂的停止写入      ->      对于业务来说是有损的

            // 在业务低峰期来执行切写  ->  对业务影响降至最低



------------------------------------------------------------------------------------------------------------------------
缓存迁移
------------------------------------------------------------------------------------------------------------------------


缓存迁移的重点是

    保持缓存的热度

使用副本组预热缓存




========================================================================================================================
缓存
========================================================================================================================


------------------------------------------------------------------------------------------------------------------------
数据查询瓶颈
------------------------------------------------------------------------------------------------------------------------

缓存（cache）：
    存储数据的组件，它的作用是  ->  让对数据的请求更快地返回

    应用：
        HTTP 协议
        抖音 一次后台提前缓存几个视频


缓冲区（buffer）：
    一块临时存储数据的区域，这些数据 后面会被传输到 其他设备上      ->   批量提交


缓存分类：

    静态缓存        ->      负载均衡层

    分布式缓存      ->      应用层和数据库层之间

    热点本地缓存    ->      应用层


缓存的不足：

    读多写少

    复杂度

    数据不一致

    内存并不是无限

    运维成本



重点：

    1、将请求尽量挡在上层   ->   因为越往下层，对于并发的承受能力越差

    2、缓存命中率

        是我们对于缓存最重要的一个监控项，越是热点的数据，缓存的命中率就越高



------------------------------------------------------------------------------------------------------------------------
缓存的读写策略
------------------------------------------------------------------------------------------------------------------------

缓存并发更新  ->  数据不一致：

    变更DB 和 变更cache  ->  两个独立的操作    ->  没有 并发控制   ==>   数据不一致


数据不一致：

    1、只要不是原子操作  ->  就一定会存在 数据不一致

    2、没有绝对完美的技术

        要数据一致   ->      关系型DB
        要高效       ->      缓存

        实际应用      ==>       mixed（cache + DB）



降低 数据不一致 的策略：

    1、Cache Aside（旁路缓存）策略                   // 最常用      ==>  适用  分布式缓存

        更新DB + 删除cache      ==>     以数据库中的数据为准，缓存中的数据是按需加载


        读策略：

            从cache中读取数据
            如果缓存命中，则直接返回数据
            如果缓存不命中，则从DB中查询数据
            查询到数据后，将数据写入到缓存中，并且返回给用户


        写策略：

            更新数据库中的记录
            删除缓存记录

        -----------------------------
        频繁更新缓存：

            1、分布式锁         ->  原子操作

            2、较短的 TTL       ->  缩短不一致时间




    2、Read/Write Through（读穿 / 写穿）策略         // 不常用      ==>   本地缓存 可尝试

        向直接上级汇报   ->   禁止越级汇报

        不常用：

            1、分布式缓存 Redis、Memcached 不支持写DB

            2、Guava Cache  ->  Loading Cache      ==>  有 Read Through 策略的影子



    3、Write Back（写回）策略                       // 日常应用 不适用        ==>   只写cache  +  异步写入DB

        核心思想：

            1、写入数据时 只写入cache

            2、标记为“脏”

            3、“脏块” 只有被再次使用到时  ->  再刷盘(DB)


        场景：
            计算机体系解决方案  -> 向磁盘中写数据   ==>     Page Cache、日志的异步刷盘、消息队列中消息的异步写入磁盘

        优势：
            避免了直接写磁盘 造成的随机写问题

        缺点：
            掉电之后，之前写入的文件会  有部分丢失  ->  Page Cache 还没来得及刷盘

        一般做法：
            定时刷、批量刷




------------------------------------------------------------------------------------------------------------------------
缓存的高可用
------------------------------------------------------------------------------------------------------------------------

分布式缓存的高可用方案

    1、客户端方案：                            // Client（ 调用Cache服务的 模块代码   ->   非用户端）

        在客户端  配置多个缓存的节点，通过 缓存写入和读取算法策略 来实现分布式，从而提高缓存的可用性


    2、中间代理层方案：                         // Proxy     ->      中间件

        在 应用代码和缓存节点 之间  ->  增加 代理层

        客户端所有的写入 和 读取的请求   ->  都通过代理层  ->  而代理层中 会内置高可用策略，帮助提升缓存系统的高可用


    3、服务端方案：                            // Server

        就是 Redis 2.4 版本后提出的 Redis Sentinel 方案


------------------------------------------------------------------------------------------------------------------------
总结：

    实现高可用的核心   ==>   依旧是集群     ->     多个缓存节点，提高容错率


    客户端实现：

        由客户端的策略  ->  决定如何写缓存、如何读缓存

        性能高，但是逻辑复杂，无法跨平台。


    中间件实现：

        所有客户端先 -> 访问中间件  ->  然后中间件 决定了缓存策略

        因为引入了中间件
            所以性能较差，但是可以跨平台，并且有能力的公司还可以自研中间件。


    服务端实现：

        主从切换  ->  由服务端实现

        最大的缺点是  ->  增加了运维成本





------------------------------------------------------------------------------------------------------------------------
缓存穿透                                                                                    // null  或  布隆过滤器
------------------------------------------------------------------------------------------------------------------------

命中率
    对于缓存来说  ->  命中率是它的生命线

    核心缓存的命中率      ->   要保持在 99% 以上
    非核心缓存的命中率    ->   90%

缓存穿透
    低缓存命中率  ->  请求 穿透缓存  到数据库


缓存：
    二八法则


两种解决方案：

    1、回种空值

        访问DB中 并不存在的数据  ->  无论查询多少次  永远都无数据   ==>   穿透永远都会发生

        思路：
            缓存  ->   null

        缺点：
            大量的空值缓存   ->   占用 缓存空间      ==>   还会剔除 有效值cache


    2、布隆过滤器

        布隆过滤器的算法：

            判断一个元素是否在一个集合中


            基本思路：               // hash表       ==>     二进制 数组  +  Hash算法

                1、集合中的每一个元素   ==>   hash(value)   ->  取模  ->  idx

                2、数组idx位置    ->   置为 1（默认0）      // 1-true   0-false

                3、判断流程

                    hash(val)   ->  取模  ->  idx  ==>  1/0

        缺点：
            1、hash冲突

            2、不支持删除元素

                hash(A)  ==  hash(B)    ===>   都位于 同一位置   ->  便无法删除



        优点：
            一旦布隆过滤器  判断这个元素不在集合中时，它就一定不在集合中


        ------------------------
        解决hash冲突：

            1、链表

            2、多个hash算法    ==>    hash_1(val) -> 1    &&    hash_2(val) -> 1    &&    hash_3(val) -> 1

                都为1  ==>  才为 true


------------------------------------------------------------------------------------------------------------------------
缓存并发穿透：

    1、分布式锁   ==>   拿到锁的  ->  去DB加载缓存

    2、缓存失效   ->  启动后台线程 ->  到DB 加载cache   ==>  在此之前，此缓存都直接返回null



------------------------------------------------------------------------------------------------------------------------
布隆过滤器的使用上，我也给你几个建议：

    1、选择多个hash函数  ->  计算多个 hash值   ==>   这样可以减少误判的几率

    2、布隆过滤器会消耗一定的内存空间

        在使用时  需要评估你的业务场景下 需要多大的内存，存储的成本是否可以接受






------------------------------------------------------------------------------------------------------------------------
CDN                                                                         // 静态资源 缓存    ->    就近访问
------------------------------------------------------------------------------------------------------------------------

CDN（Content Delivery Network/Content Distribution Network，内容分发网络）

    将静态的资源  分发到   位于多个地理位置机房中的服务器上

    很好地解决   ->   数据 就近访问 的问题   ->   也就加快了 静态资源的访问速度



CDN系统：

    1、如何将用户的请求 映射到CDN节点上

    2、如何根据用户的地理位置信息 选择到比较近的节点


如何让用户的请求到达 CDN节点：

    DNS  ->  域名解析


如何找到离用户最近的 CDN节点：

    GSLB（Global Server Load Balance，全局负载均衡）

        对于部署在不同地域的服务器之间做负载均衡   ->   可能管理了很多的本地负载均衡组件

            1、它是一种负载均衡服务器，负载均衡，顾名思义嘛，指的是让流量平均分配使得下面管理的服务器的负载更平均

            2、它还需要保证流量流经的服务器与流量源头在地缘上是比较接近的



缺点：

    同步延时
    ---------------------------------------------------------------------
    一般我们
        会通过CDN厂商的接口        ->   将静态的资源写入到某一个 CDN节点上
        再由 CDN 内部的同步机制    ->   将资源分散同步到每个CDN节点

    即使 CDN 内部网络经过了优化，这个同步的过程是有延时的




CDN 对 静态资源 进行加速：

    1、DNS 技术是 CDN 实现中使用的核心技术   ->   可以将用户的请求映射到 CDN节点上

    2、DNS 解析结果需要做本地缓存，降低 DNS 解析过程的响应时间

    3、GSLB 可以给用户返回一个离着他更近的节点，加快静态资源的访问速度



大量的静态资源请求：


    对于移动 APP 来说   ==>   这些静态资源主要是   ->   图片、视频、流媒体信息

    对于 Web 网站来说   ==>   则包括了   ->   JavaScript文件、CSS文件、静态HTML 文件等等


当下热门应用：

    目前处于 小视频和直播风口上



========================================================================================================================
消息队列                             // 暂时存储数据的一个容器   ==>  平衡  低速系统、高速系统   ->  处理任务 时间差   的一个工具
========================================================================================================================

------------------------------------------------------------------------------------------------------------------------
秒杀场景                                                                                // 解耦、异步、削峰
------------------------------------------------------------------------------------------------------------------------

本质：
    暂时存储数据的一个容器

主要作用：
    平衡  低速系统、高速系统   ->   处理任务 时间差    的一个工具

应用：
    Java线程池
    RPC框架        网络请求写入队列
    秒杀场景       流量削峰



消息队列的应用场景：
    1、流量削峰（秒杀系统）                                        ->      请求处理的延迟
    2、异步处理（非核心流程异步处理 提升效率）                       ->      消息丢失的风险
    3、系统解耦（用户下单后增加积分、通知仓储服务发货）


新问题：
    系统复杂性
    消息的丢失、重复
    消息延迟
    消息顺序
    ...




------------------------------------------------------------------------------------------------------------------------
消息幂等                                                                    // msg_id   +   Redis 插入判重
------------------------------------------------------------------------------------------------------------------------

消息丢失：
    1、写入            网络抖动（网络堵塞、网络断开、丢包）          ==>      消息重传       ->      2~3次
    2、存储            异步刷盘（减少异步刷盘间隔  / 同步刷盘）      ==>      ACK机制（acks=all）
    3、消费

------------------------------------------------------------------------------------------------------------------------
防止消息丢失：

    1、生产端的重试

    2、消息队列配置集群模式

    3、消费端合理处理消费进度


为了解决消息的丢失，通常会造成：

    1、性能上的问题

    2、消息的重复问题       ->      通过  保证消息处理的幂等性  解决



------------------------------------------------------------------------------------------------------------------------


------------------------------------------------------------------------------------------------------------------------
1、在消息生产的过程中丢失消息

    网络抖动（网络堵塞、网络断开、丢包）      ==>      消息重传       ->      2~3次


------------------------------------------------------------------------------------------------------------------------
2、在消息队列中丢失消息

    降低 异步刷盘 的间隔时间           // 不推荐      ->      严重影响 刷盘效率


--------------------------------------------------------
kafka防止消息丢失：

    ACK机制：

        集群部署    +   多个副本

        Leader     +    Follower    +   Follower-ISR（多个Leader 副本）


        1、Leader    ->  消息的写入和消费

        2、Follower  ->  从Leader异步地 同步消息

        3、ACK机制：

            生产者发送消息  +  acks   ===>  Leader + Follower-ISR    ==>  都收到并确认后   ==>  msg发送成功

        4、Leader异常      ->      从ISR中选举 新Leader    ===>    且消息并不会丢失         // 除非所有 ISR集群副本  都挂了

            --------------------------------------------------------------------------
            kafka 并不能100%保证不丢消息

                由于并没有像 MySQL一样的 WAL机制   -->  来持久化消息   ===>   说到底消息还是存在于 （Leader + ISR副本）内存中  -->  仅仅是有多份而已

                极端情况下，Leader + 所有 ISR集群副本  都挂了   ==>   会丢失部分消息


kafka场景应用：

    1、对丢失消息有一定容忍度：
        1、单机部署即可
        2、集群部署          ->      不建议开 ack模式

    2、对丢失消息 容忍度很低           // kafka没法100%

        集群部署   +   并开启 ACK机制（acks=all）

        -------------------------------------------
        不建议 同步刷盘   ===>  IO压力巨大  ->  不适用




------------------------------------------------------------------------------------------------------------------------
3、消费的过程中

    异常情况：
        1、消息接收时     ->  网络抖动

        2、处理消息时     ->  异常

        3、更新消费进度    ->  更新时，恰好宕机      ==>   未更新 消息消费状态

            以上都有可能导致   ->   重复消费   ==>   消息幂等 解决


    保证消息 只被消费一次  ->   幂等

        网络的抖动、机器的宕机、处理的异常  ->  都是比较难以避免的    ==>   工业上 并没有成熟的方法

        要求放宽  ->  重复消费不要紧  ==>  只要 重复消费结果 ->  和 消费一次结果一致   ===>   幂等（生产和消费 幂等）


------------------------------------------------------------------------------------------------------------------------
幂等：

    1、生产端
        producer idempotency（生产过程的幂等性）

        映射关系：  <生产者ID, 最后一条消息ID>     ==>     check msg_id 和 last_msg_id     -->  true/false


    2、消费端

        通用层和业务层

            1、为每一个消息  ->   生成一个唯一的 ID    ==>  插入DB（标识）

            2、消费时，check msg_id   ==>  是否被消费过

        新问题 ->  事务问题

            DB、消费msg    -->  两个操作  同时成功/失败


        解决：
            1、加锁                 // 保证原子操作

            2、乐观锁               // 双重ID校验

                生产时    ==>    msg_id   +   业务行数据_版本号

            3、分布式事务





------------------------------------------------------------------------------------------------------------------------
消息延迟                                                                                    // 提升消息队列的性能
------------------------------------------------------------------------------------------------------------------------

提升消息队列的性能       ->      降低消息消费的延迟

    1、我们可以使用消息队列提供的工具，或者通过发送监控消息的方式来监控消息的延迟情况

        1、MQ可视化监控工具 / JMX

        2、特殊消息                      // 如：时间戳msg   ->   生成时间 、 消费时间  ->  时间差    ==>  监控 消费速率

    2、横向扩展消费者是提升消费处理能力的重要方式

        1、增加 消费者数量

        2、线程池消费     ==>      异步 ->  提升消费者 吞吐量

    3、选择 高性能的数据存储方式 配合零拷贝技术，可以提升消息的消费性能

        1、高性能存储介质               // 本地存储（Page Cache + 磁盘顺序写）      ->      代替DB写（网络传输 + 随机写）

        2、零拷贝




任务堆积：

    队列是一种常用的组件，只要涉及到队列   ->   任务的堆积 就是一个不可忽视的问题

    遇到过的很多故障都是源于此


    -----------------------------------------------------------------------------------------------------
    DB慢查询（Tomcat线程 同步请求）      ->      Tomcat线程池  被占满（请求堆积）      ->        整体服务 不可用

        1、监控                     ->      对 Tomcat线程池的任务堆积情况 实时监控

        2、线程池队列 保护策略        ->      直接丢弃




========================================================================================================================
分布式服务
========================================================================================================================


------------------------------------------------------------------------------------------------------------------------
单体架构的弊端                                                                   // 扩展性、维护成本、可用性、耦合性...
------------------------------------------------------------------------------------------------------------------------

实际业务 基于什么样的考虑，做微服务拆分：

    1、系统中使用的资源出现扩展性问题，尤其是数据库的连接数出现瓶颈

    2、大团队共同维护一套代码，带来研发效率的降低和研发成本的提升

    3、系统部署成本越来越高



性能、可用性、可扩展性：

    高性能、高可用     ->      给用户带来更好的使用体验

    扩展性            ->      方便我们支撑更大量级的并发


成本：                                          // 现实：成本(金钱)  才是   架构设计中的决定性因素
    服务器的费用
    研发团队
        开发成本、沟通成本、运维成本



------------------------------------------------------------------------------------------------------------------------
微服务拆分
------------------------------------------------------------------------------------------------------------------------

微服务拆分的原则:

    1、做到单一服务内部功能的高内聚，和低耦合
    2、你需要关注服务拆分的粒度，先粗略拆分，再逐渐细化
    3、拆分的过程，要尽量避免影响产品的日常功能迭代，也就是说，要一边做产品功能迭代，一边完成服务化拆分
    4、服务接口的定义要具备可扩展性



微服务拆分以后最大的问题：

    故障 的排查与定位



------------------------------------------------------------------------------------------------------------------------
RPC框架
------------------------------------------------------------------------------------------------------------------------

新问题：
    跨网络通信
    服务治理



优化 RPC 框架的性能：

    1、I/O模型                             // 同步多路 I/O 复用模型

    2、网络通信参数 调优                     // tcp_nodelay 设置为true、  发送/接收/.. 缓冲区大小

    3、序列化协议                           // JSON  、 Thrift / Protobuf



序列化协议：

    1、性能要求不高      ->  首选JSON

    2、高性能           ->   Thrift 、Protobuf           // Thrift  提供了配套的 RPC框架

    3、存储场景         ->   用 Protobuf 替换 JSON       节省存储空间



------------------------------------------------------------------------------------------------------------------------
注册中心                                                                                // 服务注册与发现  、 变更通知
------------------------------------------------------------------------------------------------------------------------

注册中心：

    1、注册中心可以让我们动态地变更 RPC服务的节点信息

        对于动态扩缩容，故障快速恢复，以及服务的优雅关闭都有重要的意义

    2、心跳机制是一种常见的探测服务状态的方式

        你在实际的项目中也可以考虑使用

    3、我们需要对注册中心中管理的节点提供一些保护策略，避免节点被过度摘除导致的服务不可用

        服务下线 一定比例后  ->  停止摘除服务节点，并发送警报

    4、通知风暴

        1、控制一组注册中心 管理的服务集群的规模

        2、扩容

        3、通知变更信息 尽量小                          // 变更某一个节点，只通知这个节点的变更信息  ->  而不是整个节点信息一起带上



注册中心的两个基本功能：                        // 常见的注册中心有 zookeeper、eureka、etcd
    1、存储服务提供者的地址
    2、当服务提供者发生变化时，将变化通知客户端


道路交通这个比喻太形象了，很不错

    街道新增了一条道路，通知给各个车辆               注册中心的注册和发现
    监控每个道路的车辆运行情况                      服务的监控治理
    平衡每个道路的车辆数，需要交警的协调             服务的负载均衡
    道路出现拥堵或者维修                           服务的熔断
    调查道路拥堵的原因                             分布式追踪



------------------------------------------------------------------------------------------------------------------------
链路追踪                                                                                 // 调用链路关系、定位追踪
------------------------------------------------------------------------------------------------------------------------

排查单次慢请求：

    服务的追踪的需求主要有两点：

        1、对代码要无侵入

            用切面编程 的方式来解决

        2、性能上要低损耗

            用 静态代理和日志采样 的方式，来尽量减少 追踪日志对于系统性能的影响

    requestId：

        无论是单体系统还是服务化架构，无论是服务追踪还是业务问题排查，你都需要在日志中增加 requestId

        这样可以将你的日志串起来，给你呈现一个完整的问题场景

        如果 requestId 可以在客户端上生成，在请求业务接口的时候传递给服务端，那么就可以把客户端的日志体系也整合进来，对于问题的排查帮助更大



开源解决方案：

    Zipkin

    Jaeger



------------------------------------------------------------------------------------------------------------------------
负载均衡
------------------------------------------------------------------------------------------------------------------------


负载均衡策略：

    轮询
    hash
    随机
    指定权重
    最短响应时间



代理类的负载均衡服务：

    Nginx、LVS

工作中的运用技巧：

    1、网站负载均衡服务的部署，是以 LVS 承接入口流量，在应用服务器之前，部署 Nginx 做细化的流量分发和故障节点检测。
        当然，如果你的网站的并发不高，也可以考虑不引入 LVS

    2、负载均衡的策略可以优先选择 动态策略
        保证请求发送到性能最优的节点上；
        如果没有合适的动态策略，那么可以选择轮询的策略，让请求平均分配到所有的服务节点上。

    3、Nginx 可以引入 nginx_upstream_check_module
        对后端服务做定期的存活检测，后端的服务节点在重启时，也要秉承着“先切流量后重启”的原则，尽量减少节点重启对于整体系统的影响。




------------------------------------------------------------------------------------------------------------------------
网关
------------------------------------------------------------------------------------------------------------------------


网关作用：

    动态路由

    协议转换

    服务治理                // 熔断、降级、限流、分流

    统一认证和授权          // OAuth

    黑白名单               // IP、设备ID、用户ID...

    统一入口日志           // requestId 生成处



实现：

    1、Zuul                        责任链模式

    2、Spring-Cloud-Gateway        IO多路复用 + 线程池




------------------------------------------------------------------------------------------------------------------------
跨机房部署                                                                                       // 轻易不要尝试
------------------------------------------------------------------------------------------------------------------------

跨机房部署：

    1、不同机房的数据传输延迟 是造成多机房部署困难的主要原因，

        同城多机房的延迟一般在 1ms~3ms，异地机房的延迟在 50ms 以下，而跨国机房的延迟在 200ms 以下

    2、同城多机房方案

        可以允许有 跨机房数据写入 的发生

        但是 数据的读取和服务的调用 应该尽量保证在同一个机房中

    3、异地多活方案

        要避免 跨机房同步的数据写入和读取

        采取 异步的方式，将数据从一个机房 同步到 另一个机房




------------------------------------------------------------------------------------------------------------------------
Service Mesh                                                        // 抽离 服务治理代理层（Sidecar） ==>  屏蔽 服务治理细节
------------------------------------------------------------------------------------------------------------------------


微服务化过程中，引入中间件  ->  解决服务之间 通信和服务治理 的问题：

    1、用RPC框架   解决服务通信的问题

    2、用注册中心  解决服务注册和发现的问题

    3、使用 分布式Trace中间件，排查跨服务调用

    4、使用 负载均衡服务器，解决服务扩展性的问题

    5、在 API网关中 植入服务熔断、降级和流控等服务治理的策略


痛点：

    所有这些 中间件  ->  无法 跨语言    ==>   由此引出  将所有这些 中间件 剥离出来   ->   作为 独立组件(代理层) 部署   ==>  Service Mesh 思想

------------------------------------------------------------------------------------------------------------------------


Service Mesh 的核心思想：

    剥离 服务治理策略  ->  形成一个 独立代理层   ==>   屏蔽 服务化架构中 服务治理的细节

        可以考虑将服务治理的细节，从 RPC客户端 中拆分出来，形成一个代理层 单独部署

        这个代理层可以使用单一的语言实现，所有的流量都经过代理层 来使用其中的服务治理策略


    这是一种“关注点分离”的实现方式：

        1、业务代码、服务治理     ->   隔离开  解耦

        2、服务治理策略下沉，成为 独立的基础模块

        3、实现 跨语言服务治理策略的 复用

        4、还能对这些 Sidecar 做统一的管理




什么是 Service Mesh：

    RPC通信 代理层 ==>  处理服务之间的通信  ->  屏蔽 服务治理的细节

        将 服务治理的细节 从RPC客户端中拆分出来

        ==>  独立组件  ->  独立部署（部署在RPC服务-同主机）

        ==>  并作为代理层 代理RPC通信  ->  屏蔽 服务治理的细节（负载均衡、熔断降级、限流、全链路日志...）


    实现形式：

        在 应用程序同主机上 部署一个代理程序  ->  Sidecar（边车）


    通信方式：                                                           // RPC 之间的通信  ->  代理层(Sidecar) 之间的通信

        微服务           ==>        RPC客户端  ->  RPC服务端

        Service Mesh    ==>        RPC客户端  ->  Sidecar(客户端-同主机)  -->  Sidecar（服务端-同主机）  ->  RPC服务端


    Sidecar：

        服务治理    ->      服务发现、负载均衡、服务路由、流量控制

        转发请求至  ->  服务端Sidecar



------------------------------------------------------------
Service Mesh 实现方式：                   // Istio

    1、数据平面

        负责数据的传输


    2、控制平面

        控制服务治理策略的植入




国内改进方案：

    在Istio中，每次请求都需要 经过控制平面   =>   也就是说，每次请求都需要 跨网络地调用 Mixer，这会极大地影响性能

    出于性能的考虑：

        一般会把 服务治理策略 植入到 数据平面中

        控制平面  ->  负责 服务治理策略 数据的下发





将流量转发到 Sidecar：

    1、iptables 实现流量的劫持                      // 无感知    业务透明

        无感知地引入 Sidecar


    2、轻量级客户端  实现流量转发                    // 业务代码  有配置侵入

        在客户端配置 Sidecar网络端口


将Sidecar 部署到 客户端和服务端的调用链路上  ->  让它代理进出流量

    你就可以使用  ->  运行在Sidecar中的 服务治理的策略了


落地方案：

    1、Istio                     先驱者（数据平面 + 控制平面）         ->      Mixer 的性能问题（每次请求 -> 跨网络）

    2、Linkerd                   Scala 语言编写                      ->      内存占用

    3、SOFAMesh                  蚂蚁



========================================================================================================================
维护
========================================================================================================================





========================================================================================================================
实战
========================================================================================================================


1、计数系统设计


2、信息流设计